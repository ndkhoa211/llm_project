{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "80lwPwQjVVKS"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "notebook_start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjrfqGg19--I"
   },
   "source": [
    "# An LLM Architecture (Backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0wX2QPw-Kig"
   },
   "source": [
    "Let's set the configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jMThqIsL9CQO"
   },
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVMi_qlRZkUz"
   },
   "source": [
    "## A Placeholder GPT2 Model Architecture Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KaxnqgJS-wT5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "  def __init__(self,\n",
    "               config\n",
    "               ):\n",
    "    super().__init__()\n",
    "    self.token_emb = nn.Embedding(config[\"vocab_size\"],\n",
    "                                  config[\"emb_dim\"])\n",
    "    self.position_emb = nn.Embedding(config[\"context_length\"],\n",
    "                                     config[\"emb_dim\"])\n",
    "    self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n",
    "\n",
    "    # a placeholder for TransformerBlock\n",
    "    self.transformer_blocks = nn.Sequential(\n",
    "        *[DummyTransformerBlock(config) for _ in range(config[\"n_layers\"])])\n",
    "\n",
    "    # a placeholder for LayerNorm\n",
    "    self.final_norm = DummyLayerNorm(config[\"emb_dim\"])\n",
    "\n",
    "    self.out_head = nn.Linear(config[\"emb_dim\"],\n",
    "                              config[\"vocab_size\"],\n",
    "                              bias=False)\n",
    "\n",
    "  def forward(self, input_token):\n",
    "    batch_size, sequence_length = input_token.shape\n",
    "    token_embeds = self.token_emb(input_token)\n",
    "    position_embeds = self.position_emb(\n",
    "        torch.arange(sequence_length,\n",
    "                     device=input_token.device))\n",
    "    embeds = token_embeds + position_embeds\n",
    "    x = self.drop_emb(embeds)\n",
    "    x = self.transformer_blocks(x)\n",
    "    x = self.final_norm(x)\n",
    "    logits = self.out_head(x)\n",
    "    return logits\n",
    "\n",
    "\n",
    "# placeholder\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x\n",
    "\n",
    "\n",
    "# placeholder\n",
    "class DummyLayerNorm(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o-ezJkcmc7nB",
    "outputId": "4522fc60-03d4-47ad-f3f8-ced50dc03d69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6109, 3626, 6100,  345],\n",
       "        [6109, 1110, 6622,  257]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup example\n",
    "import tiktoken\n",
    "\n",
    "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "batch = []\n",
    "\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(bpe_tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(bpe_tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lp5JsM-HdeTi",
    "outputId": "424af401-6b87-447e-f91a-6eb754587d72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape:  torch.Size([2, 4, 50257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8344, -1.1415,  0.6781,  ...,  0.5266,  0.7607,  0.9310],\n",
       "         [ 0.0128,  0.4333, -0.4135,  ...,  0.5391,  0.0836,  0.6025],\n",
       "         [ 0.4171,  0.3006, -0.4873,  ..., -1.8629,  0.1682, -0.8876],\n",
       "         [ 1.0438, -0.5067,  0.0866,  ..., -0.6608, -0.5339,  1.9094]],\n",
       "\n",
       "        [[-1.2385, -1.3217,  0.4691,  ...,  0.2034,  0.8297,  0.8731],\n",
       "         [-0.2632, -0.0216, -0.5709,  ..., -0.4456,  0.6116,  0.5455],\n",
       "         [ 0.6241, -0.0704, -0.0456,  ..., -1.5775,  0.3410, -0.7189],\n",
       "         [ 0.1235,  0.4273, -1.2483,  ..., -1.3417, -1.4207,  2.0975]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an instance\n",
    "torch.manual_seed(211)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "logits = model(batch)\n",
    "print(\"output shape: \", logits.shape)\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ev8RMHLOeZgE"
   },
   "source": [
    "# Implement LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAddun8Gepo-"
   },
   "source": [
    "Let's experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dPOnv_4vd7ja",
    "outputId": "b287b578-b8b5-4431-ea99-892afcc3ca9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0540, 0.2893, 0.0442, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.8197, 0.1849, 0.1102, 1.7938, 0.3290, 1.2042, 0.0000],\n",
       "        [0.3081, 0.1382, 0.2541, 0.0000, 0.5109, 0.0000, 0.0000]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example setup\n",
    "torch.manual_seed(211)\n",
    "\n",
    "# create 3 training examples with 5 dimensions each\n",
    "batch_example = torch.randn(3, 5)\n",
    "\n",
    "layer = nn.Sequential(nn.Linear(5, 7),\n",
    "                      nn.ReLU())\n",
    "\n",
    "output = layer(batch_example)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tp4IIwPjfX6W",
    "outputId": "23af507d-4356-45d3-c4d3-654dabe402b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  tensor([[0.1982],\n",
      "        [0.6345],\n",
      "        [0.1730]], grad_fn=<MeanBackward1>)\n",
      "variance:  tensor([[0.1536],\n",
      "        [0.4460],\n",
      "        [0.0383]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# compute mean and variance of each sample\n",
    "mean = output.mean(dim=-1, keepdim=True)\n",
    "var = output.var(dim=-1, keepdim=True)\n",
    "print(\"mean: \", mean)\n",
    "print(\"variance: \", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-rhSYgqf-jy"
   },
   "source": [
    "Let's normalize (aka layer normalization) the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HLKzU-Jsf1w_",
    "outputId": "2b5b033c-2f85-4afd-dd15-f271a61142be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized mean: \n",
      " tensor([[     0.0000],\n",
      "        [     0.0000],\n",
      "        [    -0.0000]], grad_fn=<MeanBackward1>)\n",
      "normalized variance: \n",
      " tensor([[1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n",
      "normalized layer output: \n",
      " tensor([[ 2.1836,  0.2324, -0.3930, -0.5058, -0.5058, -0.5058, -0.5058],\n",
      "        [ 0.2772, -0.6733, -0.7852,  1.7360, -0.4575,  0.8530, -0.9502],\n",
      "        [ 0.6897, -0.1777,  0.4139, -0.8838,  1.7256, -0.8838, -0.8838]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "norm_output = (output - mean) / torch.sqrt(var)\n",
    "norm_mean = norm_output.mean(dim=-1, keepdim=True)\n",
    "norm_var = norm_output.var(dim=-1, keepdim=True)\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"normalized mean: \\n\", norm_mean)\n",
    "print(\"normalized variance: \\n\", norm_var)\n",
    "print(\"normalized layer output: \\n\", norm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAaAVob4hk33"
   },
   "source": [
    "## Implement LayerNorm class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SuIWCiaHgYnF"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "  def __init__(self, emb_dim):\n",
    "    super().__init__()\n",
    "    self.epsilon = 1e-5\n",
    "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "  def forward(self, x):\n",
    "    mean = x.mean(dim=-1, keepdim=True)\n",
    "    var = x.var(dim=-1,\n",
    "                unbiased=False, # Bessel's correction (n-1)\n",
    "                keepdim=True)\n",
    "    norm_x = (x - mean) / torch.sqrt(var + self.epsilon)\n",
    "    return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G5HDk-driSaW",
    "outputId": "10a2c1fb-4c56-47d6-cbe8-9ba99ad0f34c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized mean: \n",
      " tensor([[    -0.0000],\n",
      "        [    -0.0000],\n",
      "        [    -0.0000]], grad_fn=<MeanBackward1>)\n",
      "normalized variance: \n",
      " tensor([[1.0000],\n",
      "        [1.0000],\n",
      "        [0.9999]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# setup example\n",
    "layer_norm = LayerNorm(emb_dim=5)\n",
    "layer_norm_output = layer_norm(batch_example)\n",
    "mean = layer_norm_output.mean(dim=-1, keepdim=True)\n",
    "var = layer_norm_output.var(dim=-1, unbiased=False, keepdim=True)\n",
    "\n",
    "print(\"normalized mean: \\n\", mean)\n",
    "print(\"normalized variance: \\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVz_hlRtjOpm"
   },
   "source": [
    "# Implement a FFN With GELU Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GC3RkaKBjF-h"
   },
   "outputs": [],
   "source": [
    "# GELU activation function\n",
    "class GELU(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self, x):\n",
    "    return 0.5 * x * ( 1 + torch.tanh(\n",
    "        torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "        (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "gki8bcEDkhyA",
    "outputId": "e9fd7bbf-c850-4603-f130-0f03e02bdb73"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot GELU\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu = GELU()\n",
    "x = torch.linspace(-5, 5, 100)\n",
    "plt.plot(x, gelu(x))\n",
    "plt.title(\"GELU Activation Function\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"GELU(x)\")\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_xn7uy_TkuEH"
   },
   "outputs": [],
   "source": [
    "# FFN module\n",
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "        nn.Linear(config[\"emb_dim\"],       # 768\n",
    "                  4 * config[\"emb_dim\"]),  # 3072\n",
    "        GELU(),                            # 3072\n",
    "        nn.Linear(4 * config[\"emb_dim\"],   # 3072\n",
    "                  config[\"emb_dim\"])       # 768\n",
    "        )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kEnDAqRYlp07",
    "outputId": "a1334c04-c018-4419-dd17-52768d305e62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an instance\n",
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.randn(3, 5, 768)\n",
    "output = ffn(x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evrJcgC5m2Qm"
   },
   "source": [
    "# Add Skip Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HknY1CGnoKn"
   },
   "source": [
    "Let's make an example nn to show how skip connection affect learning ability of a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6V6_1LmXmaXy"
   },
   "outputs": [],
   "source": [
    "class ExampleDeepNN(nn.Module):\n",
    "  def __init__(self, layer_sizes_list, use_skip_connection):\n",
    "    super().__init__()\n",
    "    self.use_skip_connection = use_skip_connection\n",
    "    self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "    ])\n",
    "\n",
    "  def forward(self, x):\n",
    "    for layer in self.layers:\n",
    "      # Compute the output of the current layer\n",
    "      layer_output = layer(x)\n",
    "      # Check if skip_connection can be applied\n",
    "      if self.use_skip_connection and x.shape == layer_output.shape:\n",
    "        x = x + layer_output\n",
    "      else:\n",
    "        x = layer_output\n",
    "    return x\n",
    "\n",
    "\n",
    "def print_gradients(model, x):\n",
    "    # forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # calculate loss\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "\n",
    "    # calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absolute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_AqGy8QAo0qG"
   },
   "outputs": [],
   "source": [
    "# example setup\n",
    "layer_sizes = [4, 4, 4, 4, 4, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1., 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2I3YVLfqqFW6",
    "outputId": "2cf7da8d-0ae0-465d-b47e-1174712446c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.0001875293382909149\n",
      "layers.1.0.weight has gradient mean of 0.00015768714365549386\n",
      "layers.2.0.weight has gradient mean of 0.0004450336564332247\n",
      "layers.3.0.weight has gradient mean of 0.0012923781760036945\n",
      "layers.4.0.weight has gradient mean of 0.01807313971221447\n"
     ]
    }
   ],
   "source": [
    "# without skip connection\n",
    "torch.manual_seed(211)\n",
    "model_wo_skip_connection = ExampleDeepNN(\n",
    "    layer_sizes, use_skip_connection=False\n",
    ")\n",
    "print_gradients(model_wo_skip_connection, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2SbMjIG0pj1M",
    "outputId": "ca50dc43-1674-4d78-9cde-d511b3158bfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.6975078582763672\n",
      "layers.1.0.weight has gradient mean of 0.6017700433731079\n",
      "layers.2.0.weight has gradient mean of 0.6710034012794495\n",
      "layers.3.0.weight has gradient mean of 0.4883677363395691\n",
      "layers.4.0.weight has gradient mean of 4.474058151245117\n"
     ]
    }
   ],
   "source": [
    "# with skip connection\n",
    "torch.manual_seed(211)\n",
    "model_w_skip_connection = ExampleDeepNN(\n",
    "    layer_sizes, use_skip_connection=True\n",
    ")\n",
    "print_gradients(model_w_skip_connection, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lio4bauuJlLx"
   },
   "source": [
    "# Implement Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bHUYqA9fqIQw"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD = C:\\Users\\user\\Desktop\\llm_project\\exp_notebooks\n",
      "models exists?  True\n",
      "['C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\python312.zip',\n",
      " 'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\DLLs',\n",
      " 'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib',\n",
      " 'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312',\n",
      " 'C:\\\\Users\\\\user\\\\Desktop\\\\llm_project\\\\.venv']\n"
     ]
    }
   ],
   "source": [
    "import os, sys, pprint\n",
    "print(\"CWD =\", os.getcwd())          # <-- very likely .../exp_notebooks\n",
    "print(\"models exists? \", os.path.isdir(\"../models\"))\n",
    "pprint.pp(sys.path[:5])              # first few search paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added → C:\\Users\\user\\Desktop\\llm_project\n"
     ]
    }
   ],
   "source": [
    "import sys, pathlib, os\n",
    "\n",
    "# one directory up from the notebook’s location\n",
    "project_root = pathlib.Path(os.getcwd()).resolve().parent\n",
    "\n",
    "# add it to sys.path if it isn’t already there\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(\"Added →\", project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "MsRjs-REAsmb"
   },
   "outputs": [],
   "source": [
    "from models.layers.attentions import MultiHeadAttention\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.attention = MultiHeadAttention(input_embedding_dim=config[\"emb_dim\"],\n",
    "                                        output_embedding_dim=config[\"emb_dim\"],\n",
    "                                        context_length=config[\"context_length\"],\n",
    "                                        dropout=config[\"drop_rate\"],\n",
    "                                        num_heads=config[\"n_heads\"],\n",
    "                                        qkv_bias=config[\"qkv_bias\"])\n",
    "    self.feed_forward = FeedForward(config)\n",
    "    self.layer_norm1 = LayerNorm(config[\"emb_dim\"])\n",
    "    self.layer_norm2 = LayerNorm(config[\"emb_dim\"])\n",
    "    self.drop_skip = nn.Dropout(config[\"drop_rate\"])\n",
    "\n",
    "  def forward(self, x):\n",
    "    # skip connection for attention block\n",
    "    shortcut = x\n",
    "    x = self.layer_norm1(x)\n",
    "    x = self.attention(x)  # shape: [batch_size, num_tokens, emb_size]\n",
    "    x = self.drop_skip(x)\n",
    "    x = shortcut + x # skip connection\n",
    "\n",
    "    # skip connection for feed forward block\n",
    "    shortcut = x\n",
    "    x = self.layer_norm2(x)\n",
    "    x = self.feed_forward(x)\n",
    "    x = self.drop_skip(x)\n",
    "    x = shortcut + x # skip connection\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vuhrbmnqEEJ3",
    "outputId": "ada95525-73eb-4c3a-eed0-a85c0494ff34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([3, 5, 768])\n",
      "output shape:  torch.Size([3, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "# create an instance\n",
    "torch.manual_seed(211)\n",
    "x = torch.rand(3, 5, 768) # shape: [batch_size, num_tokens, emb_size]\n",
    "transformer_block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = transformer_block(x)\n",
    "\n",
    "print(\"input shape: \", x.shape)\n",
    "print(\"output shape: \", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7AdBXBoLrL2"
   },
   "source": [
    "# Implement The GPT2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "iLo0pRoWG3NM"
   },
   "outputs": [],
   "source": [
    "class GPT2Model(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.token_emb = nn.Embedding(config[\"vocab_size\"],\n",
    "                                  config[\"emb_dim\"])\n",
    "    self.position_emb = nn.Embedding(config[\"context_length\"],\n",
    "                                     config[\"emb_dim\"])\n",
    "    self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n",
    "\n",
    "    self.transformer_blocks = nn.Sequential(\n",
    "        *[TransformerBlock(config) for _ in range(config[\"n_layers\"])])\n",
    "\n",
    "    self.final_norm = LayerNorm(config[\"emb_dim\"])\n",
    "\n",
    "    self.out_head = nn.Linear(config[\"emb_dim\"],\n",
    "                              config[\"vocab_size\"],\n",
    "                              bias=False)\n",
    "\n",
    "  def forward(self, input_token):\n",
    "    batch_size, sequence_length = input_token.shape\n",
    "    token_embeds = self.token_emb(input_token)\n",
    "    position_embeds = self.position_emb(\n",
    "        torch.arange(sequence_length,\n",
    "                     device=input_token.device))\n",
    "    embeds = token_embeds + position_embeds\n",
    "    x = self.drop_emb(embeds)\n",
    "    x = self.transformer_blocks(x)\n",
    "    x = self.final_norm(x)\n",
    "    logits = self.out_head(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9IfWhCzxMjLf",
    "outputId": "e12d2df1-f12d-454c-ed64-d117046ddfe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch: \n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "output shape:  torch.Size([2, 4, 50257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1072,  0.1188,  0.1927,  ..., -0.5764,  0.1074,  0.6490],\n",
       "         [-0.2752,  0.1129, -0.6717,  ..., -0.2857,  0.4062, -0.5701],\n",
       "         [-0.1557, -0.5613, -0.9250,  ..., -0.0414, -0.1987, -0.0625],\n",
       "         [ 0.2493, -0.1712, -0.8216,  ..., -0.2317,  1.1078,  0.5068]],\n",
       "\n",
       "        [[ 0.0198,  0.7883, -0.3144,  ..., -0.5261,  0.1450,  0.2379],\n",
       "         [-0.1052,  0.3466, -1.0477,  ...,  0.0095,  0.6028,  0.4315],\n",
       "         [-0.8114,  0.0372, -0.5859,  ...,  0.1969,  0.3706, -0.9927],\n",
       "         [ 0.1452, -0.5805, -1.3235,  ..., -0.2818,  0.9731,  0.1584]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an instance\n",
    "torch.manual_seed(211)\n",
    "model = GPT2Model(GPT_CONFIG_124M)\n",
    "\n",
    "logits = model(batch)\n",
    "print(\"input batch: \\n\", batch)\n",
    "print(\"output shape: \", logits.shape)\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSSvAdQEOCHw"
   },
   "source": [
    "Let's explore the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UbmO5jwUNvkL",
    "outputId": "5f51042f-14b7-4605-d9fd-f2903a351dba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_emb.weight: torch.Size([50257, 768])\n",
      "position_emb.weight: torch.Size([1024, 768])\n",
      "transformer_blocks.0.attention.W_query.weight: torch.Size([768, 768])\n",
      "transformer_blocks.0.attention.W_key.weight: torch.Size([768, 768])\n",
      "transformer_blocks.0.attention.W_value.weight: torch.Size([768, 768])\n",
      "transformer_blocks.0.attention.output_projection.weight: torch.Size([768, 768])\n",
      "transformer_blocks.0.attention.output_projection.bias: torch.Size([768])\n",
      "transformer_blocks.0.feed_forward.layers.0.weight: torch.Size([3072, 768])\n",
      "transformer_blocks.0.feed_forward.layers.0.bias: torch.Size([3072])\n",
      "transformer_blocks.0.feed_forward.layers.2.weight: torch.Size([768, 3072])\n",
      "transformer_blocks.0.feed_forward.layers.2.bias: torch.Size([768])\n",
      "transformer_blocks.0.layer_norm1.scale: torch.Size([768])\n",
      "transformer_blocks.0.layer_norm1.shift: torch.Size([768])\n",
      "transformer_blocks.0.layer_norm2.scale: torch.Size([768])\n",
      "transformer_blocks.0.layer_norm2.shift: torch.Size([768])\n",
      "transformer_blocks.1.attention.W_query.weight: torch.Size([768, 768])\n",
      "transformer_blocks.1.attention.W_key.weight: torch.Size([768, 768])\n",
      "transformer_blocks.1.attention.W_value.weight: torch.Size([768, 768])\n",
      "transformer_blocks.1.attention.output_projection.weight: torch.Size([768, 768])\n",
      "transformer_blocks.1.attention.output_projection.bias: torch.Size([768])\n",
      "transformer_blocks.1.feed_forward.layers.0.weight: torch.Size([3072, 768])\n",
      "transformer_blocks.1.feed_forward.layers.0.bias: torch.Size([3072])\n",
      "transformer_blocks.1.feed_forward.layers.2.weight: torch.Size([768, 3072])\n",
      "transformer_blocks.1.feed_forward.layers.2.bias: torch.Size([768])\n",
      "transformer_blocks.1.layer_norm1.scale: torch.Size([768])\n",
      "transformer_blocks.1.layer_norm1.shift: torch.Size([768])\n",
      "transformer_blocks.1.layer_norm2.scale: torch.Size([768])\n",
      "transformer_blocks.1.layer_norm2.shift: torch.Size([768])\n",
      "transformer_blocks.2.attention.W_query.weight: torch.Size([768, 768])\n",
      "transformer_blocks.2.attention.W_key.weight: torch.Size([768, 768])\n",
      "transformer_blocks.2.attention.W_value.weight: torch.Size([768, 768])\n",
      "transformer_blocks.2.attention.output_projection.weight: torch.Size([768, 768])\n",
      "transformer_blocks.2.attention.output_projection.bias: torch.Size([768])\n",
      "transformer_blocks.2.feed_forward.layers.0.weight: torch.Size([3072, 768])\n",
      "transformer_blocks.2.feed_forward.layers.0.bias: torch.Size([3072])\n",
      "transformer_blocks.2.feed_forward.layers.2.weight: torch.Size([768, 3072])\n",
      "transformer_blocks.2.feed_forward.layers.2.bias: torch.Size([768])\n",
      "transformer_blocks.2.layer_norm1.scale: torch.Size([768])\n",
      "transformer_blocks.2.layer_norm1.shift: torch.Size([768])\n",
      "transformer_blocks.2.layer_norm2.scale: torch.Size([768])\n",
      "transformer_blocks.2.layer_norm2.shift: torch.Size([768])\n",
      "transformer_blocks.3.attention.W_query.weight: torch.Size([768, 768])\n",
      "transformer_blocks.3.attention.W_key.weight: torch.Size([768, 768])\n",
      "transformer_blocks.3.attention.W_value.weight: torch.Size([768, 768])\n",
      "transformer_blocks.3.attention.output_projection.weight: torch.Size([768, 768])\n",
      "transformer_blocks.3.attention.output_projection.bias: torch.Size([768])\n",
      "transformer_blocks.3.feed_forward.layers.0.weight: torch.Size([3072, 768])\n",
      "transformer_blocks.3.feed_forward.layers.0.bias: torch.Size([3072])\n",
      "transformer_blocks.3.feed_forward.layers.2.weight: torch.Size([768, 3072])\n",
      "transformer_blocks.3.feed_forward.layers.2.bias: torch.Size([768])\n",
      "transformer_blocks.3.layer_norm1.scale: torch.Size([768])\n",
      "transformer_blocks.3.layer_norm1.shift: torch.Size([768])\n",
      "transformer_blocks.3.layer_norm2.scale: torch.Size([768])\n",
      "transformer_blocks.3.layer_norm2.shift: torch.Size([768])\n",
      "transformer_blocks.4.attention.W_query.weight: torch.Size([768, 768])\n",
      "transformer_blocks.4.attention.W_key.weight: torch.Size([768, 768])\n",
      "transformer_blocks.4.attention.W_value.weight: torch.Size([768, 768])\n",
      "transformer_blocks.4.attention.output_projection.weight: torch.Size([768, 768])\n",
      "transformer_blocks.4.attention.output_projection.bias: torch.Size([768])\n",
      "transformer_blocks.4.feed_forward.layers.0.weight: torch.Size([3072, 768])\n",
      "transformer_blocks.4.feed_forward.layers.0.bias: torch.Size([3072])\n",
      "transformer_blocks.4.feed_forward.layers.2.weight: torch.Size([768, 3072])\n",
      "transformer_blocks.4.feed_forward.layers.2.bias: torch.Size([768])\n",
      "transformer_blocks.4.layer_norm1.scale: torch.Size([768])\n",
      "transformer_blocks.4.layer_norm1.shift: torch.Size([768])\n",
      "transformer_blocks.4.layer_norm2.scale: torch.Size([768])\n",
      "transformer_blocks.4.layer_norm2.shift: torch.Size([768])\n",
      "transformer_blocks.5.attention.W_query.weight: torch.Size([768, 768])\n",
      "transformer_blocks.5.attention.W_key.weight: torch.Size([768, 768])\n",
      "transformer_blocks.5.attention.W_value.weight: torch.Size([768, 768])\n",
      "transformer_blocks.5.attention.output_projection.weight: torch.Size([768, 768])\n",
      "transformer_blocks.5.attention.output_projection.bias: torch.Size([768])\n",
      "transformer_blocks.5.feed_forward.layers.0.weight: torch.Size([3072, 768])\n",
      "transformer_blocks.5.feed_forward.layers.0.bias: torch.Size([3072])\n",
      "transformer_blocks.5.feed_forward.layers.2.weight: torch.Size([768, 3072])\n",
      "transformer_blocks.5.feed_forward.layers.2.bias: torch.Size([768])\n",
      "transformer_blocks.5.layer_norm1.scale: torch.Size([768])\n",
      "transformer_blocks.5.layer_norm1.shift: torch.Size([768])\n",
      "transformer_blocks.5.layer_norm2.scale: torch.Size([768])\n",
      "transformer_blocks.5.layer_norm2.shift: torch.Size([768])\n",
      "transformer_blocks.6.attention.W_query.weight: torch.Size([768, 768])\n",
      "transformer_blocks.6.attention.W_key.weight: torch.Size([768, 768])\n",
      "transformer_blocks.6.attention.W_value.weight: torch.Size([768, 768])\n",
      "transformer_blocks.6.attention.output_projection.weight: torch.Size([768, 768])\n",
      "transformer_blocks.6.attention.output_projection.bias: torch.Size([768])\n",
      "transformer_blocks.6.feed_forward.layers.0.weight: torch.Size([3072, 768])\n",
      "transformer_blocks.6.feed_forward.layers.0.bias: torch.Size([3072])\n",
      "transformer_blocks.6.feed_forward.layers.2.weight: torch.Size([768, 3072])\n",
      "transformer_blocks.6.feed_forward.layers.2.bias: torch.Size([768])\n",
      "transformer_blocks.6.layer_norm1.scale: torch.Size([768])\n",
      "transformer_blocks.6.layer_norm1.shift: torch.Size([768])\n",
      "transformer_blocks.6.layer_norm2.scale: torch.Size([768])\n",
      "transformer_blocks.6.layer_norm2.shift: torch.Size([768])\n",
      "transformer_blocks.7.attention.W_query.weight: torch.Size([768, 768])\n",
      "transformer_blocks.7.attention.W_key.weight: torch.Size([768, 768])\n",
      "transformer_blocks.7.attention.W_value.weight: torch.Size([768, 768])\n",
      "transformer_blocks.7.attention.output_projection.weight: torch.Size([768, 768])\n",
      "transformer_blocks.7.attention.output_projection.bias: torch.Size([768])\n",
      "transformer_blocks.7.feed_forward.layers.0.weight: torch.Size([3072, 768])\n",
      "transformer_blocks.7.feed_forward.layers.0.bias: torch.Size([3072])\n",
      "transformer_blocks.7.feed_forward.layers.2.weight: torch.Size([768, 3072])\n",
      "transformer_blocks.7.feed_forward.layers.2.bias: torch.Size([768])\n",
      "transformer_blocks.7.layer_norm1.scale: torch.Size([768])\n",
      "transformer_blocks.7.layer_norm1.shift: torch.Size([768])\n",
      "transformer_blocks.7.layer_norm2.scale: torch.Size([768])\n",
      "transformer_blocks.7.layer_norm2.shift: torch.Size([768])\n",
      "transformer_blocks.8.attention.W_query.weight: torch.Size([768, 768])\n",
      "transformer_blocks.8.attention.W_key.weight: torch.Size([768, 768])\n",
      "transformer_blocks.8.attention.W_value.weight: torch.Size([768, 768])\n",
      "transformer_blocks.8.attention.output_projection.weight: torch.Size([768, 768])\n",
      "transformer_blocks.8.attention.output_projection.bias: torch.Size([768])\n",
      "transformer_blocks.8.feed_forward.layers.0.weight: torch.Size([3072, 768])\n",
      "transformer_blocks.8.feed_forward.layers.0.bias: torch.Size([3072])\n",
      "transformer_blocks.8.feed_forward.layers.2.weight: torch.Size([768, 3072])\n",
      "transformer_blocks.8.feed_forward.layers.2.bias: torch.Size([768])\n",
      "transformer_blocks.8.layer_norm1.scale: torch.Size([768])\n",
      "transformer_blocks.8.layer_norm1.shift: torch.Size([768])\n",
      "transformer_blocks.8.layer_norm2.scale: torch.Size([768])\n",
      "transformer_blocks.8.layer_norm2.shift: torch.Size([768])\n",
      "transformer_blocks.9.attention.W_query.weight: torch.Size([768, 768])\n",
      "transformer_blocks.9.attention.W_key.weight: torch.Size([768, 768])\n",
      "transformer_blocks.9.attention.W_value.weight: torch.Size([768, 768])\n",
      "transformer_blocks.9.attention.output_projection.weight: torch.Size([768, 768])\n",
      "transformer_blocks.9.attention.output_projection.bias: torch.Size([768])\n",
      "transformer_blocks.9.feed_forward.layers.0.weight: torch.Size([3072, 768])\n",
      "transformer_blocks.9.feed_forward.layers.0.bias: torch.Size([3072])\n",
      "transformer_blocks.9.feed_forward.layers.2.weight: torch.Size([768, 3072])\n",
      "transformer_blocks.9.feed_forward.layers.2.bias: torch.Size([768])\n",
      "transformer_blocks.9.layer_norm1.scale: torch.Size([768])\n",
      "transformer_blocks.9.layer_norm1.shift: torch.Size([768])\n",
      "transformer_blocks.9.layer_norm2.scale: torch.Size([768])\n",
      "transformer_blocks.9.layer_norm2.shift: torch.Size([768])\n",
      "transformer_blocks.10.attention.W_query.weight: torch.Size([768, 768])\n",
      "transformer_blocks.10.attention.W_key.weight: torch.Size([768, 768])\n",
      "transformer_blocks.10.attention.W_value.weight: torch.Size([768, 768])\n",
      "transformer_blocks.10.attention.output_projection.weight: torch.Size([768, 768])\n",
      "transformer_blocks.10.attention.output_projection.bias: torch.Size([768])\n",
      "transformer_blocks.10.feed_forward.layers.0.weight: torch.Size([3072, 768])\n",
      "transformer_blocks.10.feed_forward.layers.0.bias: torch.Size([3072])\n",
      "transformer_blocks.10.feed_forward.layers.2.weight: torch.Size([768, 3072])\n",
      "transformer_blocks.10.feed_forward.layers.2.bias: torch.Size([768])\n",
      "transformer_blocks.10.layer_norm1.scale: torch.Size([768])\n",
      "transformer_blocks.10.layer_norm1.shift: torch.Size([768])\n",
      "transformer_blocks.10.layer_norm2.scale: torch.Size([768])\n",
      "transformer_blocks.10.layer_norm2.shift: torch.Size([768])\n",
      "transformer_blocks.11.attention.W_query.weight: torch.Size([768, 768])\n",
      "transformer_blocks.11.attention.W_key.weight: torch.Size([768, 768])\n",
      "transformer_blocks.11.attention.W_value.weight: torch.Size([768, 768])\n",
      "transformer_blocks.11.attention.output_projection.weight: torch.Size([768, 768])\n",
      "transformer_blocks.11.attention.output_projection.bias: torch.Size([768])\n",
      "transformer_blocks.11.feed_forward.layers.0.weight: torch.Size([3072, 768])\n",
      "transformer_blocks.11.feed_forward.layers.0.bias: torch.Size([3072])\n",
      "transformer_blocks.11.feed_forward.layers.2.weight: torch.Size([768, 3072])\n",
      "transformer_blocks.11.feed_forward.layers.2.bias: torch.Size([768])\n",
      "transformer_blocks.11.layer_norm1.scale: torch.Size([768])\n",
      "transformer_blocks.11.layer_norm1.shift: torch.Size([768])\n",
      "transformer_blocks.11.layer_norm2.scale: torch.Size([768])\n",
      "transformer_blocks.11.layer_norm2.shift: torch.Size([768])\n",
      "final_norm.scale: torch.Size([768])\n",
      "final_norm.shift: torch.Size([768])\n",
      "out_head.weight: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")\n",
    "    #print(f\"{name}:\\n{param}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhm4u5hTNVzI",
    "outputId": "a3ac346a-3dc3-42b8-bf15-d2f2b06ef79b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163009536\n",
      "Total number of trainable parameters: 163009536\n"
     ]
    }
   ],
   "source": [
    "# compute total number of params\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters()\n",
    "                        if p.requires_grad)\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "print(f\"Total number of trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-leGItDQNtq4",
    "outputId": "8963cbc1-0f57-49b2-8d24-953c77ddd7b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_emb.weight → torch.Size([50257, 768])\n",
      "position_emb.weight → torch.Size([1024, 768])\n",
      "transformer_blocks.0.attention.mask → torch.Size([1024, 1024])\n",
      "transformer_blocks.0.attention.W_query.weight → torch.Size([768, 768])\n",
      "transformer_blocks.0.attention.W_key.weight → torch.Size([768, 768])\n",
      "transformer_blocks.0.attention.W_value.weight → torch.Size([768, 768])\n",
      "transformer_blocks.0.attention.output_projection.weight → torch.Size([768, 768])\n",
      "transformer_blocks.0.attention.output_projection.bias → torch.Size([768])\n",
      "transformer_blocks.0.feed_forward.layers.0.weight → torch.Size([3072, 768])\n",
      "transformer_blocks.0.feed_forward.layers.0.bias → torch.Size([3072])\n",
      "transformer_blocks.0.feed_forward.layers.2.weight → torch.Size([768, 3072])\n",
      "transformer_blocks.0.feed_forward.layers.2.bias → torch.Size([768])\n",
      "transformer_blocks.0.layer_norm1.scale → torch.Size([768])\n",
      "transformer_blocks.0.layer_norm1.shift → torch.Size([768])\n",
      "transformer_blocks.0.layer_norm2.scale → torch.Size([768])\n",
      "transformer_blocks.0.layer_norm2.shift → torch.Size([768])\n",
      "transformer_blocks.1.attention.mask → torch.Size([1024, 1024])\n",
      "transformer_blocks.1.attention.W_query.weight → torch.Size([768, 768])\n",
      "transformer_blocks.1.attention.W_key.weight → torch.Size([768, 768])\n",
      "transformer_blocks.1.attention.W_value.weight → torch.Size([768, 768])\n",
      "transformer_blocks.1.attention.output_projection.weight → torch.Size([768, 768])\n",
      "transformer_blocks.1.attention.output_projection.bias → torch.Size([768])\n",
      "transformer_blocks.1.feed_forward.layers.0.weight → torch.Size([3072, 768])\n",
      "transformer_blocks.1.feed_forward.layers.0.bias → torch.Size([3072])\n",
      "transformer_blocks.1.feed_forward.layers.2.weight → torch.Size([768, 3072])\n",
      "transformer_blocks.1.feed_forward.layers.2.bias → torch.Size([768])\n",
      "transformer_blocks.1.layer_norm1.scale → torch.Size([768])\n",
      "transformer_blocks.1.layer_norm1.shift → torch.Size([768])\n",
      "transformer_blocks.1.layer_norm2.scale → torch.Size([768])\n",
      "transformer_blocks.1.layer_norm2.shift → torch.Size([768])\n",
      "transformer_blocks.2.attention.mask → torch.Size([1024, 1024])\n",
      "transformer_blocks.2.attention.W_query.weight → torch.Size([768, 768])\n",
      "transformer_blocks.2.attention.W_key.weight → torch.Size([768, 768])\n",
      "transformer_blocks.2.attention.W_value.weight → torch.Size([768, 768])\n",
      "transformer_blocks.2.attention.output_projection.weight → torch.Size([768, 768])\n",
      "transformer_blocks.2.attention.output_projection.bias → torch.Size([768])\n",
      "transformer_blocks.2.feed_forward.layers.0.weight → torch.Size([3072, 768])\n",
      "transformer_blocks.2.feed_forward.layers.0.bias → torch.Size([3072])\n",
      "transformer_blocks.2.feed_forward.layers.2.weight → torch.Size([768, 3072])\n",
      "transformer_blocks.2.feed_forward.layers.2.bias → torch.Size([768])\n",
      "transformer_blocks.2.layer_norm1.scale → torch.Size([768])\n",
      "transformer_blocks.2.layer_norm1.shift → torch.Size([768])\n",
      "transformer_blocks.2.layer_norm2.scale → torch.Size([768])\n",
      "transformer_blocks.2.layer_norm2.shift → torch.Size([768])\n",
      "transformer_blocks.3.attention.mask → torch.Size([1024, 1024])\n",
      "transformer_blocks.3.attention.W_query.weight → torch.Size([768, 768])\n",
      "transformer_blocks.3.attention.W_key.weight → torch.Size([768, 768])\n",
      "transformer_blocks.3.attention.W_value.weight → torch.Size([768, 768])\n",
      "transformer_blocks.3.attention.output_projection.weight → torch.Size([768, 768])\n",
      "transformer_blocks.3.attention.output_projection.bias → torch.Size([768])\n",
      "transformer_blocks.3.feed_forward.layers.0.weight → torch.Size([3072, 768])\n",
      "transformer_blocks.3.feed_forward.layers.0.bias → torch.Size([3072])\n",
      "transformer_blocks.3.feed_forward.layers.2.weight → torch.Size([768, 3072])\n",
      "transformer_blocks.3.feed_forward.layers.2.bias → torch.Size([768])\n",
      "transformer_blocks.3.layer_norm1.scale → torch.Size([768])\n",
      "transformer_blocks.3.layer_norm1.shift → torch.Size([768])\n",
      "transformer_blocks.3.layer_norm2.scale → torch.Size([768])\n",
      "transformer_blocks.3.layer_norm2.shift → torch.Size([768])\n",
      "transformer_blocks.4.attention.mask → torch.Size([1024, 1024])\n",
      "transformer_blocks.4.attention.W_query.weight → torch.Size([768, 768])\n",
      "transformer_blocks.4.attention.W_key.weight → torch.Size([768, 768])\n",
      "transformer_blocks.4.attention.W_value.weight → torch.Size([768, 768])\n",
      "transformer_blocks.4.attention.output_projection.weight → torch.Size([768, 768])\n",
      "transformer_blocks.4.attention.output_projection.bias → torch.Size([768])\n",
      "transformer_blocks.4.feed_forward.layers.0.weight → torch.Size([3072, 768])\n",
      "transformer_blocks.4.feed_forward.layers.0.bias → torch.Size([3072])\n",
      "transformer_blocks.4.feed_forward.layers.2.weight → torch.Size([768, 3072])\n",
      "transformer_blocks.4.feed_forward.layers.2.bias → torch.Size([768])\n",
      "transformer_blocks.4.layer_norm1.scale → torch.Size([768])\n",
      "transformer_blocks.4.layer_norm1.shift → torch.Size([768])\n",
      "transformer_blocks.4.layer_norm2.scale → torch.Size([768])\n",
      "transformer_blocks.4.layer_norm2.shift → torch.Size([768])\n",
      "transformer_blocks.5.attention.mask → torch.Size([1024, 1024])\n",
      "transformer_blocks.5.attention.W_query.weight → torch.Size([768, 768])\n",
      "transformer_blocks.5.attention.W_key.weight → torch.Size([768, 768])\n",
      "transformer_blocks.5.attention.W_value.weight → torch.Size([768, 768])\n",
      "transformer_blocks.5.attention.output_projection.weight → torch.Size([768, 768])\n",
      "transformer_blocks.5.attention.output_projection.bias → torch.Size([768])\n",
      "transformer_blocks.5.feed_forward.layers.0.weight → torch.Size([3072, 768])\n",
      "transformer_blocks.5.feed_forward.layers.0.bias → torch.Size([3072])\n",
      "transformer_blocks.5.feed_forward.layers.2.weight → torch.Size([768, 3072])\n",
      "transformer_blocks.5.feed_forward.layers.2.bias → torch.Size([768])\n",
      "transformer_blocks.5.layer_norm1.scale → torch.Size([768])\n",
      "transformer_blocks.5.layer_norm1.shift → torch.Size([768])\n",
      "transformer_blocks.5.layer_norm2.scale → torch.Size([768])\n",
      "transformer_blocks.5.layer_norm2.shift → torch.Size([768])\n",
      "transformer_blocks.6.attention.mask → torch.Size([1024, 1024])\n",
      "transformer_blocks.6.attention.W_query.weight → torch.Size([768, 768])\n",
      "transformer_blocks.6.attention.W_key.weight → torch.Size([768, 768])\n",
      "transformer_blocks.6.attention.W_value.weight → torch.Size([768, 768])\n",
      "transformer_blocks.6.attention.output_projection.weight → torch.Size([768, 768])\n",
      "transformer_blocks.6.attention.output_projection.bias → torch.Size([768])\n",
      "transformer_blocks.6.feed_forward.layers.0.weight → torch.Size([3072, 768])\n",
      "transformer_blocks.6.feed_forward.layers.0.bias → torch.Size([3072])\n",
      "transformer_blocks.6.feed_forward.layers.2.weight → torch.Size([768, 3072])\n",
      "transformer_blocks.6.feed_forward.layers.2.bias → torch.Size([768])\n",
      "transformer_blocks.6.layer_norm1.scale → torch.Size([768])\n",
      "transformer_blocks.6.layer_norm1.shift → torch.Size([768])\n",
      "transformer_blocks.6.layer_norm2.scale → torch.Size([768])\n",
      "transformer_blocks.6.layer_norm2.shift → torch.Size([768])\n",
      "transformer_blocks.7.attention.mask → torch.Size([1024, 1024])\n",
      "transformer_blocks.7.attention.W_query.weight → torch.Size([768, 768])\n",
      "transformer_blocks.7.attention.W_key.weight → torch.Size([768, 768])\n",
      "transformer_blocks.7.attention.W_value.weight → torch.Size([768, 768])\n",
      "transformer_blocks.7.attention.output_projection.weight → torch.Size([768, 768])\n",
      "transformer_blocks.7.attention.output_projection.bias → torch.Size([768])\n",
      "transformer_blocks.7.feed_forward.layers.0.weight → torch.Size([3072, 768])\n",
      "transformer_blocks.7.feed_forward.layers.0.bias → torch.Size([3072])\n",
      "transformer_blocks.7.feed_forward.layers.2.weight → torch.Size([768, 3072])\n",
      "transformer_blocks.7.feed_forward.layers.2.bias → torch.Size([768])\n",
      "transformer_blocks.7.layer_norm1.scale → torch.Size([768])\n",
      "transformer_blocks.7.layer_norm1.shift → torch.Size([768])\n",
      "transformer_blocks.7.layer_norm2.scale → torch.Size([768])\n",
      "transformer_blocks.7.layer_norm2.shift → torch.Size([768])\n",
      "transformer_blocks.8.attention.mask → torch.Size([1024, 1024])\n",
      "transformer_blocks.8.attention.W_query.weight → torch.Size([768, 768])\n",
      "transformer_blocks.8.attention.W_key.weight → torch.Size([768, 768])\n",
      "transformer_blocks.8.attention.W_value.weight → torch.Size([768, 768])\n",
      "transformer_blocks.8.attention.output_projection.weight → torch.Size([768, 768])\n",
      "transformer_blocks.8.attention.output_projection.bias → torch.Size([768])\n",
      "transformer_blocks.8.feed_forward.layers.0.weight → torch.Size([3072, 768])\n",
      "transformer_blocks.8.feed_forward.layers.0.bias → torch.Size([3072])\n",
      "transformer_blocks.8.feed_forward.layers.2.weight → torch.Size([768, 3072])\n",
      "transformer_blocks.8.feed_forward.layers.2.bias → torch.Size([768])\n",
      "transformer_blocks.8.layer_norm1.scale → torch.Size([768])\n",
      "transformer_blocks.8.layer_norm1.shift → torch.Size([768])\n",
      "transformer_blocks.8.layer_norm2.scale → torch.Size([768])\n",
      "transformer_blocks.8.layer_norm2.shift → torch.Size([768])\n",
      "transformer_blocks.9.attention.mask → torch.Size([1024, 1024])\n",
      "transformer_blocks.9.attention.W_query.weight → torch.Size([768, 768])\n",
      "transformer_blocks.9.attention.W_key.weight → torch.Size([768, 768])\n",
      "transformer_blocks.9.attention.W_value.weight → torch.Size([768, 768])\n",
      "transformer_blocks.9.attention.output_projection.weight → torch.Size([768, 768])\n",
      "transformer_blocks.9.attention.output_projection.bias → torch.Size([768])\n",
      "transformer_blocks.9.feed_forward.layers.0.weight → torch.Size([3072, 768])\n",
      "transformer_blocks.9.feed_forward.layers.0.bias → torch.Size([3072])\n",
      "transformer_blocks.9.feed_forward.layers.2.weight → torch.Size([768, 3072])\n",
      "transformer_blocks.9.feed_forward.layers.2.bias → torch.Size([768])\n",
      "transformer_blocks.9.layer_norm1.scale → torch.Size([768])\n",
      "transformer_blocks.9.layer_norm1.shift → torch.Size([768])\n",
      "transformer_blocks.9.layer_norm2.scale → torch.Size([768])\n",
      "transformer_blocks.9.layer_norm2.shift → torch.Size([768])\n",
      "transformer_blocks.10.attention.mask → torch.Size([1024, 1024])\n",
      "transformer_blocks.10.attention.W_query.weight → torch.Size([768, 768])\n",
      "transformer_blocks.10.attention.W_key.weight → torch.Size([768, 768])\n",
      "transformer_blocks.10.attention.W_value.weight → torch.Size([768, 768])\n",
      "transformer_blocks.10.attention.output_projection.weight → torch.Size([768, 768])\n",
      "transformer_blocks.10.attention.output_projection.bias → torch.Size([768])\n",
      "transformer_blocks.10.feed_forward.layers.0.weight → torch.Size([3072, 768])\n",
      "transformer_blocks.10.feed_forward.layers.0.bias → torch.Size([3072])\n",
      "transformer_blocks.10.feed_forward.layers.2.weight → torch.Size([768, 3072])\n",
      "transformer_blocks.10.feed_forward.layers.2.bias → torch.Size([768])\n",
      "transformer_blocks.10.layer_norm1.scale → torch.Size([768])\n",
      "transformer_blocks.10.layer_norm1.shift → torch.Size([768])\n",
      "transformer_blocks.10.layer_norm2.scale → torch.Size([768])\n",
      "transformer_blocks.10.layer_norm2.shift → torch.Size([768])\n",
      "transformer_blocks.11.attention.mask → torch.Size([1024, 1024])\n",
      "transformer_blocks.11.attention.W_query.weight → torch.Size([768, 768])\n",
      "transformer_blocks.11.attention.W_key.weight → torch.Size([768, 768])\n",
      "transformer_blocks.11.attention.W_value.weight → torch.Size([768, 768])\n",
      "transformer_blocks.11.attention.output_projection.weight → torch.Size([768, 768])\n",
      "transformer_blocks.11.attention.output_projection.bias → torch.Size([768])\n",
      "transformer_blocks.11.feed_forward.layers.0.weight → torch.Size([3072, 768])\n",
      "transformer_blocks.11.feed_forward.layers.0.bias → torch.Size([3072])\n",
      "transformer_blocks.11.feed_forward.layers.2.weight → torch.Size([768, 3072])\n",
      "transformer_blocks.11.feed_forward.layers.2.bias → torch.Size([768])\n",
      "transformer_blocks.11.layer_norm1.scale → torch.Size([768])\n",
      "transformer_blocks.11.layer_norm1.shift → torch.Size([768])\n",
      "transformer_blocks.11.layer_norm2.scale → torch.Size([768])\n",
      "transformer_blocks.11.layer_norm2.shift → torch.Size([768])\n",
      "final_norm.scale → torch.Size([768])\n",
      "final_norm.shift → torch.Size([768])\n",
      "out_head.weight → torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "# can retrieve weights and layer names with state_dict()\n",
    "for name, weights in model.state_dict().items():\n",
    "    print(f\"{name} → {weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYmRzS5SPSwX"
   },
   "source": [
    "later we will use `weight tying`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kIToC8RoOpiM",
    "outputId": "2bd6a7d3-ba9f-429f-dd30-36f6613a99e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token embedding layer shape:  torch.Size([50257, 768])\n",
      "output embedding layer shape:  torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "# weight tying\n",
    "print(\"token embedding layer shape: \", model.token_emb.weight.shape)\n",
    "print(\"output embedding layer shape: \", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWt2rt2NPPk9",
    "outputId": "5e30fd94-6580-44ed-9ea5-4e9a1391af43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in GPT-2: 124412160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = (\n",
    "    total_params - sum(p.numel()\n",
    "    for p in model.out_head.parameters())\n",
    ")\n",
    "\n",
    "print(f\"Total number of parameters in GPT-2: {total_params_gpt2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAPuiic4Pxb3"
   },
   "source": [
    "Let's compute the memory requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oHyCpqVePnHg",
    "outputId": "c0e39fe5-96a7-49ca-f904-84d3f49c4704"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of GPT-2 in MB: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of GPT-2 in MB: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGnCW2AwQWZt"
   },
   "source": [
    "# Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "rMNfdh6XP9aX"
   },
   "outputs": [],
   "source": [
    "def generate_text_simple(model,\n",
    "                         input_batch, # [batch, num_tokens]\n",
    "                         max_new_tokens, # numbers of new tokens to be predicted\n",
    "                         context_size):\n",
    "  for _ in range(max_new_tokens):\n",
    "    # crop current context if it exceeds the supported context_size\n",
    "    crop_input_batch = input_batch[:, -context_size:]\n",
    "\n",
    "    # predict next token\n",
    "    with torch.no_grad():\n",
    "      logits = model(crop_input_batch)\n",
    "\n",
    "    # consider only logits of the last token\n",
    "    logits = logits[:, -1, :] # (batch, n_tokens, vocab_size) -> (batch, vocab_size)\n",
    "    probas = torch.softmax(logits, dim=-1) # (batch, vocab_size)\n",
    "    predicted_tokens = torch.argmax(probas, dim=-1, keepdim=True) # (batch, 1)\n",
    "    # update input_batch (append predicted tokens to the sequences)\n",
    "    input_batch = torch.cat([input_batch, predicted_tokens], dim=-1) # [batch, num_tokens+1]\n",
    "\n",
    "  return input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e72rAU1ATmGE",
    "outputId": "aceed3c4-76f5-4846-eb39-a4233630a477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded:  [17250, 11, 616, 1438, 318]\n",
      "encoded tensor: \n",
      " tensor([[17250,    11,   616,  1438,   318]])\n",
      "encoded tensor shape:  torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "# example setup\n",
    "start_context = \"Hi, my name is\"\n",
    "encoded = bpe_tokenizer.encode(start_context)\n",
    "print(\"encoded: \", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded tensor: \\n\", encoded_tensor)\n",
    "print(\"encoded tensor shape: \", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6UXNhDQUYJd"
   },
   "source": [
    "Since, we now don't have to train, let's put the model in `.eval()` mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UMT1IrZ8T9ZO",
    "outputId": "7b1d7e27-28f7-4360-d075-a4b723a53231"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length:  15\n",
      "output: \n",
      " tensor([[17250,    11,   616,  1438,   318,  9758, 42360, 37069, 27391, 36602,\n",
      "         13152, 33906, 36790, 22446, 42465]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "output = generate_text_simple(\n",
    "    model=model,\n",
    "    input_batch=encoded_tensor,\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"output length: \", output.shape[1])\n",
    "print(\"output: \\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jecz4g7SU40o",
    "outputId": "c0e9f674-95be-4438-9507-c162c8e5c9f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, my name is constitutionaliageplet Gent stricterMediaVaMetal(). recurrent\n"
     ]
    }
   ],
   "source": [
    "decoded_text = bpe_tokenizer.decode(output.squeeze().tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FL2jl7CcVF1v",
    "outputId": "c2b28b08-715a-415f-f351-a82af206040d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook runtime: 0 min 7.67 sec\n"
     ]
    }
   ],
   "source": [
    "notebook_end_time = time.time()\n",
    "runtime_in_seconds = notebook_end_time - notebook_start_time\n",
    "\n",
    "# format as minutes and seconds\n",
    "minutes, seconds = divmod(runtime_in_seconds, 60)\n",
    "print(f\"Notebook runtime: {int(minutes)} min {seconds:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "LLMs from scratch (uv)",
   "language": "python",
   "name": "llms_uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
