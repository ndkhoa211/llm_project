{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qpacV6IHuWLH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9cab318-9457-4c91-984d-94b9e0c65b5f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'vocab_size': 50257,\n",
              " 'context_length': 1024,\n",
              " 'drop_rate': 0.0,\n",
              " 'qkv_bias': True,\n",
              " 'emb_dim': 1280,\n",
              " 'n_layers': 36,\n",
              " 'n_heads': 20}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "CHOOSE_MODEL = \"gpt2-large (774M)\"\n",
        "INPUT_PROMPT = \"Every effort moves\"\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"drop_rate\": 0.0,        # Dropout rate\n",
        "    \"qkv_bias\": True         # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "BASE_CONFIG"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. GPT2 Architecture"
      ],
      "metadata": {
        "id": "xrAAMF3fugyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1\n",
        "\n",
        "- Let's load:\n",
        "  1. `MultiHeadAttention` class\n",
        "  2. `LayerNorm` class\n",
        "  3. `GELU` class\n",
        "  4. `FeedForward` class\n",
        "  5. `TransformerBlock` class\n",
        "  6. `GPT2Model` class"
      ],
      "metadata": {
        "id": "NbVsuqzrulez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_embedding_dim,\n",
        "                 output_embedding_dim,\n",
        "                 context_length,\n",
        "                 dropout,\n",
        "                 num_heads,\n",
        "                 qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (output_embedding_dim % num_heads == 0), \\\n",
        "            \"output_embedding_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.output_embedding_dim = output_embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = output_embedding_dim // num_heads\n",
        "        self.W_query = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
        "                                 bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
        "                               bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
        "                                 bias=qkv_bias)\n",
        "        self.output_projection = nn.Linear(output_embedding_dim,\n",
        "                                           output_embedding_dim)  # to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        batch, num_tokens, input_embedding_dim = inputs.shape\n",
        "\n",
        "        # qkv shapes : (batch, num_tokens, output_embedding_dim)\n",
        "        keys = self.W_key(inputs)\n",
        "        values = self.W_value(inputs)\n",
        "        queries = self.W_query(inputs)\n",
        "\n",
        "        # qkv shapes : (batch, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # qkv shapes : (batch, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "\n",
        "        # compute attention scores for each head\n",
        "        attention_scores = queries @ keys.transpose(3, 2)\n",
        "        attention_scores.masked_fill_(\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], - torch.inf)\n",
        "\n",
        "        # compute attention weights + dropout\n",
        "        masked_attention_weight = torch.softmax(\n",
        "            attention_scores / (keys.shape[-1] ** 0.5),\n",
        "            dim=-1)\n",
        "        masked_attention_dropout_weight = self.dropout(masked_attention_weight)\n",
        "\n",
        "        # compute context vectors\n",
        "        # shape : (batch, num_tokens, num_heads, head_dim)\n",
        "        context_vector = (masked_attention_dropout_weight @ values).transpose(1, 2)\n",
        "\n",
        "        # combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        # shape : (batch, num_tokens, output_embedding_dim)\n",
        "        context_vector = context_vector.contiguous().view(\n",
        "            batch, num_tokens, self.output_embedding_dim)\n",
        "\n",
        "        # linear projection (optional)\n",
        "        context_vector = self.output_projection(context_vector)\n",
        "\n",
        "        return context_vector\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.epsilon = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1,\n",
        "                    unbiased=False,  # Bessel's correction (n-1)\n",
        "                    keepdim=True)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.epsilon)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(config[\"emb_dim\"],  # 768\n",
        "                      4 * config[\"emb_dim\"]),  # 3072\n",
        "            GELU(),  # 3072\n",
        "            nn.Linear(4 * config[\"emb_dim\"],  # 3072\n",
        "                      config[\"emb_dim\"])  # 768\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(input_embedding_dim=config[\"emb_dim\"],\n",
        "                                            output_embedding_dim=config[\"emb_dim\"],\n",
        "                                            context_length=config[\"context_length\"],\n",
        "                                            dropout=config[\"drop_rate\"],\n",
        "                                            num_heads=config[\"n_heads\"],\n",
        "                                            qkv_bias=config[\"qkv_bias\"])\n",
        "        self.feed_forward = FeedForward(config)\n",
        "        self.layer_norm1 = LayerNorm(config[\"emb_dim\"])\n",
        "        self.layer_norm2 = LayerNorm(config[\"emb_dim\"])\n",
        "        self.drop_skip = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # skip connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.layer_norm1(x)\n",
        "        x = self.attention(x)  # shape: [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_skip(x)\n",
        "        x = shortcut + x  # skip connection\n",
        "\n",
        "        # skip connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.layer_norm2(x)\n",
        "        x = self.feed_forward(x)\n",
        "        x = self.drop_skip(x)\n",
        "        x = shortcut + x  # skip connection\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT2Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(config[\"vocab_size\"],\n",
        "                                      config[\"emb_dim\"])\n",
        "        self.position_emb = nn.Embedding(config[\"context_length\"],\n",
        "                                         config[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "        self.transformer_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(config) for _ in range(config[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(config[\"emb_dim\"])\n",
        "\n",
        "        self.out_head = nn.Linear(config[\"emb_dim\"],\n",
        "                                  config[\"vocab_size\"],\n",
        "                                  bias=False)\n",
        "\n",
        "    def forward(self, input_token):\n",
        "        batch_size, sequence_length = input_token.shape\n",
        "        token_embeds = self.token_emb(input_token)\n",
        "        position_embeds = self.position_emb(\n",
        "            torch.arange(sequence_length,\n",
        "                         device=input_token.device))\n",
        "        embeds = token_embeds + position_embeds\n",
        "        x = self.drop_emb(embeds)\n",
        "        x = self.transformer_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GPT2DatasetV1(Dataset):\n",
        "  def __init__(self,\n",
        "               text,\n",
        "               tokenizer,\n",
        "               context_length, # length of each input vector\n",
        "               stride # chunk the text into overlapping sequence of context_length\n",
        "               ):\n",
        "    self.input_id_vectors = []\n",
        "    self.target_id_vectors = []\n",
        "\n",
        "    # tokenize the entire text\n",
        "    token_list = tokenizer.encode(text)\n",
        "\n",
        "    # append input and target vectors\n",
        "    for i in range(0, len(token_list) - context_length, stride):\n",
        "      input_vector = token_list[i:i+context_length]\n",
        "      target_vector = token_list[i+1:i+context_length+1]\n",
        "      self.input_id_vectors.append(torch.tensor(input_vector))\n",
        "      self.target_id_vectors.append(torch.tensor(target_vector))\n",
        "\n",
        "  # get the number of input vectors\n",
        "  def __len__(self):\n",
        "    return len(self.input_id_vectors)\n",
        "\n",
        "  # return the (input vector, target vector) pair\n",
        "  def __getitem__(self, id):\n",
        "    return self.input_id_vectors[id], self.target_id_vectors[id]\n",
        "\n",
        "\n",
        "\n",
        "def create_dataloader_V1(text,\n",
        "                 batch_size=4,\n",
        "                 context_length=256,\n",
        "                 stride=128,\n",
        "                 shuffle=True, # shuffle dataset\n",
        "                 drop_last=True, # drop last batch if it not equal required size\n",
        "                 num_workers=0 # number of CPU processes for preprocessing\n",
        "                 ):\n",
        "\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "  dataset = GPT2DatasetV1(text=text,\n",
        "                          tokenizer=tokenizer,\n",
        "                          context_length=context_length,\n",
        "                          stride=stride)\n",
        "\n",
        "  dataloader = DataLoader(dataset=dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=shuffle,\n",
        "                          drop_last=drop_last,\n",
        "                          num_workers=num_workers)\n",
        "\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "y4Py46X4uaay"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Training and Evaluation Functions\n",
        "\n",
        "- Let's load:\n",
        "  1. `train_model_simple`: without warmup rate, cosine decay, gradient clipping\n",
        "  2. `train_model`\n",
        "  3. `evaluate_model`"
      ],
      "metadata": {
        "id": "K9gMF509u4Gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_model_simple(model,\n",
        "                       train_loader,\n",
        "                       val_loader,\n",
        "                       optimizer,\n",
        "                       device,\n",
        "                       num_epochs,\n",
        "                       eval_freq,\n",
        "                       eval_iter,\n",
        "                       start_context,\n",
        "                       tokenizer):\n",
        "\n",
        "  # initialize lists to track losses and tokens seen\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "  track_tokens_seen = []\n",
        "  token_seen = 0\n",
        "  global_step = -1\n",
        "\n",
        "  # main training loop - iterate over training epochs\n",
        "  for epoch in range(num_epochs):\n",
        "    # set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # iterate over batches in each training epoch\n",
        "    for input_batch, target_batch in train_loader:\n",
        "      # reset loss gradients from previous batch iteration\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # calculate loss on current batch\n",
        "      loss = calc_loss_batch(input_batch,\n",
        "                             target_batch,\n",
        "                             model,\n",
        "                             device)\n",
        "\n",
        "      # backward pass to calculate loss gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # update model weights using loss gradients\n",
        "      optimizer.step()\n",
        "      token_seen += input_batch.numel()\n",
        "      global_step += 1\n",
        "\n",
        "      # optional evaluation step\n",
        "      if global_step % eval_freq == 0:\n",
        "        train_loss, val_loss = evaluate_model(model,\n",
        "                                              train_loader,\n",
        "                                              val_loader,\n",
        "                                              device,\n",
        "                                              eval_iter)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        track_tokens_seen.append(token_seen)\n",
        "        # print training and evaluation set loss\n",
        "        print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "              f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "    # generative sample text for visual inspection\n",
        "    generate_and_print_sample(model,\n",
        "                              tokenizer,\n",
        "                              device,\n",
        "                              start_context)\n",
        "\n",
        "  return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ORIG_BOOK_VERSION = False\n",
        "def train_model(model,\n",
        "                train_loader,\n",
        "                val_loader,\n",
        "                optimizer,\n",
        "                device,\n",
        "                n_epochs,\n",
        "                eval_freq,\n",
        "                eval_iter,\n",
        "                start_context,\n",
        "                tokenizer,\n",
        "                warmup_steps,\n",
        "                initial_lr=3e-05,\n",
        "                min_lr=1e-6):\n",
        "\n",
        "  train_losses, val_losses = [], []\n",
        "  track_tokens_seen, track_lrs = [], []\n",
        "\n",
        "  token_seen = 0\n",
        "  global_step = -1\n",
        "\n",
        "  # retrieve the maximum/peak learning rate from the optimizer\n",
        "  peak_lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "  # calculate the total number of iterations in the training process\n",
        "  total_training_steps = len(train_loader) * n_epochs\n",
        "\n",
        "  # calculate the learning rate increment during the warmup phase\n",
        "  lr_increment = (peak_lr - initial_lr) / warmup_steps\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    for input_batch, target_batch in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      global_step += 1\n",
        "\n",
        "      # adjust the learning rate based on the current phase (warmup or cosine)\n",
        "      if global_step < warmup_steps:\n",
        "        lr = initial_lr + global_step * lr_increment\n",
        "      else:\n",
        "        # cosine annealing after warmup\n",
        "        progress = ((global_step - warmup_steps) /\n",
        "                    (total_training_steps - warmup_steps))\n",
        "        lr = (min_lr +\n",
        "         (peak_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress)))\n",
        "\n",
        "      # apply the calculated learning rate to the optimizer\n",
        "      for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "      track_lrs.append(lr) # store the current learning rate\n",
        "\n",
        "      # calculate and backpropagate the loss\n",
        "      loss = calc_loss_batch(input_batch,\n",
        "                             target_batch,\n",
        "                             model,\n",
        "                             device)\n",
        "      loss.backward()\n",
        "\n",
        "      # apply gradient clipping after the warmup phase to avoid exploding gradients\n",
        "      if ORIG_BOOK_VERSION:\n",
        "        if global_step > warmup_steps:\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "      else:\n",
        "        # the book originally used global_step > warmup_steps, which led to a skipped clipping step after warmup\n",
        "        if global_step >= warmup_steps:\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "      optimizer.step()\n",
        "      token_seen += input_batch.numel()\n",
        "\n",
        "      # periodically evaluate the model on the training and validation sets\n",
        "      if global_step % eval_freq == 0:\n",
        "        train_loss, val_loss = evaluate_model(model,\n",
        "                                              train_loader,\n",
        "                                              val_loader,\n",
        "                                              device,\n",
        "                                              eval_iter)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        track_tokens_seen.append(token_seen)\n",
        "        # print the current losses\n",
        "        print(f\"Ep {epoch+1} (Iter {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, \"\n",
        "                      f\"Val loss {val_loss:.3f}\")\n",
        "\n",
        "    # generate and print a sample from the model to monitor progess\n",
        "    generate_and_print_sample(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=device,\n",
        "        start_context=start_context\n",
        "    )\n",
        "\n",
        "  return train_losses, val_losses, track_tokens_seen, track_lrs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(model,\n",
        "                    train_loader,\n",
        "                    val_loader,\n",
        "                    device,\n",
        "                    eval_iter):\n",
        "  # set model to evaluation mode\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    # calculate loss\n",
        "    train_loss = calc_loss_loader(train_loader,\n",
        "                                  model,\n",
        "                                  device,\n",
        "                                  num_batches=eval_iter)\n",
        "    val_loss = calc_loss_loader(val_loader,\n",
        "                                model,\n",
        "                                device,\n",
        "                                num_batches=eval_iter)\n",
        "\n",
        "  # set model back to training mode\n",
        "  model.train()\n",
        "  return train_loss, val_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############################### utils functions #############################\n",
        "############################### utils functions #############################\n",
        "############################### utils functions #############################\n",
        "############################### utils functions #############################\n",
        "############################### utils functions #############################\n",
        "\n",
        "\n",
        "def calc_loss_batch(input_batch,\n",
        "                    target_batch,\n",
        "                    model,\n",
        "                    device):\n",
        "  input_batch = input_batch.to(device)\n",
        "  target_batch = target_batch.to(device)\n",
        "\n",
        "  logits = model(input_batch)\n",
        "  loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1),\n",
        "                                           target_batch.flatten())\n",
        "  return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(dataloader,\n",
        "                     model,\n",
        "                     device,\n",
        "                     num_batches=None):\n",
        "  total_loss = 0.\n",
        "  if len(dataloader) == 0:\n",
        "    return float(\"nan\")\n",
        "  elif num_batches is None:\n",
        "    num_batches = len(dataloader)\n",
        "  else:\n",
        "    # reduce the number of batches to match the total number of batches in the data loader\n",
        "    # if num_batches exceeds the number of batches in the data loader\n",
        "    num_batches = min(num_batches, len(dataloader))\n",
        "  for i, (input_batch, target_batch) in enumerate(dataloader):\n",
        "    if i < num_batches:\n",
        "      loss = calc_loss_batch(input_batch,\n",
        "                             target_batch,\n",
        "                             model,\n",
        "                             device)\n",
        "      total_loss += loss.item()\n",
        "    else:\n",
        "      break\n",
        "  return total_loss / num_batches\n",
        "\n",
        "\n",
        "\n",
        "def generate_and_print_sample(model,\n",
        "                              tokenizer,\n",
        "                              device,\n",
        "                              start_context):\n",
        "  # set model to evaluation mode\n",
        "  model.eval()\n",
        "  context_size = model.position_emb.weight.shape[0]\n",
        "  encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "  with torch.no_grad():\n",
        "    token_ids = generate_text_simple(model=model,\n",
        "                                     input_batch=encoded,\n",
        "                                     max_new_tokens=50,\n",
        "                                     context_size=context_size)\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \")) # compact print format\n",
        "  # set model back to training mode\n",
        "  model.train()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "  encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "  # turn the list of token IDs into tensor with batch dimension\n",
        "  encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "  return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(encoded_tensor, tokenizer):\n",
        "  # turn tensor without batch dimension to list\n",
        "  token_ids = encoded_tensor.squeeze(0).tolist()\n",
        "  text = tokenizer.decode(token_ids)\n",
        "  return text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_text_simple(model,\n",
        "                         input_batch,  # [batch, num_tokens]\n",
        "                         max_new_tokens,  # numbers of new tokens to be predicted\n",
        "                         context_size):\n",
        "    for _ in range(max_new_tokens):\n",
        "        # crop current context if it exceeds the supported context_size\n",
        "        crop_input_batch = input_batch[:, -context_size:]\n",
        "\n",
        "        # predict next token\n",
        "        with torch.no_grad():\n",
        "            logits = model(crop_input_batch)\n",
        "\n",
        "        # consider only logits of the last token\n",
        "        logits = logits[:, -1, :]  # (batch, n_tokens, vocab_size) -> (batch, vocab_size)\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "        predicted_tokens = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "        # update input_batch (append predicted tokens to the sequences)\n",
        "        input_batch = torch.cat([input_batch, predicted_tokens], dim=-1)  # [batch, num_tokens+1]\n",
        "\n",
        "    return input_batch\n",
        "\n",
        "\n",
        "\n",
        "def generate_text(model,\n",
        "                  input_batch,\n",
        "                  max_new_tokens,\n",
        "                  context_size,\n",
        "                  temperature=0.0,\n",
        "                  top_k=None,\n",
        "                  eos_id=None):\n",
        "  for _ in range(max_new_tokens):\n",
        "    # crop current context if it exceeds the supported context_size\n",
        "    crop_input_batch = input_batch[:, -context_size:]\n",
        "\n",
        "    # predict next token\n",
        "    with torch.no_grad():\n",
        "      logits = model(crop_input_batch)\n",
        "\n",
        "    # consider only logits of the last token\n",
        "    logits = logits[:, -1, :] # (batch, n_tokens, vocab_size) -> (batch, vocab_size)\n",
        "\n",
        "    # NEW: filter logits with top_k sampling\n",
        "    if top_k is not None:\n",
        "      # keep only top_k values\n",
        "      top_logits, _ = torch.topk(logits, top_k)\n",
        "      min_val = top_logits[:, -1] # min value among the top_k values\n",
        "      # all values other than top_k values will be set to -inf\n",
        "      logits = torch.where(logits < min_val,\n",
        "                           torch.tensor(-torch.inf).to(logits.device),\n",
        "                           logits)\n",
        "\n",
        "    # NEW: temperature scaling\n",
        "    if temperature > 0.0:\n",
        "      logits = logits / temperature\n",
        "\n",
        "      probas = torch.softmax(logits, dim=-1) # (batch, vocab_size)\n",
        "      predicted_tokens = torch.multinomial(probas, num_samples=1) # (batch, 1)\n",
        "\n",
        "    else: # same as before\n",
        "      #probas = torch.softmax(logits, dim=-1) # (batch, vocab_size)\n",
        "      predicted_tokens = torch.argmax(logits, dim=-1, keepdim=True) # (batch, 1)\n",
        "\n",
        "    if predicted_tokens == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "    # update input_batch (append predicted tokens to the sequences)\n",
        "    input_batch = torch.cat([input_batch, predicted_tokens], dim=1) # [batch, num_tokens+1]\n",
        "\n",
        "  return input_batch\n",
        "############################### utils functions #############################\n",
        "############################### utils functions #############################\n",
        "############################### utils functions #############################\n",
        "############################### utils functions #############################\n",
        "############################### utils functions #############################"
      ],
      "metadata": {
        "id": "7aAorxrHuaYL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 LoRA layer"
      ],
      "metadata": {
        "id": "2ujalGwqvNov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "\n",
        "class LoRALayer(torch.nn.Module):\n",
        "  def __init__(self, in_dim, out_dim, rank, alpha):\n",
        "    super().__init__()\n",
        "    self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
        "    # Kaiming/He uniform initialization, similar to standard weight initialization\n",
        "    torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
        "    self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.alpha * (x @ self.A @ self.B)\n",
        "    return x\n",
        "\n",
        "\n",
        "class LinearLayerWithLoRA(torch.nn.Module):\n",
        "  def __init__(self, linear, rank, alpha):\n",
        "    super().__init__()\n",
        "    self.linear = linear\n",
        "    self.lora = LoRALayer(linear.in_features,\n",
        "                          linear.out_features,\n",
        "                          rank,\n",
        "                          alpha)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear(x) + self.lora(x)\n",
        "\n",
        "\n",
        "\n",
        "def replace_linear_with_lora(model, rank, alpha):\n",
        "  for name, module in model.named_children():\n",
        "    if isinstance(module, torch.nn.Linear):\n",
        "      print(f\"Replacing {name} with LinearLayerWithLoRA\")\n",
        "      setattr(model, name, LinearLayerWithLoRA(module, rank, alpha))\n",
        "    else:\n",
        "      # recursively apply the same function to child modules\n",
        "      replace_linear_with_lora(module, rank, alpha)"
      ],
      "metadata": {
        "id": "gkGLSDbguaVh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Load weights"
      ],
      "metadata": {
        "id": "Ir_ddY8UR_9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.position_emb.weight = assign(gpt.position_emb.weight, params['wpe'])\n",
        "    gpt.token_emb.weight = assign(gpt.token_emb.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.transformer_blocks[b].attention.W_query.weight = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_query.weight, q_w.T)\n",
        "        gpt.transformer_blocks[b].attention.W_key.weight = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_key.weight, k_w.T)\n",
        "        gpt.transformer_blocks[b].attention.W_value.weight = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.transformer_blocks[b].attention.W_query.bias = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_query.bias, q_b)\n",
        "        gpt.transformer_blocks[b].attention.W_key.bias = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_key.bias, k_b)\n",
        "        gpt.transformer_blocks[b].attention.W_value.bias = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_value.bias, v_b)\n",
        "\n",
        "        gpt.transformer_blocks[b].attention.output_projection.weight = assign(\n",
        "            gpt.transformer_blocks[b].attention.output_projection.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.transformer_blocks[b].attention.output_projection.bias = assign(\n",
        "            gpt.transformer_blocks[b].attention.output_projection.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.transformer_blocks[b].feed_forward.layers[0].weight = assign(\n",
        "            gpt.transformer_blocks[b].feed_forward.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.transformer_blocks[b].feed_forward.layers[0].bias = assign(\n",
        "            gpt.transformer_blocks[b].feed_forward.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.transformer_blocks[b].feed_forward.layers[2].weight = assign(\n",
        "            gpt.transformer_blocks[b].feed_forward.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.transformer_blocks[b].feed_forward.layers[2].bias = assign(\n",
        "            gpt.transformer_blocks[b].feed_forward.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.transformer_blocks[b].layer_norm1.scale = assign(\n",
        "            gpt.transformer_blocks[b].layer_norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.transformer_blocks[b].layer_norm1.shift = assign(\n",
        "            gpt.transformer_blocks[b].layer_norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.transformer_blocks[b].layer_norm2.scale = assign(\n",
        "            gpt.transformer_blocks[b].layer_norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.transformer_blocks[b].layer_norm2.shift = assign(\n",
        "            gpt.transformer_blocks[b].layer_norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
      ],
      "metadata": {
        "id": "3YkWo22WSF6_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Dataset\n",
        "- We'll use `war-and-peace.txt` dataset"
      ],
      "metadata": {
        "id": "F8M8MdgZvwXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Gradio app"
      ],
      "metadata": {
        "id": "I5WNdNN6v2Le"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Finetune"
      ],
      "metadata": {
        "id": "fJpCE8hWR152"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Clean up GPU memory ---\n",
        "import gc\n",
        "# del train_loss, val_loss  # if you don't need them anymore\n",
        "#del train_dataloader, val_dataloader  # optional, if not reused\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()"
      ],
      "metadata": {
        "id": "yN1VxY6jRs-3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import urllib\n",
        "\n",
        "\n",
        "def download_and_load_file(file_path, url):\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            text_data = response.read().decode(\"utf-8\")\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(text_data)\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "file_path = \"instruction-data.json\"\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
        "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
        ")\n",
        "\n",
        "data = download_and_load_file(file_path, url)\n",
        "print(\"Number of entries:\", len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7B5PhFERs8E",
        "outputId": "bc3f3453-831b-4183-c884-e05911ad692f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of entries: 1100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_input(entry):\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "    )\n",
        "\n",
        "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "\n",
        "    return instruction_text + input_text"
      ],
      "metadata": {
        "id": "0T5IAZkzRs5n"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_input = format_input(data[999])\n",
        "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
        "\n",
        "print(model_input + desired_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dxds-jChRs2y",
        "outputId": "546034a2-8829-4cee-ab09-c8cb081cdeb5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is an antonym of 'complicated'?\n",
            "\n",
            "### Response:\n",
            "The correct spelling is 'Occasion.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_portion = int(len(data) * 0.85)  # 85% for training\n",
        "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
        "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
        "\n",
        "train_data = data[:train_portion]\n",
        "test_data = data[train_portion:train_portion + test_portion]\n",
        "val_data = data[train_portion + test_portion:]\n",
        "\n",
        "print(\"Training set length:\", len(train_data))\n",
        "print(\"Validation set length:\", len(val_data))\n",
        "print(\"Test set length:\", len(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQxrp3nMRs0K",
        "outputId": "1726cc49-1c7f-4303-f991-bd4d4448fea4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set length: 935\n",
            "Validation set length: 55\n",
            "Test set length: 110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "  def __init__(self, data, tokenizer):\n",
        "    self.data = data\n",
        "\n",
        "    # pre-tokenizer texts\n",
        "    self.encoded_texts = []\n",
        "    for entry in data:\n",
        "      instruction_plus_input = format_input(entry)\n",
        "      response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "      full_text = instruction_plus_input + response_text\n",
        "      self.encoded_texts.append(tokenizer.encode(full_text))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.encoded_texts[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ],
      "metadata": {
        "id": "KzV7NrBERsxu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYzZfYz7RsvO",
        "outputId": "6102371e-2412-4b63-9bc2-ccfa0dd34ff4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50256]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch,\n",
        "                      pad_token_id=50256,\n",
        "                      ignore_index=-100,\n",
        "                      allowed_max_length=None,\n",
        "                      device=\"cpu\"):\n",
        "  # find the longest sequence in the batch and increase max_length\n",
        "  # by +1 for the padding token, which indicates the end of sequence/answer\n",
        "  batch_max_length = max(len(item) + 1 for item in batch)\n",
        "\n",
        "  # pad and prepare inputs\n",
        "  inputs_lst = []\n",
        "  targets_lst = []\n",
        "\n",
        "  for item in batch:\n",
        "    new_item = item.copy()\n",
        "    # Add an <|endoftext|> token\n",
        "    new_item += [pad_token_id]\n",
        "    # Pad sequences to batch_max_length\n",
        "    padded = (new_item + [pad_token_id] * (batch_max_length - len(new_item)))\n",
        "\n",
        "    inputs = torch.tensor(padded[:-1]) # Truncate the last token for inputs\n",
        "    targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "\n",
        "    # replace all but the first padding tokens in targets by `ignore_index`\n",
        "    mask = targets == pad_token_id\n",
        "    padding_tokens_indices = torch.nonzero(mask).squeeze() # indices of the padding tokens in `targets`\n",
        "    if padding_tokens_indices.numel() > 1:\n",
        "      targets[padding_tokens_indices[1:]] = ignore_index\n",
        "\n",
        "    # optionally truncate to maximum sequence length\n",
        "    if allowed_max_length is not None:\n",
        "      inputs = inputs[:allowed_max_length]\n",
        "      targets = targets[:allowed_max_length]\n",
        "\n",
        "    inputs_lst.append(inputs)\n",
        "    targets_lst.append(targets)\n",
        "\n",
        "  # convert list of inputs to tensor and transfer to target device\n",
        "  inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "  targets_tensor = torch.stack(targets_lst).to(device)\n",
        "\n",
        "  return inputs_tensor, targets_tensor"
      ],
      "metadata": {
        "id": "umt1JUkkRssz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "customized_collate_fn = partial(\n",
        "    custom_collate_fn,\n",
        "    device=device,\n",
        "    allowed_max_length=1024\n",
        ")"
      ],
      "metadata": {
        "id": "uuSYxhFPSb-s"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(211)\n",
        "\n",
        "train_dataset = InstructionDataset(train_data, tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "val_dataset = InstructionDataset(val_data, tokenizer)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "test_dataset = InstructionDataset(test_data, tokenizer)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")"
      ],
      "metadata": {
        "id": "SOiExUO7Sb8R"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train loader:\")\n",
        "for inputs, targets in train_loader:\n",
        "    print(inputs.shape, targets.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CIcxo66Sb5q",
        "outputId": "ec09312a-238d-4f04-a763-7184de71dc8c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 57]) torch.Size([8, 57])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 58]) torch.Size([8, 58])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 57]) torch.Size([8, 57])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 79]) torch.Size([8, 79])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 55]) torch.Size([8, 55])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/\"\n",
        "    \"LLMs-from-scratch/main/ch05/\"\n",
        "    \"01_main-chapter-code/gpt_download.py\"\n",
        ")\n",
        "filename = url.split(\"/\")[-1]\n",
        "urllib.request.urlretrieve(url, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2Ld1M9RSb3I",
        "outputId": "edb00011-cd03-484c-eedc-2a53524f250b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gpt_download.py', <http.client.HTTPMessage at 0x796dc65b9910>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "model_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NBUQJZ1JSb04",
        "outputId": "f6845d0c-ed3a-429a-c3a5-d73b754cf665"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'774M'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_download import download_and_load_gpt2\n",
        "\n",
        "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
        "print(settings)\n",
        "print(params.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6oTflISSbyk",
        "outputId": "6acfbf44-6174-495c-a06c-715fd18b0668"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 190kiB/s]\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 4.49MiB/s]\n",
            "hparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 217kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 3.10G/3.10G [02:23<00:00, 21.6MiB/s]\n",
            "model.ckpt.index: 100%|██████████| 15.5k/15.5k [00:00<00:00, 29.3MiB/s]\n",
            "model.ckpt.meta: 100%|██████████| 1.38M/1.38M [00:00<00:00, 5.95MiB/s]\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 2.46MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 1280, 'n_head': 20, 'n_layer': 36}\n",
            "dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2Model(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval();"
      ],
      "metadata": {
        "id": "ASdvBTb6Rsp9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_1 = \"In the midst of winter\"\n",
        "\n",
        "token_ids = generate_text(\n",
        "    model=model,\n",
        "    input_batch=text_to_token_ids(text_1, tokenizer),\n",
        "    max_new_tokens=50,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        "    temperature = 2.0,\n",
        "    top_k = 10\n",
        ")\n",
        "\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFxfxoaaRsng",
        "outputId": "f8aeea69-fdcb-464f-d902-238b5f4d3219"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the midst of winter in Europe and in the Ural Mountains we are witnessing a major ice sheet advance,\" he said in Moscow last week. \"We are talking of a major event with an ice thickness approaching three kilometres. It means the end for this region and also the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(211)\n",
        "\n",
        "input_text = format_input(val_data[50])\n",
        "print(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUiL7aUURshh",
        "outputId": "2385a08d-6622-4699-82bc-06666724e4b7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Label whether the following statements are true or false.\n",
            "\n",
            "### Input:\n",
            "The moon is a planet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = generate_text(\n",
        "    model=model,\n",
        "    input_batch=text_to_token_ids(input_text, tokenizer),\n",
        "    max_new_tokens=50,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        "    eos_id=50256,\n",
        ")\n",
        "generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "\n",
        "\n",
        "response_text = (\n",
        "    generated_text[len(input_text):]\n",
        "    .replace(\"### Response:\", \"\")\n",
        "    .strip()\n",
        ")\n",
        "print(response_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee-Ba6wfS93s",
        "outputId": "e741a297-e9f8-40b7-84ed-7f26e5c05d1c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Output:\n",
            "\n",
            "The moon is a planet.\n",
            "\n",
            "### Label:\n",
            "\n",
            "The moon is a planet.\n",
            "\n",
            "### Input:\n",
            "\n",
            "The moon is a planet.\n",
            "\n",
            "### Output:\n",
            "\n",
            "The moon is a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "\n",
        "torch.manual_seed(211)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f8JD_lDS91E",
        "outputId": "70e87df3-a235-4225-cb77-f4fe5e069557"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 3.6910677433013914\n",
            "Validation loss: 3.614051675796509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters before: {total_params:,}\")\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters after: {total_params:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lXTjAV7S9yn",
        "outputId": "f3cb10e4-b8d8-40ac-9775-d0186be85d93"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total trainable parameters before: 838,359,040\n",
            "Total trainable parameters after: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "replace_linear_with_lora(model, rank=16, alpha=16)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable LoRA parameters: {total_params:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16z6Pyq2S9uA",
        "outputId": "aed24c2c-db6d-449b-f3d5-9d0f5642d5de"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing out_head with LinearLayerWithLoRA\n",
            "Total trainable LoRA parameters: 14,095,632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1g14uTrBTQIW",
        "outputId": "6dfffbaa-08b3-4d44-c5d7-b998e4ffcaa7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Model(\n",
              "  (token_emb): Embedding(50257, 1280)\n",
              "  (position_emb): Embedding(1024, 1280)\n",
              "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
              "  (transformer_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (12): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (13): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (14): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (15): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (16): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (17): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (18): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (19): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (20): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (21): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (22): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (23): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (24): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (25): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (26): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (27): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (28): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (29): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (30): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (31): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (32): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (33): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (34): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (35): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): LinearLayerWithLoRA(\n",
              "    (linear): Linear(in_features=1280, out_features=50257, bias=False)\n",
              "    (lora): LoRALayer()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "initial_lr = 0.0001\n",
        "peak_lr = 3e-5\n",
        "\n",
        "\n",
        "total_steps = len(train_loader) * num_epochs\n",
        "warmup_steps = int(0.2 * total_steps) # 20% warmup\n",
        "print(warmup_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvizhukGTQGk",
        "outputId": "fec4210d-4f2f-4282-e510-faaf5b009971"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "training_start_time = time.time()\n",
        "\n",
        "torch.manual_seed(211)\n",
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                              lr=peak_lr,\n",
        "                              weight_decay=0.01)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_losses, val_losses, tokens_seen, track_lrs = train_model(model=model,\n",
        "                                                    train_loader=train_loader,\n",
        "                                                    val_loader=val_loader,\n",
        "                                                    optimizer=optimizer,\n",
        "                                                    device=device,\n",
        "                                                    n_epochs=num_epochs,\n",
        "                                                    eval_freq=5,\n",
        "                                                    eval_iter=5,\n",
        "                                                    start_context=format_input(val_data[50]),\n",
        "                                                    tokenizer=tokenizer,\n",
        "                                                    warmup_steps=warmup_steps,\n",
        "                                                    initial_lr=1e-5,\n",
        "                                                    min_lr=1e-5)\n",
        "\n",
        "\n",
        "training_end_time = time.time()\n",
        "runtime_in_seconds = training_end_time - training_start_time\n",
        "# format as minutes and seconds\n",
        "minutes, seconds = divmod(runtime_in_seconds, 60)\n",
        "print(f\"device: {device}\")\n",
        "print(f\"training runtime: {int(minutes)} min {seconds:.2f} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk2YcHuETQEJ",
        "outputId": "67cba024-1ca3-426e-a7a5-363f53061ba7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Iter 000000): Train loss 2.805, Val loss 2.752\n",
            "Ep 1 (Iter 000005): Train loss 1.360, Val loss 1.326\n",
            "Ep 1 (Iter 000010): Train loss 0.936, Val loss 0.971\n",
            "Ep 1 (Iter 000015): Train loss 0.794, Val loss 0.891\n",
            "Ep 1 (Iter 000020): Train loss 0.806, Val loss 0.842\n",
            "Ep 1 (Iter 000025): Train loss 0.693, Val loss 0.829\n",
            "Ep 1 (Iter 000030): Train loss 0.749, Val loss 0.809\n",
            "Ep 1 (Iter 000035): Train loss 0.638, Val loss 0.777\n",
            "Ep 1 (Iter 000040): Train loss 0.703, Val loss 0.767\n",
            "Ep 1 (Iter 000045): Train loss 0.604, Val loss 0.741\n",
            "Ep 1 (Iter 000050): Train loss 0.708, Val loss 0.736\n",
            "Ep 1 (Iter 000055): Train loss 0.587, Val loss 0.726\n",
            "Ep 1 (Iter 000060): Train loss 0.628, Val loss 0.717\n",
            "Ep 1 (Iter 000065): Train loss 0.586, Val loss 0.718\n",
            "Ep 1 (Iter 000070): Train loss 0.681, Val loss 0.712\n",
            "Ep 1 (Iter 000075): Train loss 0.545, Val loss 0.695\n",
            "Ep 1 (Iter 000080): Train loss 0.579, Val loss 0.680\n",
            "Ep 1 (Iter 000085): Train loss 0.480, Val loss 0.666\n",
            "Ep 1 (Iter 000090): Train loss 0.501, Val loss 0.666\n",
            "Ep 1 (Iter 000095): Train loss 0.562, Val loss 0.656\n",
            "Ep 1 (Iter 000100): Train loss 0.534, Val loss 0.649\n",
            "Ep 1 (Iter 000105): Train loss 0.488, Val loss 0.644\n",
            "Ep 1 (Iter 000110): Train loss 0.511, Val loss 0.639\n",
            "Ep 1 (Iter 000115): Train loss 0.543, Val loss 0.645\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Label whether the following statements are true or false.  ### Input: The moon is a planet.  ### Response: The statement \"The moon is a planet\" is false.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the boiling point of\n",
            "Ep 2 (Iter 000120): Train loss 0.447, Val loss 0.634\n",
            "Ep 2 (Iter 000125): Train loss 0.492, Val loss 0.631\n",
            "Ep 2 (Iter 000130): Train loss 0.415, Val loss 0.628\n",
            "Ep 2 (Iter 000135): Train loss 0.428, Val loss 0.635\n",
            "Ep 2 (Iter 000140): Train loss 0.434, Val loss 0.629\n",
            "Ep 2 (Iter 000145): Train loss 0.391, Val loss 0.618\n",
            "Ep 2 (Iter 000150): Train loss 0.431, Val loss 0.624\n",
            "Ep 2 (Iter 000155): Train loss 0.383, Val loss 0.622\n",
            "Ep 2 (Iter 000160): Train loss 0.481, Val loss 0.621\n",
            "Ep 2 (Iter 000165): Train loss 0.392, Val loss 0.603\n",
            "Ep 2 (Iter 000170): Train loss 0.413, Val loss 0.613\n",
            "Ep 2 (Iter 000175): Train loss 0.427, Val loss 0.608\n",
            "Ep 2 (Iter 000180): Train loss 0.372, Val loss 0.601\n",
            "Ep 2 (Iter 000185): Train loss 0.405, Val loss 0.599\n",
            "Ep 2 (Iter 000190): Train loss 0.403, Val loss 0.609\n",
            "Ep 2 (Iter 000195): Train loss 0.385, Val loss 0.607\n",
            "Ep 2 (Iter 000200): Train loss 0.385, Val loss 0.607\n",
            "Ep 2 (Iter 000205): Train loss 0.335, Val loss 0.605\n",
            "Ep 2 (Iter 000210): Train loss 0.318, Val loss 0.618\n",
            "Ep 2 (Iter 000215): Train loss 0.372, Val loss 0.613\n",
            "Ep 2 (Iter 000220): Train loss 0.359, Val loss 0.616\n",
            "Ep 2 (Iter 000225): Train loss 0.344, Val loss 0.615\n",
            "Ep 2 (Iter 000230): Train loss 0.339, Val loss 0.617\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Label whether the following statements are true or false.  ### Input: The moon is a planet.  ### Response: The moon is a planet.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Input: The sun is shining.  ### Response: The\n",
            "Ep 3 (Iter 000235): Train loss 0.330, Val loss 0.627\n",
            "Ep 3 (Iter 000240): Train loss 0.333, Val loss 0.641\n",
            "Ep 3 (Iter 000245): Train loss 0.343, Val loss 0.650\n",
            "Ep 3 (Iter 000250): Train loss 0.340, Val loss 0.653\n",
            "Ep 3 (Iter 000255): Train loss 0.327, Val loss 0.652\n",
            "Ep 3 (Iter 000260): Train loss 0.347, Val loss 0.651\n",
            "Ep 3 (Iter 000265): Train loss 0.334, Val loss 0.649\n",
            "Ep 3 (Iter 000270): Train loss 0.291, Val loss 0.646\n",
            "Ep 3 (Iter 000275): Train loss 0.306, Val loss 0.643\n",
            "Ep 3 (Iter 000280): Train loss 0.342, Val loss 0.638\n",
            "Ep 3 (Iter 000285): Train loss 0.281, Val loss 0.636\n",
            "Ep 3 (Iter 000290): Train loss 0.278, Val loss 0.634\n",
            "Ep 3 (Iter 000295): Train loss 0.291, Val loss 0.633\n",
            "Ep 3 (Iter 000300): Train loss 0.286, Val loss 0.632\n",
            "Ep 3 (Iter 000305): Train loss 0.269, Val loss 0.632\n",
            "Ep 3 (Iter 000310): Train loss 0.260, Val loss 0.632\n",
            "Ep 3 (Iter 000315): Train loss 0.293, Val loss 0.632\n",
            "Ep 3 (Iter 000320): Train loss 0.265, Val loss 0.632\n",
            "Ep 3 (Iter 000325): Train loss 0.265, Val loss 0.631\n",
            "Ep 3 (Iter 000330): Train loss 0.272, Val loss 0.630\n",
            "Ep 3 (Iter 000335): Train loss 0.279, Val loss 0.629\n",
            "Ep 3 (Iter 000340): Train loss 0.268, Val loss 0.627\n",
            "Ep 3 (Iter 000345): Train loss 0.263, Val loss 0.625\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Label whether the following statements are true or false.  ### Input: The moon is a planet.  ### Response: The statement 'The moon is a planet' is false.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Input: What is the opposite of '\n",
            "Ep 4 (Iter 000350): Train loss 0.233, Val loss 0.623\n",
            "Ep 4 (Iter 000355): Train loss 0.260, Val loss 0.624\n",
            "Ep 4 (Iter 000360): Train loss 0.275, Val loss 0.625\n",
            "Ep 4 (Iter 000365): Train loss 0.232, Val loss 0.626\n",
            "Ep 4 (Iter 000370): Train loss 0.234, Val loss 0.628\n",
            "Ep 4 (Iter 000375): Train loss 0.256, Val loss 0.629\n",
            "Ep 4 (Iter 000380): Train loss 0.264, Val loss 0.630\n",
            "Ep 4 (Iter 000385): Train loss 0.241, Val loss 0.631\n",
            "Ep 4 (Iter 000390): Train loss 0.228, Val loss 0.631\n",
            "Ep 4 (Iter 000395): Train loss 0.234, Val loss 0.631\n",
            "Ep 4 (Iter 000400): Train loss 0.237, Val loss 0.631\n",
            "Ep 4 (Iter 000405): Train loss 0.239, Val loss 0.631\n",
            "Ep 4 (Iter 000410): Train loss 0.224, Val loss 0.631\n",
            "Ep 4 (Iter 000415): Train loss 0.231, Val loss 0.630\n",
            "Ep 4 (Iter 000420): Train loss 0.213, Val loss 0.629\n",
            "Ep 4 (Iter 000425): Train loss 0.214, Val loss 0.629\n",
            "Ep 4 (Iter 000430): Train loss 0.228, Val loss 0.628\n",
            "Ep 4 (Iter 000435): Train loss 0.204, Val loss 0.627\n",
            "Ep 4 (Iter 000440): Train loss 0.229, Val loss 0.627\n",
            "Ep 4 (Iter 000445): Train loss 0.227, Val loss 0.626\n",
            "Ep 4 (Iter 000450): Train loss 0.248, Val loss 0.626\n",
            "Ep 4 (Iter 000455): Train loss 0.200, Val loss 0.624\n",
            "Ep 4 (Iter 000460): Train loss 0.209, Val loss 0.624\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Label whether the following statements are true or false.  ### Input: The moon is a planet.  ### Response: The statement 'The moon is a planet' is false.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Input: What is the capital of the\n",
            "Ep 5 (Iter 000465): Train loss 0.216, Val loss 0.623\n",
            "Ep 5 (Iter 000470): Train loss 0.212, Val loss 0.625\n",
            "Ep 5 (Iter 000475): Train loss 0.216, Val loss 0.628\n",
            "Ep 5 (Iter 000480): Train loss 0.195, Val loss 0.631\n",
            "Ep 5 (Iter 000485): Train loss 0.191, Val loss 0.635\n",
            "Ep 5 (Iter 000490): Train loss 0.206, Val loss 0.636\n",
            "Ep 5 (Iter 000495): Train loss 0.189, Val loss 0.637\n",
            "Ep 5 (Iter 000500): Train loss 0.192, Val loss 0.637\n",
            "Ep 5 (Iter 000505): Train loss 0.217, Val loss 0.637\n",
            "Ep 5 (Iter 000510): Train loss 0.212, Val loss 0.638\n",
            "Ep 5 (Iter 000515): Train loss 0.215, Val loss 0.638\n",
            "Ep 5 (Iter 000520): Train loss 0.217, Val loss 0.636\n",
            "Ep 5 (Iter 000525): Train loss 0.216, Val loss 0.635\n",
            "Ep 5 (Iter 000530): Train loss 0.184, Val loss 0.636\n",
            "Ep 5 (Iter 000535): Train loss 0.176, Val loss 0.636\n",
            "Ep 5 (Iter 000540): Train loss 0.187, Val loss 0.636\n",
            "Ep 5 (Iter 000545): Train loss 0.189, Val loss 0.636\n",
            "Ep 5 (Iter 000550): Train loss 0.198, Val loss 0.636\n",
            "Ep 5 (Iter 000555): Train loss 0.190, Val loss 0.636\n",
            "Ep 5 (Iter 000560): Train loss 0.200, Val loss 0.636\n",
            "Ep 5 (Iter 000565): Train loss 0.189, Val loss 0.635\n",
            "Ep 5 (Iter 000570): Train loss 0.198, Val loss 0.633\n",
            "Ep 5 (Iter 000575): Train loss 0.192, Val loss 0.632\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Label whether the following statements are true or false.  ### Input: The moon is a planet.  ### Response: The statement 'The moon is a planet' is false.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Input: What is the capital of the\n",
            "Ep 6 (Iter 000580): Train loss 0.191, Val loss 0.632\n",
            "Ep 6 (Iter 000585): Train loss 0.210, Val loss 0.635\n",
            "Ep 6 (Iter 000590): Train loss 0.183, Val loss 0.640\n",
            "Ep 6 (Iter 000595): Train loss 0.199, Val loss 0.645\n",
            "Ep 6 (Iter 000600): Train loss 0.192, Val loss 0.649\n",
            "Ep 6 (Iter 000605): Train loss 0.183, Val loss 0.652\n",
            "Ep 6 (Iter 000610): Train loss 0.192, Val loss 0.654\n",
            "Ep 6 (Iter 000615): Train loss 0.181, Val loss 0.655\n",
            "Ep 6 (Iter 000620): Train loss 0.192, Val loss 0.655\n",
            "Ep 6 (Iter 000625): Train loss 0.174, Val loss 0.654\n",
            "Ep 6 (Iter 000630): Train loss 0.171, Val loss 0.653\n",
            "Ep 6 (Iter 000635): Train loss 0.171, Val loss 0.652\n",
            "Ep 6 (Iter 000640): Train loss 0.179, Val loss 0.650\n",
            "Ep 6 (Iter 000645): Train loss 0.168, Val loss 0.649\n",
            "Ep 6 (Iter 000650): Train loss 0.175, Val loss 0.650\n",
            "Ep 6 (Iter 000655): Train loss 0.181, Val loss 0.652\n",
            "Ep 6 (Iter 000660): Train loss 0.177, Val loss 0.653\n",
            "Ep 6 (Iter 000665): Train loss 0.170, Val loss 0.653\n",
            "Ep 6 (Iter 000670): Train loss 0.174, Val loss 0.653\n",
            "Ep 6 (Iter 000675): Train loss 0.160, Val loss 0.654\n",
            "Ep 6 (Iter 000680): Train loss 0.171, Val loss 0.654\n",
            "Ep 6 (Iter 000685): Train loss 0.168, Val loss 0.653\n",
            "Ep 6 (Iter 000690): Train loss 0.171, Val loss 0.652\n",
            "Ep 6 (Iter 000695): Train loss 0.168, Val loss 0.651\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Label whether the following statements are true or false.  ### Input: The moon is a planet.  ### Response: The statement 'The moon is a planet' is false.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Input: What is the capital of the\n",
            "Ep 7 (Iter 000700): Train loss 0.160, Val loss 0.652\n",
            "Ep 7 (Iter 000705): Train loss 0.173, Val loss 0.655\n",
            "Ep 7 (Iter 000710): Train loss 0.165, Val loss 0.658\n",
            "Ep 7 (Iter 000715): Train loss 0.160, Val loss 0.660\n",
            "Ep 7 (Iter 000720): Train loss 0.178, Val loss 0.661\n",
            "Ep 7 (Iter 000725): Train loss 0.167, Val loss 0.662\n",
            "Ep 7 (Iter 000730): Train loss 0.161, Val loss 0.664\n",
            "Ep 7 (Iter 000735): Train loss 0.160, Val loss 0.664\n",
            "Ep 7 (Iter 000740): Train loss 0.167, Val loss 0.666\n",
            "Ep 7 (Iter 000745): Train loss 0.170, Val loss 0.667\n",
            "Ep 7 (Iter 000750): Train loss 0.161, Val loss 0.668\n",
            "Ep 7 (Iter 000755): Train loss 0.159, Val loss 0.669\n",
            "Ep 7 (Iter 000760): Train loss 0.171, Val loss 0.669\n",
            "Ep 7 (Iter 000765): Train loss 0.169, Val loss 0.669\n",
            "Ep 7 (Iter 000770): Train loss 0.160, Val loss 0.668\n",
            "Ep 7 (Iter 000775): Train loss 0.155, Val loss 0.668\n",
            "Ep 7 (Iter 000780): Train loss 0.169, Val loss 0.667\n",
            "Ep 7 (Iter 000785): Train loss 0.164, Val loss 0.668\n",
            "Ep 7 (Iter 000790): Train loss 0.156, Val loss 0.667\n",
            "Ep 7 (Iter 000795): Train loss 0.156, Val loss 0.667\n",
            "Ep 7 (Iter 000800): Train loss 0.149, Val loss 0.667\n",
            "Ep 7 (Iter 000805): Train loss 0.159, Val loss 0.666\n",
            "Ep 7 (Iter 000810): Train loss 0.156, Val loss 0.666\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Label whether the following statements are true or false.  ### Input: The moon is a planet.  ### Response: The statement 'The moon is a planet' is false.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Input: What is the capital of the\n",
            "Ep 8 (Iter 000815): Train loss 0.148, Val loss 0.665\n",
            "Ep 8 (Iter 000820): Train loss 0.156, Val loss 0.666\n",
            "Ep 8 (Iter 000825): Train loss 0.152, Val loss 0.667\n",
            "Ep 8 (Iter 000830): Train loss 0.158, Val loss 0.668\n",
            "Ep 8 (Iter 000835): Train loss 0.160, Val loss 0.670\n",
            "Ep 8 (Iter 000840): Train loss 0.160, Val loss 0.672\n",
            "Ep 8 (Iter 000845): Train loss 0.155, Val loss 0.674\n",
            "Ep 8 (Iter 000850): Train loss 0.161, Val loss 0.677\n",
            "Ep 8 (Iter 000855): Train loss 0.163, Val loss 0.678\n",
            "Ep 8 (Iter 000860): Train loss 0.165, Val loss 0.678\n",
            "Ep 8 (Iter 000865): Train loss 0.153, Val loss 0.677\n",
            "Ep 8 (Iter 000870): Train loss 0.160, Val loss 0.677\n",
            "Ep 8 (Iter 000875): Train loss 0.156, Val loss 0.678\n",
            "Ep 8 (Iter 000880): Train loss 0.152, Val loss 0.677\n",
            "Ep 8 (Iter 000885): Train loss 0.149, Val loss 0.676\n",
            "Ep 8 (Iter 000890): Train loss 0.148, Val loss 0.675\n",
            "Ep 8 (Iter 000895): Train loss 0.155, Val loss 0.675\n",
            "Ep 8 (Iter 000900): Train loss 0.151, Val loss 0.674\n",
            "Ep 8 (Iter 000905): Train loss 0.153, Val loss 0.674\n",
            "Ep 8 (Iter 000910): Train loss 0.155, Val loss 0.674\n",
            "Ep 8 (Iter 000915): Train loss 0.148, Val loss 0.675\n",
            "Ep 8 (Iter 000920): Train loss 0.149, Val loss 0.675\n",
            "Ep 8 (Iter 000925): Train loss 0.151, Val loss 0.675\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Label whether the following statements are true or false.  ### Input: The moon is a planet.  ### Response: The statement 'The moon is a planet' is false.<|endoftext|>The following blog post, unless otherwise noted, was written by a member of Gamasutra's community.  The thoughts and opinions expressed are those of\n",
            "Ep 9 (Iter 000930): Train loss 0.146, Val loss 0.675\n",
            "Ep 9 (Iter 000935): Train loss 0.153, Val loss 0.677\n",
            "Ep 9 (Iter 000940): Train loss 0.154, Val loss 0.679\n",
            "Ep 9 (Iter 000945): Train loss 0.155, Val loss 0.680\n",
            "Ep 9 (Iter 000950): Train loss 0.154, Val loss 0.681\n",
            "Ep 9 (Iter 000955): Train loss 0.148, Val loss 0.682\n",
            "Ep 9 (Iter 000960): Train loss 0.149, Val loss 0.684\n",
            "Ep 9 (Iter 000965): Train loss 0.149, Val loss 0.685\n",
            "Ep 9 (Iter 000970): Train loss 0.150, Val loss 0.687\n",
            "Ep 9 (Iter 000975): Train loss 0.153, Val loss 0.688\n",
            "Ep 9 (Iter 000980): Train loss 0.147, Val loss 0.688\n",
            "Ep 9 (Iter 000985): Train loss 0.157, Val loss 0.688\n",
            "Ep 9 (Iter 000990): Train loss 0.150, Val loss 0.688\n",
            "Ep 9 (Iter 000995): Train loss 0.149, Val loss 0.690\n",
            "Ep 9 (Iter 001000): Train loss 0.146, Val loss 0.689\n",
            "Ep 9 (Iter 001005): Train loss 0.147, Val loss 0.688\n",
            "Ep 9 (Iter 001010): Train loss 0.150, Val loss 0.688\n",
            "Ep 9 (Iter 001015): Train loss 0.148, Val loss 0.688\n",
            "Ep 9 (Iter 001020): Train loss 0.149, Val loss 0.687\n",
            "Ep 9 (Iter 001025): Train loss 0.144, Val loss 0.687\n",
            "Ep 9 (Iter 001030): Train loss 0.151, Val loss 0.686\n",
            "Ep 9 (Iter 001035): Train loss 0.137, Val loss 0.687\n",
            "Ep 9 (Iter 001040): Train loss 0.145, Val loss 0.686\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Label whether the following statements are true or false.  ### Input: The moon is a planet.  ### Response: The statement 'The moon is a planet' is false.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Input: What is the capital of the\n",
            "Ep 10 (Iter 001045): Train loss 0.148, Val loss 0.686\n",
            "Ep 10 (Iter 001050): Train loss 0.149, Val loss 0.686\n",
            "Ep 10 (Iter 001055): Train loss 0.141, Val loss 0.687\n",
            "Ep 10 (Iter 001060): Train loss 0.142, Val loss 0.688\n",
            "Ep 10 (Iter 001065): Train loss 0.150, Val loss 0.690\n",
            "Ep 10 (Iter 001070): Train loss 0.146, Val loss 0.693\n",
            "Ep 10 (Iter 001075): Train loss 0.146, Val loss 0.694\n",
            "Ep 10 (Iter 001080): Train loss 0.153, Val loss 0.696\n",
            "Ep 10 (Iter 001085): Train loss 0.140, Val loss 0.699\n",
            "Ep 10 (Iter 001090): Train loss 0.142, Val loss 0.701\n",
            "Ep 10 (Iter 001095): Train loss 0.144, Val loss 0.703\n",
            "Ep 10 (Iter 001100): Train loss 0.149, Val loss 0.703\n",
            "Ep 10 (Iter 001105): Train loss 0.142, Val loss 0.702\n",
            "Ep 10 (Iter 001110): Train loss 0.145, Val loss 0.700\n",
            "Ep 10 (Iter 001115): Train loss 0.148, Val loss 0.697\n",
            "Ep 10 (Iter 001120): Train loss 0.145, Val loss 0.696\n",
            "Ep 10 (Iter 001125): Train loss 0.147, Val loss 0.694\n",
            "Ep 10 (Iter 001130): Train loss 0.149, Val loss 0.693\n",
            "Ep 10 (Iter 001135): Train loss 0.136, Val loss 0.691\n",
            "Ep 10 (Iter 001140): Train loss 0.146, Val loss 0.690\n",
            "Ep 10 (Iter 001145): Train loss 0.144, Val loss 0.689\n",
            "Ep 10 (Iter 001150): Train loss 0.140, Val loss 0.688\n",
            "Ep 10 (Iter 001155): Train loss 0.143, Val loss 0.687\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Label whether the following statements are true or false.  ### Input: The moon is a planet.  ### Response: The statement 'The moon is a planet' is false.<|endoftext|>The following blog post, unless otherwise noted, was written by a member of Gamasutra's community.  The thoughts and opinions expressed are those of\n",
            "device: cuda\n",
            "training runtime: 7 min 47.61 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(range(len(track_lrs)), track_lrs)\n",
        "plt.ylabel(\"Learning rate\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "1ckGMcIfTQBp",
        "outputId": "bf44292a-82b0-4a17-f255-d9ab886fa572"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's plot a loss graph:\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_losses(epoch_seen,\n",
        "                tokens_seen,\n",
        "                train_losses,\n",
        "                val_losses):\n",
        "  \"\"\"Plot training and validation losses\"\"\"\n",
        "\n",
        "  fig, ax1 = plt.subplots(figsize=(5, 5))\n",
        "\n",
        "  # plot training and validation loss against epochs\n",
        "  ax1.plot(epoch_seen, train_losses, label=\"Training Loss\")\n",
        "  ax1.plot(epoch_seen, val_losses, linestyle=\"-.\", label=\"Validation Loss\")\n",
        "  ax1.set_xlabel(\"Epoch\")\n",
        "  ax1.set_ylabel(\"Loss\")\n",
        "  ax1.legend(loc=\"upper right\")\n",
        "  ax1.xaxis.set_major_locator(MaxNLocator(integer=True)) # only show integer labels on x-axis\n",
        "\n",
        "  # create a second x-axis for token seen\n",
        "  ax2 = ax1.twiny() # create a second x-axis that shares the same y-axis\n",
        "  ax2.plot(tokens_seen, train_losses, alpha=0) # invisible plot for aligning ticks\n",
        "  ax2.set_xlabel(\"Tokens Seen\")\n",
        "\n",
        "  fig.tight_layout() # asjust layput to make room\n",
        "  plt.savefig(\"instruction_finetune_gpt2_loss_plot.png\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "9LGUF8pITP_i",
        "outputId": "ba954d48-b6cf-4eb4-b426-da72456d6f1a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(211)\n",
        "\n",
        "\n",
        "for entry in test_data[55:64]:\n",
        "\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "    token_ids = generate_text(\n",
        "        model=model,\n",
        "        input_batch=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    response_text = (\n",
        "        generated_text[len(input_text):]\n",
        "        .replace(\"### Response:\", \"\")\n",
        "        .strip()\n",
        ")\n",
        "\n",
        "    print(input_text)\n",
        "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
        "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
        "    print(\"-------------------------------------\")\n",
        "    print(\"-------------------------------------\")\n",
        "    print(\"-------------------------------------\")\n",
        "    print(\"-------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQxxY5aeTP9N",
        "outputId": "d6cc3494-2736-4aa1-8934-c20595a9fc94"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Explain the primary function of the human heart.\n",
            "\n",
            "Correct response:\n",
            ">> The primary function of the human heart is to pump blood throughout the body, delivering oxygen and nutrients to tissues and removing carbon dioxide and other wastes.\n",
            "\n",
            "Model response:\n",
            ">> The primary function of the human heart is to pump blood around the body, supplying all the tissues with oxygen and nutrients. It also pumps blood away from the body and into the lungs, where it is used for respiration.\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Reword the following sentence to the future tense.\n",
            "\n",
            "### Input:\n",
            "He is reading a novel inspired by his grandmother.\n",
            "\n",
            "Correct response:\n",
            ">> He will be reading a novel inspired by his grandmother.\n",
            "\n",
            "Model response:\n",
            ">> He will be reading a novel inspired by his grandmother.\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert the given sentence into active voice.\n",
            "\n",
            "### Input:\n",
            "The law was passed by the government.\n",
            "\n",
            "Correct response:\n",
            ">> The government passed the law.\n",
            "\n",
            "Model response:\n",
            ">> The government passed the law.\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Create a sentence using the word 'inevitable'.\n",
            "\n",
            "Correct response:\n",
            ">> The confrontation was inevitable given the circumstances.\n",
            "\n",
            "Model response:\n",
            ">> The collapse of the company was inevitable due to poor management.\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Categorize the following sentence as either factual or opinion-based.\n",
            "\n",
            "### Input:\n",
            "Chocolate is the best dessert.\n",
            "\n",
            "Correct response:\n",
            ">> Opinion-based.\n",
            "\n",
            "Model response:\n",
            ">> The best dessert is chocolate.\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is an antonym of 'old'?\n",
            "\n",
            "Correct response:\n",
            ">> young.\n",
            "\n",
            "Model response:\n",
            ">> An antonym of 'old' is 'young'.\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Provide a synonym for 'hardworking'.\n",
            "\n",
            "Correct response:\n",
            ">> A synonym for 'hardworking' is 'diligent'.\n",
            "\n",
            "Model response:\n",
            ">> A synonym for 'hardworking' is 'thrifty'.\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the boiling point of sulfur in Celsius?\n",
            "\n",
            "Correct response:\n",
            ">> The boiling point of sulfur is 444.6 degrees Celsius.\n",
            "\n",
            "Model response:\n",
            ">> The boiling point of sulfur is 674 degrees Celsius.\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the plural form of 'child'?\n",
            "\n",
            "Correct response:\n",
            ">> The plural form of 'child' is 'children'.\n",
            "\n",
            "Model response:\n",
            ">> The plural form of 'child' is 'cousins'.\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Example entry:\\n\", test_data[50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEeF-XKvTP69",
        "outputId": "9d85a612-38d4-45f7-dd98-be1f52e2ba60"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example entry:\n",
            " {'instruction': 'Edit the given text to ensure all plural nouns are spelled correctly.', 'input': 'The birds sings beautiful songs.', 'output': 'The birds sing beautiful songs.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# generate responses for whole dataset\n",
        "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
        "\n",
        "  input_text = format_input(entry)\n",
        "\n",
        "  token_ids = generate_text(\n",
        "      model=model,\n",
        "      input_batch=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "      max_new_tokens=256,\n",
        "      context_size=BASE_CONFIG[\"context_length\"],\n",
        "      eos_id=50256)\n",
        "  generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "  response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
        "\n",
        "  # add \"model_response\" field into `test_data`\n",
        "  test_data[i][\"model_response\"] = response_text\n",
        "\n",
        "\n",
        "# save `test_data` into .json file\n",
        "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
        "  json.dump(test_data, file, indent=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSTvgzu8Tx_5",
        "outputId": "d40d2055-a9db-49ea-f000-e0e5b1e106c0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 110/110 [02:24<00:00,  1.31s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_data[50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QuAFf-eTx9a",
        "outputId": "fc003ab2-41b4-430f-a701-c9a02c181bea"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'instruction': 'Edit the given text to ensure all plural nouns are spelled correctly.', 'input': 'The birds sings beautiful songs.', 'output': 'The birds sing beautiful songs.', 'model_response': 'The birds sing beautiful songs.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQoklwlgTx7N",
        "outputId": "af62d4ae-78a7-4b6c-c5d7-d37164c793cf"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dotenv\n",
            "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
            "Collecting python-dotenv (from dotenv)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, dotenv\n",
            "Successfully installed dotenv-0.9.9 python-dotenv-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "pkgs = [\"openai\",  # OpenAI API\n",
        "        \"tqdm\",    # Progress bar\n",
        "        \"dotenv\"\n",
        "        ]\n",
        "\n",
        "for p in pkgs:\n",
        "    print(f\"{p} version: {version(p)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBNA9WLITx5K",
        "outputId": "e29d345c-69ec-443b-df09-2e9643be4a39"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openai version: 1.93.0\n",
            "tqdm version: 4.67.1\n",
            "dotenv version: 0.9.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
        "    print(\"API key looks good so far\")\n",
        "else:\n",
        "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
        "\n",
        "MODEL = \"gpt-4.1-mini\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvtroQo0Tx24",
        "outputId": "e5f85ddd-a32f-4d19-8b1a-82a7e0a4e8c7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API key looks good so far\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_chatgpt(prompt, client, model=MODEL):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.0,\n",
        "        seed=123,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "prompt = \"Respond with 'hello world' if you got this message.\"\n",
        "run_chatgpt(prompt, client)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "WxGtOuuTTx0e",
        "outputId": "f839695e-96cc-422d-bd88-f6e8b35dd2c8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello world'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for entry in test_data[55:64]:\n",
        "    prompt = (\n",
        "        f\"Given the input `{format_input(entry)}` \"\n",
        "        f\"and correct output `{entry['output']}`, \"\n",
        "        f\"score the model response `{entry['model_response']}`\"\n",
        "        f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
        "        f\"Respond with the integer number only.\"\n",
        "    )\n",
        "    print(\"\\nDataset response:\")\n",
        "    print(\">>\", entry['output'])\n",
        "    print(\"\\nModel response:\")\n",
        "    print(\">>\", entry[\"model_response\"])\n",
        "    print(\"\\nScore:\")\n",
        "    print(\">>\", run_chatgpt(prompt, client))\n",
        "    print(\"\\n-------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0Sih8XtTxyf",
        "outputId": "df1146bb-c776-4b10-af8d-bc5ff9c4cedd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset response:\n",
            ">> The primary function of the human heart is to pump blood throughout the body, delivering oxygen and nutrients to tissues and removing carbon dioxide and other wastes.\n",
            "\n",
            "Model response:\n",
            ">> The primary function of the human heart is to pump blood around the body, supplying all the tissues with oxygen and nutrients. It also pumps blood away from the body and into the lungs, where it is used for respiration.\n",
            "\n",
            "Score:\n",
            ">> 85\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> He will be reading a novel inspired by his grandmother.\n",
            "\n",
            "Model response:\n",
            ">> He will be reading a novel inspired by his grandmother.\n",
            "\n",
            "Score:\n",
            ">> 100\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> The government passed the law.\n",
            "\n",
            "Model response:\n",
            ">> The government passed the law.\n",
            "\n",
            "Score:\n",
            ">> 100\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> The confrontation was inevitable given the circumstances.\n",
            "\n",
            "Model response:\n",
            ">> The collapse of the company was inevitable due to poor management.\n",
            "\n",
            "Score:\n",
            ">> 100\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> Opinion-based.\n",
            "\n",
            "Model response:\n",
            ">> The best dessert is chocolate.\n",
            "\n",
            "Score:\n",
            ">> 80\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> young.\n",
            "\n",
            "Model response:\n",
            ">> An antonym of 'old' is 'young'.\n",
            "\n",
            "Score:\n",
            ">> 100\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> A synonym for 'hardworking' is 'diligent'.\n",
            "\n",
            "Model response:\n",
            ">> A synonym for 'hardworking' is 'thrifty'.\n",
            "\n",
            "Score:\n",
            ">> 20\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> The boiling point of sulfur is 444.6 degrees Celsius.\n",
            "\n",
            "Model response:\n",
            ">> The boiling point of sulfur is 674 degrees Celsius.\n",
            "\n",
            "Score:\n",
            ">> 30\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> The plural form of 'child' is 'children'.\n",
            "\n",
            "Model response:\n",
            ">> The plural form of 'child' is 'cousins'.\n",
            "\n",
            "Score:\n",
            ">> 0\n",
            "\n",
            "-------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_model_scores(json_data, json_key, model=MODEL):\n",
        "    scores = []\n",
        "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
        "        prompt = (\n",
        "            f\"Given the input `{format_input(entry)}` \"\n",
        "            f\"and correct output `{entry['output']}`, \"\n",
        "            f\"score the model response `{entry[json_key]}`\"\n",
        "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
        "            f\"Respond with the integer number only.\"\n",
        "        )\n",
        "        score = run_chatgpt(prompt, client, model)\n",
        "        try:\n",
        "            scores.append(int(score))\n",
        "        except ValueError:\n",
        "            print(f\"Could not convert score: {score}\")\n",
        "            continue\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "scores = generate_model_scores(test_data, \"model_response\")\n",
        "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
        "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3wd5MyxTxwP",
        "outputId": "79473eb8-1fca-4290-afeb-86dc1e90ddf5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scoring entries: 100%|██████████| 110/110 [00:48<00:00,  2.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of scores: 110 of 110\n",
            "Average score: 47.68\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model state_dict\n",
        "torch.save(model.state_dict(), \"instruction_finetune_gpt2.pth\")"
      ],
      "metadata": {
        "id": "my1nLVnQTxt8"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model state_dict\n",
        "model = GPT2Model(BASE_CONFIG)\n",
        "replace_linear_with_lora(model, rank=16, alpha=16)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable LoRA parameters: {total_params:,}\")\n",
        "\n",
        "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "model.load_state_dict(torch.load(\"instruction_finetune_gpt2.pth\", map_location=\"cpu\", weights_only=True))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sR4kDBWTPzj",
        "outputId": "fee70593-7973-4069-bbf3-82566705a4a2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing out_head with LinearLayerWithLoRA\n",
            "Total trainable LoRA parameters: 852,454,672\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Model(\n",
              "  (token_emb): Embedding(50257, 1280)\n",
              "  (position_emb): Embedding(1024, 1280)\n",
              "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
              "  (transformer_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (12): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (13): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (14): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (15): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (16): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (17): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (18): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (19): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (20): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (21): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (22): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (23): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (24): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (25): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (26): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (27): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (28): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (29): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (30): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (31): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (32): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (33): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (34): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (35): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): LinearLayerWithLoRA(\n",
              "    (linear): Linear(in_features=1280, out_features=50257, bias=False)\n",
              "    (lora): LoRALayer()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Gradio"
      ],
      "metadata": {
        "id": "5n9dMRHKY6Qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "N8V2-l5-Ygzm"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "def generate_answer(instruction:str = \"\", input:str = \"\"):\n",
        "\n",
        "  # start the timer\n",
        "  start_time = time.time()\n",
        "\n",
        "  torch.manual_seed(211)\n",
        "  model.eval()\n",
        "\n",
        "  input_prompt = {'instruction': instruction, 'input': input}\n",
        "  input_text = format_input(input_prompt)\n",
        "\n",
        "  token_ids = generate_text(\n",
        "        model=model,\n",
        "        input_batch=text_to_token_ids(input_text, bpe_tokenizer).to(\"cpu\"),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "\n",
        "  generated_text = token_ids_to_text(token_ids, bpe_tokenizer)\n",
        "  response_text = (\n",
        "      generated_text[len(input_text):]\n",
        "      .replace(\"### Response:\", \"\")\n",
        "      .strip()\n",
        "      )\n",
        "\n",
        "  # Calculate the prediction time\n",
        "  pred_time = round(time.time() - start_time)\n",
        "\n",
        "  # print(\"Output text:\\n\", token_ids_to_text(token_ids, bpe_tokenizer))\n",
        "  return response_text.strip(), pred_time, \"instruction_finetune_gpt2_loss_plot.png\""
      ],
      "metadata": {
        "id": "htXOBNu2Zbme"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Create title, description and article strings\n",
        "title = \"GPT2 From Scratch\"\n",
        "description = \"Instruction Finetune GPT2 (774M).\"\n",
        "article = \"Learning LLM from scratch\"\n",
        "\n",
        "\n",
        "# example list\n",
        "example_list = [\n",
        "    [\"Generate a sentence using the word 'curiousity'.\", \"\"],\n",
        "    [\"What is the formula for area of a circle?\", \"\"],\n",
        "    [\"How to train a dragon?\", \"\"],\n",
        "    [\"Convert this sentence to passive voice\", \"The chef cooked a tasty meal.\"],\n",
        "    [\"What is the capital of Taiwan?\", \"\"],\n",
        "    [\"List 3 antonyms for 'intelligent'.\", \"\"],\n",
        "    [\"what is a language model?\", \"\"],\n",
        "    [\"Identify the correct spelling of the following word.\", \"ingelligent\"]\n",
        "]\n",
        "\n",
        "\n",
        "# Create the Gradio demo\n",
        "demo = gr.Interface(fn=generate_answer, # mapping function from input to output\n",
        "                    inputs=[\n",
        "                        gr.Textbox(label=\"Instruction\"),\n",
        "                        gr.Textbox(label=\"Input\",\n",
        "                                   placeholder=\"can be left empty\"),\n",
        "                        ],\n",
        "                    outputs=[\n",
        "                        gr.Textbox(label=\"Result\"),\n",
        "                        gr.Number(label=\"Time Taken (s)\"),\n",
        "                        gr.Image(label=\"Training Loss\"),\n",
        "                        ],\n",
        "                    title=title,\n",
        "                    description=description,\n",
        "                    article=article,\n",
        "                    examples=example_list,\n",
        "                    #examples_per_page=3,\n",
        "                    )\n",
        "\n",
        "# Launch the demo!\n",
        "demo.launch(debug=True, # print errors locally?\n",
        "            share=True) # generate a publically shareable URL?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "qwULsRoEZbjH",
        "outputId": "961e3beb-18b2-44af-9b3a-cd3a99a3fe28"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://1fb5344113dbb37952.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1fb5344113dbb37952.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://1fb5344113dbb37952.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MEN626hNcwtO"
      },
      "execution_count": 46,
      "outputs": []
    }
  ]
}