{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "L4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zPeuxRfap96_"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "notebook_start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. Setup"
   ],
   "metadata": {
    "id": "9Y79HwMTqDtJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ],
   "metadata": {
    "id": "na64m4FAqFVZ"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\",\n",
    "        \"numpy\",\n",
    "        \"tiktoken\",\n",
    "        \"torch\",\n",
    "        \"tensorflow\", # For OpenAI's pretrained weights\n",
    "        \"pandas\"      # Dataset loading\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wfjlj3YWqM_F",
    "outputId": "b2dc1c64-398a-4fc8-8a44-23e902d1bbf5"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "matplotlib version: 3.10.0\n",
      "numpy version: 2.0.2\n",
      "tiktoken version: 0.9.0\n",
      "torch version: 2.6.0+cu124\n",
      "tensorflow version: 2.18.0\n",
      "pandas version: 2.2.2\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Data Processing"
   ],
   "metadata": {
    "id": "6TJ1c13wIKnG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import urllib\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "        return\n",
    "\n",
    "    # Downloading the file\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "    # Unzipping the file\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    # Add .tsv file extension\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def create_balanced_dataset(df):\n",
    "  # count the instances of `spam`\n",
    "  num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "\n",
    "  # randomly sub-sample \"ham\" with `num_spam` rows\n",
    "  ham_sub = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "\n",
    "  # concatenate subsets\n",
    "  balanced_df = pd.concat([ham_sub, df[df[\"Label\"] == \"spam\"]])\n",
    "\n",
    "  return balanced_df\n",
    "\n",
    "\n",
    "def random_split(df, train_frac, validation_frac):\n",
    "\n",
    "  # shuffle dataset\n",
    "  df = df.sample(frac=1, random_state=211).reset_index(drop=True)\n",
    "\n",
    "  # compute split range\n",
    "  train_end = int(len(df) * train_frac)\n",
    "  validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "  # split\n",
    "  train_df = df[:train_end]\n",
    "  validation_df = df[train_end:validation_end]\n",
    "  test_df = df[validation_end:]\n",
    "\n",
    "  return train_df, validation_df, test_df\n",
    "\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "try:\n",
    "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "except (urllib.error.HTTPError, urllib.error.URLError, TimeoutError) as e:\n",
    "    print(f\"Primary URL failed: {e}. Trying backup URL...\")\n",
    "    url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/sms%2Bspam%2Bcollection.zip\"\n",
    "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jMvNDyL1qM7l",
    "outputId": "9ab3a556-859a-45b0-85e5-c2f7655371f4"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sms_spam_collection/SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "  def __init__(self,\n",
    "               csv_file,\n",
    "               tokenizer,\n",
    "               max_length=None,\n",
    "               pad_token_id=50256):\n",
    "    self.data = pd.read_csv(csv_file)\n",
    "\n",
    "    # pre-tokenizer texts\n",
    "    self.encoded_texts = [tokenizer.encode(text) for text in self.data[\"Text\"]]\n",
    "\n",
    "    if max_length is None:\n",
    "      self.max_length = self._longest_encoded_length()\n",
    "    else:\n",
    "      self.max_length = max_length\n",
    "      # truncate sequences if they're longer than max_length\n",
    "      self.encoded_texts = [encoded_text[:self.max_length] for encoded_text in self.encoded_texts]\n",
    "\n",
    "    # pad tokens\n",
    "    self.encoded_texts = [\n",
    "        encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "        for encoded_text in self.encoded_texts\n",
    "    ]\n",
    "\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    encoded_sequence = self.encoded_texts[index]\n",
    "    label = self.data.iloc[index][\"Label\"]\n",
    "    return (\n",
    "        torch.tensor(encoded_sequence, dtype=torch.long),\n",
    "        torch.tensor(label, dtype=torch.long)\n",
    "    )\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def _longest_encoded_length(self):\n",
    "    return max(len(encoded_text) for encoded_text in self.encoded_texts)\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "train_dataset = SpamDataset(\"train.csv\", max_length=None, tokenizer=tokenizer)\n",
    "val_dataset = SpamDataset(\"validation.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
    "test_dataset = SpamDataset(\"test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)"
   ],
   "metadata": {
    "id": "mE90tsdPqM48"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZPiZHq5qM2p",
    "outputId": "fcd35435-777c-4174-b0ab-822a8b04c3a9"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bzPH2nUqM0W",
    "outputId": "a837ac8c-8d90-4246-f98f-2ca44ff51710"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Initializa the model"
   ],
   "metadata": {
    "id": "qZUlELeUI0K4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"In the midst of winter\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
    "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
    "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
    "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
    ")"
   ],
   "metadata": {
    "id": "X4nt4hwtqMyu"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import urllib.request\n",
    "\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\"\n",
    "    \"LLMs-from-scratch/main/ch05/\"\n",
    "    \"01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split(\"/\")[-1]\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "\n",
    "from gpt_download import download_and_load_gpt2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_embedding_dim,\n",
    "                 output_embedding_dim,\n",
    "                 context_length,\n",
    "                 dropout,\n",
    "                 num_heads,\n",
    "                 qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (output_embedding_dim % num_heads == 0), \\\n",
    "            \"output_embedding_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.output_embedding_dim = output_embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = output_embedding_dim // num_heads\n",
    "        self.W_query = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
    "                                 bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
    "                               bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
    "                                 bias=qkv_bias)\n",
    "        self.output_projection = nn.Linear(output_embedding_dim,\n",
    "                                           output_embedding_dim)  # to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch, num_tokens, input_embedding_dim = inputs.shape\n",
    "\n",
    "        # qkv shapes : (batch, num_tokens, output_embedding_dim)\n",
    "        keys = self.W_key(inputs)\n",
    "        values = self.W_value(inputs)\n",
    "        queries = self.W_query(inputs)\n",
    "\n",
    "        # qkv shapes : (batch, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # qkv shapes : (batch, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "\n",
    "        # compute attention scores for each head\n",
    "        attention_scores = queries @ keys.transpose(3, 2)\n",
    "        attention_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], - torch.inf)\n",
    "\n",
    "        # compute attention weights + dropout\n",
    "        masked_attention_weight = torch.softmax(\n",
    "            attention_scores / (keys.shape[-1] ** 0.5),\n",
    "            dim=-1)\n",
    "        masked_attention_dropout_weight = self.dropout(masked_attention_weight)\n",
    "\n",
    "        # compute context vectors\n",
    "        # shape : (batch, num_tokens, num_heads, head_dim)\n",
    "        context_vector = (masked_attention_dropout_weight @ values).transpose(1, 2)\n",
    "\n",
    "        # combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        # shape : (batch, num_tokens, output_embedding_dim)\n",
    "        context_vector = context_vector.contiguous().view(\n",
    "            batch, num_tokens, self.output_embedding_dim)\n",
    "\n",
    "        # linear projection (optional)\n",
    "        context_vector = self.output_projection(context_vector)\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.epsilon = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1,\n",
    "                    unbiased=False,  # Bessel's correction (n-1)\n",
    "                    keepdim=True)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.epsilon)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(config[\"emb_dim\"],  # 768\n",
    "                      4 * config[\"emb_dim\"]),  # 3072\n",
    "            GELU(),  # 3072\n",
    "            nn.Linear(4 * config[\"emb_dim\"],  # 3072\n",
    "                      config[\"emb_dim\"])  # 768\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(input_embedding_dim=config[\"emb_dim\"],\n",
    "                                            output_embedding_dim=config[\"emb_dim\"],\n",
    "                                            context_length=config[\"context_length\"],\n",
    "                                            dropout=config[\"drop_rate\"],\n",
    "                                            num_heads=config[\"n_heads\"],\n",
    "                                            qkv_bias=config[\"qkv_bias\"])\n",
    "        self.feed_forward = FeedForward(config)\n",
    "        self.layer_norm1 = LayerNorm(config[\"emb_dim\"])\n",
    "        self.layer_norm2 = LayerNorm(config[\"emb_dim\"])\n",
    "        self.drop_skip = nn.Dropout(config[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # skip connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.layer_norm1(x)\n",
    "        x = self.attention(x)  # shape: [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_skip(x)\n",
    "        x = shortcut + x  # skip connection\n",
    "\n",
    "        # skip connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.layer_norm2(x)\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.drop_skip(x)\n",
    "        x = shortcut + x  # skip connection\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT2Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(config[\"vocab_size\"],\n",
    "                                      config[\"emb_dim\"])\n",
    "        self.position_emb = nn.Embedding(config[\"context_length\"],\n",
    "                                         config[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n",
    "\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(config) for _ in range(config[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(config[\"emb_dim\"])\n",
    "\n",
    "        self.out_head = nn.Linear(config[\"emb_dim\"],\n",
    "                                  config[\"vocab_size\"],\n",
    "                                  bias=False)\n",
    "\n",
    "    def forward(self, input_token):\n",
    "        batch_size, sequence_length = input_token.shape\n",
    "        token_embeds = self.token_emb(input_token)\n",
    "        position_embeds = self.position_emb(\n",
    "            torch.arange(sequence_length,\n",
    "                         device=input_token.device))\n",
    "        embeds = token_embeds + position_embeds\n",
    "        x = self.drop_emb(embeds)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.position_emb.weight = assign(gpt.position_emb.weight, params['wpe'])\n",
    "    gpt.token_emb.weight = assign(gpt.token_emb.weight, params['wte'])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.transformer_blocks[b].attention.W_query.weight = assign(\n",
    "            gpt.transformer_blocks[b].attention.W_query.weight, q_w.T)\n",
    "        gpt.transformer_blocks[b].attention.W_key.weight = assign(\n",
    "            gpt.transformer_blocks[b].attention.W_key.weight, k_w.T)\n",
    "        gpt.transformer_blocks[b].attention.W_value.weight = assign(\n",
    "            gpt.transformer_blocks[b].attention.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.transformer_blocks[b].attention.W_query.bias = assign(\n",
    "            gpt.transformer_blocks[b].attention.W_query.bias, q_b)\n",
    "        gpt.transformer_blocks[b].attention.W_key.bias = assign(\n",
    "            gpt.transformer_blocks[b].attention.W_key.bias, k_b)\n",
    "        gpt.transformer_blocks[b].attention.W_value.bias = assign(\n",
    "            gpt.transformer_blocks[b].attention.W_value.bias, v_b)\n",
    "\n",
    "        gpt.transformer_blocks[b].attention.output_projection.weight = assign(\n",
    "            gpt.transformer_blocks[b].attention.output_projection.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.transformer_blocks[b].attention.output_projection.bias = assign(\n",
    "            gpt.transformer_blocks[b].attention.output_projection.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.transformer_blocks[b].feed_forward.layers[0].weight = assign(\n",
    "            gpt.transformer_blocks[b].feed_forward.layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.transformer_blocks[b].feed_forward.layers[0].bias = assign(\n",
    "            gpt.transformer_blocks[b].feed_forward.layers[0].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.transformer_blocks[b].feed_forward.layers[2].weight = assign(\n",
    "            gpt.transformer_blocks[b].feed_forward.layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.transformer_blocks[b].feed_forward.layers[2].bias = assign(\n",
    "            gpt.transformer_blocks[b].feed_forward.layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.transformer_blocks[b].layer_norm1.scale = assign(\n",
    "            gpt.transformer_blocks[b].layer_norm1.scale,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.transformer_blocks[b].layer_norm1.shift = assign(\n",
    "            gpt.transformer_blocks[b].layer_norm1.shift,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.transformer_blocks[b].layer_norm2.scale = assign(\n",
    "            gpt.transformer_blocks[b].layer_norm2.scale,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.transformer_blocks[b].layer_norm2.shift = assign(\n",
    "            gpt.transformer_blocks[b].layer_norm2.shift,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ],
   "metadata": {
    "id": "BUb8nI3aqMvg"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "model_size"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "p0eIF-DTqMtM",
    "outputId": "9f65de59-f101-4502-db2e-d05f04fdb43c"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'124M'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 10
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "print(settings)\n",
    "print(params.keys())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kITVrRK_qMqz",
    "outputId": "0a7484d0-35cf-45d1-dcab-14024ad5c12e"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n",
      "{'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model = GPT2Model(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ],
   "metadata": {
    "id": "gltLlfrDqMn4"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "  encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "  # turn the list of token IDs into tensor with batch dimension\n",
    "  encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "  return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(encoded_tensor, tokenizer):\n",
    "  # turn tensor without batch dimension to list\n",
    "  token_ids = encoded_tensor.squeeze(0).tolist()\n",
    "  text = tokenizer.decode(token_ids)\n",
    "  return text\n",
    "\n",
    "\n",
    "def generate_text(model,\n",
    "                  input_batch,\n",
    "                  max_new_tokens,\n",
    "                  context_size,\n",
    "                  temperature=0.0,\n",
    "                  top_k=None,\n",
    "                  eos_id=None):\n",
    "  for _ in range(max_new_tokens):\n",
    "    # crop current context if it exceeds the supported context_size\n",
    "    crop_input_batch = input_batch[:, -context_size:]\n",
    "\n",
    "    # predict next token\n",
    "    with torch.no_grad():\n",
    "      logits = model(crop_input_batch)\n",
    "\n",
    "    # consider only logits of the last token\n",
    "    logits = logits[:, -1, :] # (batch, n_tokens, vocab_size) -> (batch, vocab_size)\n",
    "\n",
    "    # NEW: filter logits with top_k sampling\n",
    "    if top_k is not None:\n",
    "      # keep only top_k values\n",
    "      top_logits, _ = torch.topk(logits, top_k)\n",
    "      min_val = top_logits[:, -1] # min value among the top_k values\n",
    "      # all values other than top_k values will be set to -inf\n",
    "      logits = torch.where(logits < min_val,\n",
    "                           torch.tensor(-torch.inf).to(logits.device),\n",
    "                           logits)\n",
    "\n",
    "    # NEW: temperature scaling\n",
    "    if temperature > 0.0:\n",
    "      logits = logits / temperature\n",
    "\n",
    "      probas = torch.softmax(logits, dim=-1) # (batch, vocab_size)\n",
    "      predicted_tokens = torch.multinomial(probas, num_samples=1) # (batch, 1)\n",
    "\n",
    "    else: # same as before\n",
    "      #probas = torch.softmax(logits, dim=-1) # (batch, vocab_size)\n",
    "      predicted_tokens = torch.argmax(logits, dim=-1, keepdim=True) # (batch, 1)\n",
    "\n",
    "    if predicted_tokens == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "    # update input_batch (append predicted tokens to the sequences)\n",
    "    input_batch = torch.cat([input_batch, predicted_tokens], dim=1) # [batch, num_tokens+1]\n",
    "\n",
    "  return input_batch"
   ],
   "metadata": {
    "id": "vHoRGYTwqMlc"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "text_1 = \"In the midst of winter\"\n",
    "\n",
    "token_ids = generate_text(\n",
    "    model=model,\n",
    "    input_batch=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=50,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    temperature = 2.0,\n",
    "    top_k = 10\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "238McutFqMjK",
    "outputId": "3840e846-72db-4e46-895f-c98a45b9f06a"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "In the midst of winter, the wind blew down the trees that had been planted for winter and began the first winter-day harvest. The first year of the season was a good time to plant. It had the advantage in getting fresh fruits and dried berries. In the first\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "\n",
    "token_ids = generate_text(\n",
    "    model=model,\n",
    "    input_batch=text_to_token_ids(text_2, tokenizer),\n",
    "    max_new_tokens=23,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wRbHy89DqMhQ",
    "outputId": "1d0ac9cf-f7a9-4a8e-b04d-cc5d30021012"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
      "\n",
      "The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(model)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ecPyc7jdqMeM",
    "outputId": "17cb1d13-7515-4ce9-fe83-3ee17926606f"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPT2Model(\n",
      "  (token_emb): Embedding(50257, 768)\n",
      "  (position_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#!pip install torchinfo"
   ],
   "metadata": {
    "id": "rACi6cGnqMcB"
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(\n",
    "    model,\n",
    "    input_size=(1, BASE_CONFIG[\"context_length\"]),      # (batch, seq_len)\n",
    "    dtypes=[torch.long],                                # token IDs are int64\n",
    "    col_names=(\"input_size\", \"output_size\", \"num_params\", \"trainable\"),\n",
    "    row_settings=(\"depth\", \"var_names\"),                # valid row options\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Owsa3sTKqMZg",
    "outputId": "9b241187-5b94-47a3-c6fe-519ef50b7dd9"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "===========================================================================================================================================================\n",
       "Layer (type (var_name):depth-idx)                       Input Shape               Output Shape              Param #                   Trainable\n",
       "===========================================================================================================================================================\n",
       "GPT2Model (GPT2Model)                                   [1, 1024]                 [1, 1024, 50257]          --                        True\n",
       "├─Embedding (token_emb): 1-1                            [1, 1024]                 [1, 1024, 768]            38,597,376                True\n",
       "├─Embedding (position_emb): 1-2                         [1024]                    [1024, 768]               786,432                   True\n",
       "├─Dropout (drop_emb): 1-3                               [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "├─Sequential (transformer_blocks): 1-4                  [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    └─TransformerBlock (0): 2-1                        [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    │    └─LayerNorm (layer_norm1): 3-1                [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─MultiHeadAttention (attention): 3-2         [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
       "│    │    └─Dropout (drop_skip): 3-3                    [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    │    └─LayerNorm (layer_norm2): 3-4                [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─FeedForward (feed_forward): 3-5             [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
       "│    │    └─Dropout (drop_skip): 3-6                    [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    └─TransformerBlock (1): 2-2                        [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    │    └─LayerNorm (layer_norm1): 3-7                [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─MultiHeadAttention (attention): 3-8         [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
       "│    │    └─Dropout (drop_skip): 3-9                    [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    │    └─LayerNorm (layer_norm2): 3-10               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─FeedForward (feed_forward): 3-11            [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
       "│    │    └─Dropout (drop_skip): 3-12                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    └─TransformerBlock (2): 2-3                        [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    │    └─LayerNorm (layer_norm1): 3-13               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─MultiHeadAttention (attention): 3-14        [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
       "│    │    └─Dropout (drop_skip): 3-15                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    │    └─LayerNorm (layer_norm2): 3-16               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─FeedForward (feed_forward): 3-17            [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
       "│    │    └─Dropout (drop_skip): 3-18                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    └─TransformerBlock (3): 2-4                        [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    │    └─LayerNorm (layer_norm1): 3-19               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─MultiHeadAttention (attention): 3-20        [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
       "│    │    └─Dropout (drop_skip): 3-21                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    │    └─LayerNorm (layer_norm2): 3-22               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─FeedForward (feed_forward): 3-23            [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
       "│    │    └─Dropout (drop_skip): 3-24                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    └─TransformerBlock (4): 2-5                        [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    │    └─LayerNorm (layer_norm1): 3-25               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─MultiHeadAttention (attention): 3-26        [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
       "│    │    └─Dropout (drop_skip): 3-27                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    │    └─LayerNorm (layer_norm2): 3-28               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─FeedForward (feed_forward): 3-29            [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
       "│    │    └─Dropout (drop_skip): 3-30                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    └─TransformerBlock (5): 2-6                        [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    │    └─LayerNorm (layer_norm1): 3-31               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─MultiHeadAttention (attention): 3-32        [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
       "│    │    └─Dropout (drop_skip): 3-33                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    │    └─LayerNorm (layer_norm2): 3-34               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─FeedForward (feed_forward): 3-35            [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
       "│    │    └─Dropout (drop_skip): 3-36                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    └─TransformerBlock (6): 2-7                        [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    │    └─LayerNorm (layer_norm1): 3-37               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─MultiHeadAttention (attention): 3-38        [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
       "│    │    └─Dropout (drop_skip): 3-39                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    │    └─LayerNorm (layer_norm2): 3-40               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─FeedForward (feed_forward): 3-41            [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
       "│    │    └─Dropout (drop_skip): 3-42                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    └─TransformerBlock (7): 2-8                        [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    │    └─LayerNorm (layer_norm1): 3-43               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─MultiHeadAttention (attention): 3-44        [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
       "│    │    └─Dropout (drop_skip): 3-45                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    │    └─LayerNorm (layer_norm2): 3-46               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─FeedForward (feed_forward): 3-47            [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
       "│    │    └─Dropout (drop_skip): 3-48                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    └─TransformerBlock (8): 2-9                        [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    │    └─LayerNorm (layer_norm1): 3-49               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─MultiHeadAttention (attention): 3-50        [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
       "│    │    └─Dropout (drop_skip): 3-51                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    │    └─LayerNorm (layer_norm2): 3-52               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─FeedForward (feed_forward): 3-53            [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
       "│    │    └─Dropout (drop_skip): 3-54                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    └─TransformerBlock (9): 2-10                       [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    │    └─LayerNorm (layer_norm1): 3-55               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─MultiHeadAttention (attention): 3-56        [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
       "│    │    └─Dropout (drop_skip): 3-57                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    │    └─LayerNorm (layer_norm2): 3-58               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─FeedForward (feed_forward): 3-59            [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
       "│    │    └─Dropout (drop_skip): 3-60                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    └─TransformerBlock (10): 2-11                      [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    │    └─LayerNorm (layer_norm1): 3-61               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─MultiHeadAttention (attention): 3-62        [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
       "│    │    └─Dropout (drop_skip): 3-63                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    │    └─LayerNorm (layer_norm2): 3-64               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─FeedForward (feed_forward): 3-65            [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
       "│    │    └─Dropout (drop_skip): 3-66                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    └─TransformerBlock (11): 2-12                      [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    │    └─LayerNorm (layer_norm1): 3-67               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─MultiHeadAttention (attention): 3-68        [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
       "│    │    └─Dropout (drop_skip): 3-69                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    │    └─LayerNorm (layer_norm2): 3-70               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─FeedForward (feed_forward): 3-71            [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
       "│    │    └─Dropout (drop_skip): 3-72                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "├─LayerNorm (final_norm): 1-5                           [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "├─Linear (out_head): 1-6                                [1, 1024, 768]            [1, 1024, 50257]          38,597,376                True\n",
       "===========================================================================================================================================================\n",
       "Total params: 163,037,184\n",
       "Trainable params: 163,037,184\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 967.52\n",
       "===========================================================================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 1261.05\n",
       "Params size (MB): 652.15\n",
       "Estimated Total Size (MB): 1913.21\n",
       "==========================================================================================================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "torch.manual_seed(211)\n",
    "\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)"
   ],
   "metadata": {
    "id": "K7y0Vdp1qMVC"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(\n",
    "    model,\n",
    "    input_size=(1, BASE_CONFIG[\"context_length\"]),      # (batch, seq_len)\n",
    "    dtypes=[torch.long],                                # token IDs are int64\n",
    "    col_names=(\"input_size\", \"output_size\", \"num_params\", \"trainable\"),\n",
    "    row_settings=(\"depth\", \"var_names\"),                # valid row options\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cOKGMEbBqMQj",
    "outputId": "60ecd575-f57b-4053-dcef-eff8560a1560"
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "===========================================================================================================================================================\n",
       "Layer (type (var_name):depth-idx)                       Input Shape               Output Shape              Param #                   Trainable\n",
       "===========================================================================================================================================================\n",
       "GPT2Model (GPT2Model)                                   [1, 1024]                 [1, 1024, 2]              --                        True\n",
       "├─Embedding (token_emb): 1-1                            [1, 1024]                 [1, 1024, 768]            38,597,376                True\n",
       "├─Embedding (position_emb): 1-2                         [1024]                    [1024, 768]               786,432                   True\n",
       "├─Dropout (drop_emb): 1-3                               [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "├─Sequential (transformer_blocks): 1-4                  [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    └─TransformerBlock (0): 2-1                        [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    │    └─LayerNorm (layer_norm1): 3-1                [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─MultiHeadAttention (attention): 3-2         [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
       "│    │    └─Dropout (drop_skip): 3-3                    [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    │    └─LayerNorm (layer_norm2): 3-4                [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─FeedForward (feed_forward): 3-5             [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
       "│    │    └─Dropout (drop_skip): 3-6                    [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    └─TransformerBlock (1): 2-2                        [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    │    └─LayerNorm (layer_norm1): 3-7                [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─MultiHeadAttention (attention): 3-8         [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
       "│    │    └─Dropout (drop_skip): 3-9                    [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    │    └─LayerNorm (layer_norm2): 3-10               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─FeedForward (feed_forward): 3-11            [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
       "│    │    └─Dropout (drop_skip): 3-12                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    └─TransformerBlock (2): 2-3                        [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    │    └─LayerNorm (layer_norm1): 3-13               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─MultiHeadAttention (attention): 3-14        [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
       "│    │    └─Dropout (drop_skip): 3-15                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    │    └─LayerNorm (layer_norm2): 3-16               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─FeedForward (feed_forward): 3-17            [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
       "│    │    └─Dropout (drop_skip): 3-18                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    └─TransformerBlock (3): 2-4                        [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    │    └─LayerNorm (layer_norm1): 3-19               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─MultiHeadAttention (attention): 3-20        [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
       "│    │    └─Dropout (drop_skip): 3-21                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    │    └─LayerNorm (layer_norm2): 3-22               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─FeedForward (feed_forward): 3-23            [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
       "│    │    └─Dropout (drop_skip): 3-24                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    └─TransformerBlock (4): 2-5                        [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    │    └─LayerNorm (layer_norm1): 3-25               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─MultiHeadAttention (attention): 3-26        [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
       "│    │    └─Dropout (drop_skip): 3-27                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    │    └─LayerNorm (layer_norm2): 3-28               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─FeedForward (feed_forward): 3-29            [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
       "│    │    └─Dropout (drop_skip): 3-30                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    └─TransformerBlock (5): 2-6                        [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    │    └─LayerNorm (layer_norm1): 3-31               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─MultiHeadAttention (attention): 3-32        [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
       "│    │    └─Dropout (drop_skip): 3-33                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    │    └─LayerNorm (layer_norm2): 3-34               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─FeedForward (feed_forward): 3-35            [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
       "│    │    └─Dropout (drop_skip): 3-36                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    └─TransformerBlock (6): 2-7                        [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    │    └─LayerNorm (layer_norm1): 3-37               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─MultiHeadAttention (attention): 3-38        [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
       "│    │    └─Dropout (drop_skip): 3-39                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    │    └─LayerNorm (layer_norm2): 3-40               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─FeedForward (feed_forward): 3-41            [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
       "│    │    └─Dropout (drop_skip): 3-42                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    └─TransformerBlock (7): 2-8                        [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    │    └─LayerNorm (layer_norm1): 3-43               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─MultiHeadAttention (attention): 3-44        [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
       "│    │    └─Dropout (drop_skip): 3-45                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    │    └─LayerNorm (layer_norm2): 3-46               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─FeedForward (feed_forward): 3-47            [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
       "│    │    └─Dropout (drop_skip): 3-48                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    └─TransformerBlock (8): 2-9                        [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    │    └─LayerNorm (layer_norm1): 3-49               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─MultiHeadAttention (attention): 3-50        [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
       "│    │    └─Dropout (drop_skip): 3-51                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    │    └─LayerNorm (layer_norm2): 3-52               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─FeedForward (feed_forward): 3-53            [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
       "│    │    └─Dropout (drop_skip): 3-54                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    └─TransformerBlock (9): 2-10                       [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    │    └─LayerNorm (layer_norm1): 3-55               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─MultiHeadAttention (attention): 3-56        [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
       "│    │    └─Dropout (drop_skip): 3-57                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    │    └─LayerNorm (layer_norm2): 3-58               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─FeedForward (feed_forward): 3-59            [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
       "│    │    └─Dropout (drop_skip): 3-60                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    └─TransformerBlock (10): 2-11                      [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    │    └─LayerNorm (layer_norm1): 3-61               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─MultiHeadAttention (attention): 3-62        [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
       "│    │    └─Dropout (drop_skip): 3-63                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    │    └─LayerNorm (layer_norm2): 3-64               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─FeedForward (feed_forward): 3-65            [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
       "│    │    └─Dropout (drop_skip): 3-66                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    └─TransformerBlock (11): 2-12                      [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
       "│    │    └─LayerNorm (layer_norm1): 3-67               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─MultiHeadAttention (attention): 3-68        [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
       "│    │    └─Dropout (drop_skip): 3-69                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "│    │    └─LayerNorm (layer_norm2): 3-70               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "│    │    └─FeedForward (feed_forward): 3-71            [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
       "│    │    └─Dropout (drop_skip): 3-72                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
       "├─LayerNorm (final_norm): 1-5                           [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
       "├─Linear (out_head): 1-6                                [1, 1024, 768]            [1, 1024, 2]              1,538                     True\n",
       "===========================================================================================================================================================\n",
       "Total params: 124,441,346\n",
       "Trainable params: 124,441,346\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 928.92\n",
       "===========================================================================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 849.36\n",
       "Params size (MB): 497.77\n",
       "Estimated Total Size (MB): 1347.14\n",
       "==========================================================================================================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model.to(device);"
   ],
   "metadata": {
    "id": "IKHWuACMqMNl"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def calc_accuracy_loader(data_loader,\n",
    "                         model,\n",
    "                         device,\n",
    "                         num_batches=None):\n",
    "  model.eval()\n",
    "  correct_predictions, num_examples = 0, 0\n",
    "\n",
    "  if num_batches is None:\n",
    "    num_batches = len(data_loader)\n",
    "  else:\n",
    "    num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "  for batch_id, (input_batch, label_batch) in enumerate(data_loader):\n",
    "    if batch_id < num_batches:\n",
    "      input_batch, label_batch = input_batch.to(device), label_batch.to(device)\n",
    "\n",
    "      with torch.no_grad():\n",
    "        logits = model(input_batch)[:, -1, :] # logits of the last output token\n",
    "      predicted_labels = torch.argmax(logits, dim=-1)\n",
    "\n",
    "      num_examples += predicted_labels.shape[0]\n",
    "      correct_predictions += (predicted_labels == label_batch).sum().item()\n",
    "    else:\n",
    "      break\n",
    "  return correct_predictions / num_examples\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")\n",
    "\n",
    "\n",
    "torch.manual_seed(211)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7gm9c0sqMLc",
    "outputId": "eeb4fbf3-db2c-4679-b7b5-547a97ea3a16"
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n",
      "Training accuracy: 51.25%\n",
      "Validation accuracy: 58.75%\n",
      "Test accuracy: 37.50%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Parameter-Efficient Finetuning (PEFT) with LoRA"
   ],
   "metadata": {
    "id": "9_WoZH7xLD5w"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Let's create the LoRA layer:"
   ],
   "metadata": {
    "id": "nC-Id4gyOKlf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "  def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "    super().__init__()\n",
    "    self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
    "    # Kaiming/He uniform initialization, similar to standard weight initialization\n",
    "    torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "    self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "    self.alpha = alpha\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.alpha * (x @ self.A @ self.B)\n",
    "    return x"
   ],
   "metadata": {
    "id": "XxcM-f1dqMJP"
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Now, let's create the Linear layer that incoporates LoRA:"
   ],
   "metadata": {
    "id": "xeZqlCVWONsz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class LinearLayerWithLoRA(torch.nn.Module):\n",
    "  def __init__(self, linear, rank, alpha):\n",
    "    super().__init__()\n",
    "    self.linear = linear\n",
    "    self.lora = LoRALayer(linear.in_features,\n",
    "                          linear.out_features,\n",
    "                          rank,\n",
    "                          alpha)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.linear(x) + self.lora(x)"
   ],
   "metadata": {
    "id": "YI0CCGSlqMID"
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's define a function to replace linear layers with our `LinearLayer WithLoRA` layer:"
   ],
   "metadata": {
    "id": "mV-gr-IWQOh4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for name, module in model.named_children():\n",
    "  print(name)\n",
    "  #print(module)\n",
    "  print(\"\\n\\n\\n\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vg7RnGQjRRew",
    "outputId": "35115de9-ebbe-427d-d998-ee7b6b515ecb"
   },
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "token_emb\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "position_emb\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "drop_emb\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "transformer_blocks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "final_norm\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "out_head\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def replace_linear_with_lora(model, rank, alpha):\n",
    "  for name, module in model.named_children():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "      print(f\"Replacing {name} with LinearLayerWithLoRA\")\n",
    "      setattr(model, name, LinearLayerWithLoRA(module, rank, alpha))\n",
    "    else:\n",
    "      # recursively apply the same function to child modules\n",
    "      replace_linear_with_lora(module, rank, alpha)"
   ],
   "metadata": {
    "id": "R2dLLnBWqMEC"
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Let's freeze the parameters:"
   ],
   "metadata": {
    "id": "_HHAW4jvR9J3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters before: {total_params:,}\")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters after: {total_params:,}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nqAzNLXrqKhK",
    "outputId": "dc10d791-4edd-403c-a2a9-df036582e30c"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total trainable parameters before: 124,441,346\n",
      "Total trainable parameters after: 0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "replace_linear_with_lora(model, rank=16, alpha=16)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable LoRA parameters: {total_params:,}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p8qyfEuvSC8h",
    "outputId": "7b6b1fb9-cab8-4c20-f6c9-f3998a957b61"
   },
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Replacing W_query with LinearLayerWithLoRA\n",
      "Replacing W_key with LinearLayerWithLoRA\n",
      "Replacing W_value with LinearLayerWithLoRA\n",
      "Replacing output_projection with LinearLayerWithLoRA\n",
      "Replacing 0 with LinearLayerWithLoRA\n",
      "Replacing 2 with LinearLayerWithLoRA\n",
      "Replacing W_query with LinearLayerWithLoRA\n",
      "Replacing W_key with LinearLayerWithLoRA\n",
      "Replacing W_value with LinearLayerWithLoRA\n",
      "Replacing output_projection with LinearLayerWithLoRA\n",
      "Replacing 0 with LinearLayerWithLoRA\n",
      "Replacing 2 with LinearLayerWithLoRA\n",
      "Replacing W_query with LinearLayerWithLoRA\n",
      "Replacing W_key with LinearLayerWithLoRA\n",
      "Replacing W_value with LinearLayerWithLoRA\n",
      "Replacing output_projection with LinearLayerWithLoRA\n",
      "Replacing 0 with LinearLayerWithLoRA\n",
      "Replacing 2 with LinearLayerWithLoRA\n",
      "Replacing W_query with LinearLayerWithLoRA\n",
      "Replacing W_key with LinearLayerWithLoRA\n",
      "Replacing W_value with LinearLayerWithLoRA\n",
      "Replacing output_projection with LinearLayerWithLoRA\n",
      "Replacing 0 with LinearLayerWithLoRA\n",
      "Replacing 2 with LinearLayerWithLoRA\n",
      "Replacing W_query with LinearLayerWithLoRA\n",
      "Replacing W_key with LinearLayerWithLoRA\n",
      "Replacing W_value with LinearLayerWithLoRA\n",
      "Replacing output_projection with LinearLayerWithLoRA\n",
      "Replacing 0 with LinearLayerWithLoRA\n",
      "Replacing 2 with LinearLayerWithLoRA\n",
      "Replacing W_query with LinearLayerWithLoRA\n",
      "Replacing W_key with LinearLayerWithLoRA\n",
      "Replacing W_value with LinearLayerWithLoRA\n",
      "Replacing output_projection with LinearLayerWithLoRA\n",
      "Replacing 0 with LinearLayerWithLoRA\n",
      "Replacing 2 with LinearLayerWithLoRA\n",
      "Replacing W_query with LinearLayerWithLoRA\n",
      "Replacing W_key with LinearLayerWithLoRA\n",
      "Replacing W_value with LinearLayerWithLoRA\n",
      "Replacing output_projection with LinearLayerWithLoRA\n",
      "Replacing 0 with LinearLayerWithLoRA\n",
      "Replacing 2 with LinearLayerWithLoRA\n",
      "Replacing W_query with LinearLayerWithLoRA\n",
      "Replacing W_key with LinearLayerWithLoRA\n",
      "Replacing W_value with LinearLayerWithLoRA\n",
      "Replacing output_projection with LinearLayerWithLoRA\n",
      "Replacing 0 with LinearLayerWithLoRA\n",
      "Replacing 2 with LinearLayerWithLoRA\n",
      "Replacing W_query with LinearLayerWithLoRA\n",
      "Replacing W_key with LinearLayerWithLoRA\n",
      "Replacing W_value with LinearLayerWithLoRA\n",
      "Replacing output_projection with LinearLayerWithLoRA\n",
      "Replacing 0 with LinearLayerWithLoRA\n",
      "Replacing 2 with LinearLayerWithLoRA\n",
      "Replacing W_query with LinearLayerWithLoRA\n",
      "Replacing W_key with LinearLayerWithLoRA\n",
      "Replacing W_value with LinearLayerWithLoRA\n",
      "Replacing output_projection with LinearLayerWithLoRA\n",
      "Replacing 0 with LinearLayerWithLoRA\n",
      "Replacing 2 with LinearLayerWithLoRA\n",
      "Replacing W_query with LinearLayerWithLoRA\n",
      "Replacing W_key with LinearLayerWithLoRA\n",
      "Replacing W_value with LinearLayerWithLoRA\n",
      "Replacing output_projection with LinearLayerWithLoRA\n",
      "Replacing 0 with LinearLayerWithLoRA\n",
      "Replacing 2 with LinearLayerWithLoRA\n",
      "Replacing W_query with LinearLayerWithLoRA\n",
      "Replacing W_key with LinearLayerWithLoRA\n",
      "Replacing W_value with LinearLayerWithLoRA\n",
      "Replacing output_projection with LinearLayerWithLoRA\n",
      "Replacing 0 with LinearLayerWithLoRA\n",
      "Replacing 2 with LinearLayerWithLoRA\n",
      "Replacing out_head with LinearLayerWithLoRA\n",
      "Total trainable LoRA parameters: 2,666,528\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Those mysterious “0” and “2” are not top-level layers of the Transformer; they are the numeric names that PyTorch automatically gives to the sub-modules living inside an nn.Sequential container.\n",
    "\n",
    "In Sebastian Raschka’s minimal-GPT code every Transformer block contains a position-wise feed-forward network (FFN) that is built like this (simplified):\n",
    "```python\n",
    "self.ffn = nn.Sequential(\n",
    "    nn.Linear(d_model, 4 * d_model, bias=False),   # index 0\n",
    "    nn.GELU(),                                     # index 1\n",
    "    nn.Linear(4 * d_model, d_model, bias=False),   # index 2\n",
    "    nn.Dropout(p_dropout)                          # index 3 (if present)\n",
    ")\n",
    "```\n",
    "\n",
    "When you iterate over block.ffn.named_modules() PyTorch yields\n",
    "\n",
    "| `name` string | object type                                     |\n",
    "| ------------- | ----------------------------------------------- |\n",
    "| `\"0\"`         | first `nn.Linear` (expands hidden size)         |\n",
    "| `\"1\"`         | `nn.GELU`                                       |\n",
    "| `\"2\"`         | second `nn.Linear` (projects back to `d_model`) |\n",
    "| `\"3\"`         | `nn.Dropout` (if included)                      |\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "V3l7lts8Y1Nu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L-zyJqoTSC6M",
    "outputId": "9c34e611-c158-44b9-eeac-542423dc359e"
   },
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPT2Model(\n",
      "  (token_emb): Embedding(50257, 768)\n",
      "  (position_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (output_projection): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearLayerWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearLayerWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (output_projection): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearLayerWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearLayerWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (output_projection): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearLayerWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearLayerWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (output_projection): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearLayerWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearLayerWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (output_projection): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearLayerWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearLayerWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (output_projection): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearLayerWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearLayerWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (output_projection): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearLayerWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearLayerWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (output_projection): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearLayerWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearLayerWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (output_projection): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearLayerWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearLayerWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (output_projection): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearLayerWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearLayerWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (output_projection): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearLayerWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearLayerWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (output_projection): LinearLayerWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearLayerWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearLayerWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): LinearLayerWithLoRA(\n",
      "    (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Let's take a look at the accuracy again:"
   ],
   "metadata": {
    "id": "Ltiykvz4aRxZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "torch.manual_seed(211)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FTwd_aRgSC4a",
    "outputId": "d5b1a01b-bbd2-429e-95cd-0c927ed5439b"
   },
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training accuracy: 51.25%\n",
      "Validation accuracy: 58.75%\n",
      "Test accuracy: 37.50%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def calc_loss_batch(input_batch,\n",
    "                    label_batch,\n",
    "                    model,\n",
    "                    device):\n",
    "  input_batch, label_batch = input_batch.to(device), label_batch.to(device)\n",
    "  logits = model(input_batch)[:, -1, :] # logits of the last output token\n",
    "  loss = torch.nn.functional.cross_entropy(logits, label_batch)\n",
    "  return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader,\n",
    "                     model,\n",
    "                     device,\n",
    "                     num_batches=None):\n",
    "  total_loss = 0.0\n",
    "  if len(data_loader) == 0:\n",
    "    return float(\"nan\")\n",
    "  elif num_batches is None:\n",
    "    num_batches = len(data_loader)\n",
    "  else:\n",
    "    num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "  for batch_id, (input_batch, label_batch) in enumerate(data_loader):\n",
    "    if batch_id < num_batches:\n",
    "      loss = calc_loss_batch(input_batch, label_batch, model, device)\n",
    "      total_loss += loss.item()\n",
    "    else:\n",
    "      break\n",
    "  return total_loss / num_batches\n",
    "\n",
    "\n",
    "def train_classifier_simple(model,\n",
    "                            train_loader,\n",
    "                            val_loader,\n",
    "                            optimizer,\n",
    "                            device,\n",
    "                            num_epochs,\n",
    "                            eval_freq,\n",
    "                            eval_iter # number of batches to evaluate from\n",
    "                            ):\n",
    "  train_losses, val_losses = [], []\n",
    "  train_accuracies, val_accuracies = [], []\n",
    "  examples_seen, global_step = 0, -1\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for input_batch, label_batch in train_loader:\n",
    "      optimizer.zero_grad()\n",
    "      loss = calc_loss_batch(input_batch, label_batch, model, device)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      examples_seen += input_batch.shape[0] # New: track examples instead of tokens\n",
    "      global_step += 1\n",
    "\n",
    "\n",
    "      # optional evaluation step\n",
    "      if global_step % eval_freq == 0:\n",
    "        train_loss, val_loss = evaluate_model(model,\n",
    "                                              train_loader,\n",
    "                                              val_loader,\n",
    "                                              device,\n",
    "                                              eval_iter)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "    # calculate accuracy after each epoch\n",
    "    train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "    val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "    print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "  return train_losses, val_losses, train_accuracies, val_accuracies, examples_seen\n",
    "\n",
    "\n",
    "# same as pretraining\n",
    "def evaluate_model(model,\n",
    "                   train_loader,\n",
    "                   val_loader,\n",
    "                   device,\n",
    "                   eval_iter):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "  model.train()\n",
    "  return train_loss, val_loss"
   ],
   "metadata": {
    "id": "UXE3wcxWSC0v"
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(211)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s9YiIykxSCyZ",
    "outputId": "c304461d-4727-4ef6-b342-8c2888418c57"
   },
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.682, Val loss 3.386\n",
      "Ep 1 (Step 000050): Train loss 0.468, Val loss 0.332\n",
      "Ep 1 (Step 000100): Train loss 0.075, Val loss 0.089\n",
      "Training accuracy: 92.50% | Validation accuracy: 95.00%\n",
      "Ep 2 (Step 000150): Train loss 0.149, Val loss 0.071\n",
      "Ep 2 (Step 000200): Train loss 0.076, Val loss 0.013\n",
      "Ep 2 (Step 000250): Train loss 0.028, Val loss 0.055\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Ep 3 (Step 000300): Train loss 0.008, Val loss 0.043\n",
      "Ep 3 (Step 000350): Train loss 0.017, Val loss 0.043\n",
      "Training accuracy: 97.50% | Validation accuracy: 100.00%\n",
      "Ep 4 (Step 000400): Train loss 0.005, Val loss 0.053\n",
      "Ep 4 (Step 000450): Train loss 0.007, Val loss 0.004\n",
      "Ep 4 (Step 000500): Train loss 0.048, Val loss 0.005\n",
      "Training accuracy: 100.00% | Validation accuracy: 95.00%\n",
      "Ep 5 (Step 000550): Train loss 0.001, Val loss 0.003\n",
      "Ep 5 (Step 000600): Train loss 0.000, Val loss 0.009\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Ep 6 (Step 000650): Train loss 0.000, Val loss 0.053\n",
      "Ep 6 (Step 000700): Train loss 0.004, Val loss 0.145\n",
      "Ep 6 (Step 000750): Train loss 0.000, Val loss 0.062\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Ep 7 (Step 000800): Train loss 0.000, Val loss 0.049\n",
      "Ep 7 (Step 000850): Train loss 0.000, Val loss 0.056\n",
      "Ep 7 (Step 000900): Train loss 0.026, Val loss 0.039\n",
      "Training accuracy: 97.50% | Validation accuracy: 100.00%\n",
      "Ep 8 (Step 000950): Train loss 0.002, Val loss 0.058\n",
      "Ep 8 (Step 001000): Train loss 0.006, Val loss 0.005\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Ep 9 (Step 001050): Train loss 0.008, Val loss 0.039\n",
      "Ep 9 (Step 001100): Train loss 0.001, Val loss 0.019\n",
      "Ep 9 (Step 001150): Train loss 0.001, Val loss 0.032\n",
      "Training accuracy: 100.00% | Validation accuracy: 100.00%\n",
      "Ep 10 (Step 001200): Train loss 0.037, Val loss 0.004\n",
      "Ep 10 (Step 001250): Train loss 0.001, Val loss 0.023\n",
      "Training accuracy: 100.00% | Validation accuracy: 100.00%\n",
      "Training completed in 1.98 minutes.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "\n",
    "    # Create a second x-axis for examples seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(examples_seen, train_values, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Examples seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(f\"{label}-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses, label=\"loss\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "L4nwhrHJSCwA",
    "outputId": "0c25674b-9042-49e7-c1b5-de6e5b5efd5f"
   },
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdwAAAEiCAYAAABTO2OcAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAATwxJREFUeJzt3Xd4FNX6wPHv7G52k00vkAIktNBblHIBC16QIqLYUC5qsF41CMhFkYsU9YdBRcV2USygVwRFxWtBqoCK9I7EIBIILQQI6ckm2T2/PzZZsiRgEpJMAu/neeZhZ+bszDuHzb57Zs7M0ZRSCiGEEELUKIPeAQghhBCXA0m4QgghRC2QhCuEEELUAkm4QgghRC2QhCuEEELUAkm4QgghRC2QhCuEEELUAkm4QgghRC2QhCuEEELUAkm4QlyG+vTpw9ixY/UOQ4jLiiRcIapg5MiRaJpWZho4cKDeoQkh6iiT3gEIUV8NHDiQuXPnui2zWCw6RSOEqOukhStEFVksFsLCwtymwMBAANasWYPZbObnn392lX/ppZdo2LAhJ06cAGDp0qVcddVVBAQEEBwczI033siff/7pKn/w4EE0TePzzz/n6quvxsvLi27durFv3z42b95M165d8fHxYdCgQZw8edL1vpEjRzJ06FCeffZZGjRogJ+fH4888ggFBQXnPRabzcb48eNp1KgR3t7e9OjRgzVr1rjWHzp0iCFDhhAYGIi3tzft27dnyZIl593ef/7zH6Kjo/H09CQ0NJTbb7/dtc7hcBAfH0+zZs3w8vKic+fOfPHFF27v37NnD4MGDcLHx4fQ0FDuueceTp065Vrfp08fRo8ezVNPPUVQUBBhYWFMmzbtvPEIURdIwhWiBpRcI73nnnvIyMhg+/btTJ48mffff5/Q0FAAcnJyGDduHFu2bGHVqlUYDAZuueUWHA6H27amTp3KM888w7Zt2zCZTPzjH//gqaee4vXXX+fnn39m//79TJkyxe09q1atIiEhgTVr1rBgwQK++uornn322fPGO2rUKNavX8/ChQvZtWsXd9xxBwMHDuSPP/4AIC4uDpvNxk8//cTu3bt58cUX8fHxKXdbW7ZsYfTo0Tz33HMkJiaydOlSrrnmGtf6+Ph4Pv74Y9555x1+++03nnjiCe6++27Wrl0LQHp6On//+9+JiYlhy5YtLF26lBMnTjBs2DC3/Xz00Ud4e3uzceNGXnrpJZ577jlWrFhRwf8hIXSghBCVFhsbq4xGo/L29nabpk+f7ipjs9lUly5d1LBhw1S7du3UQw89dMFtnjx5UgFq9+7dSimlkpKSFKDef/99V5kFCxYoQK1atcq1LD4+XrVu3dottqCgIJWTk+NaNnv2bOXj46PsdrtSSqlrr71WjRkzRiml1KFDh5TRaFRHjx51i6dv375q4sSJSimlOnbsqKZNm1ahuvnyyy+Vn5+fyszMLLMuPz9fWa1W9euvv7otf+CBB9Tw4cOVUko9//zzqn///m7rDx8+rACVmJjoiv+qq65yK9OtWzc1YcKECsUohB7kGq4QVXTdddcxe/Zst2VBQUGu12azmfnz59OpUyeioqJ47bXX3Mr+8ccfTJkyhY0bN3Lq1ClXyzY5OZkOHTq4ynXq1Mn1uqR13LFjR7dlqampbtvu3LkzVqvVNd+zZ0+ys7M5fPgwUVFRbmV3796N3W6nVatWbsttNhvBwcEAjB49mkcffZTly5fTr18/brvtNre4Srv++uuJioqiefPmDBw4kIEDB3LLLbdgtVrZv38/ubm5XH/99W7vKSgoICYmBoCdO3eyevXqclvQf/75pyvOc/cfHh5eph6EqEsk4QpRRd7e3rRs2fKCZX799VcA0tLSSEtLw9vb27VuyJAhREVF8d577xEREYHD4aBDhw5lrrV6eHi4XmuaVu6yc09DV0Z2djZGo5GtW7diNBrd1pUkvQcffJABAwbw/fffs3z5cuLj43nllVd4/PHHy2zP19eXbdu2sWbNGpYvX86UKVOYNm0amzdvJjs7G4Dvv/+eRo0aub2vpMNZdnY2Q4YM4cUXXyyz7fDwcNfr0nUAF18PQtQ0SbhC1JA///yTJ554gvfee4/PPvuM2NhYVq5cicFg4PTp0yQmJvLee+9x9dVXA/DLL79U27537txJXl4eXl5eAGzYsAEfHx+aNGlSpmxMTAx2u53U1FRXLOVp0qQJjzzyCI888ggTJ07kvffeKzfhAphMJvr160e/fv2YOnUqAQEB/Pjjj1x//fVYLBaSk5O59tpry33vFVdcwZdffknTpk0xmeQrSlw65NMsRBXZbDZSUlLclplMJkJCQrDb7dx9990MGDCA++67j4EDB9KxY0deeeUVnnzySQIDAwkODmbOnDmEh4eTnJzM008/XW2xFRQU8MADD/DMM89w8OBBpk6dyqhRozAYyvaTbNWqFSNGjODee+/llVdeISYmhpMnT7Jq1So6derE4MGDGTt2LIMGDaJVq1acOXOG1atX07Zt23L3/d1333HgwAGuueYaAgMDWbJkCQ6Hg9atW+Pr68v48eN54okncDgcXHXVVWRkZLBu3Tr8/PyIjY0lLi6O9957j+HDh7t6Ie/fv5+FCxfy/vvvl2mFC1FfSMIVooqWLl3qdooToHXr1vz+++9Mnz6dQ4cO8d133wHOU6Fz5sxh+PDh9O/fn86dO7Nw4UJGjx5Nhw4daN26NW+88QZ9+vSpltj69u1LdHQ011xzDTabjeHDh1/wtpm5c+fyf//3f/zrX//i6NGjhISE8Le//Y0bb7wRALvdTlxcHEeOHMHPz4+BAweWuSZdIiAggK+++opp06aRn59PdHQ0CxYsoH379gA8//zzNGjQgPj4eA4cOEBAQABXXHEF//73vwGIiIhg3bp1TJgwgf79+2Oz2YiKimLgwIHl/mAQor7QlFJK7yCEENVn5MiRpKen8/XXX+sdihCiFPm5KIQQQtQCSbhCCCFELZBTykIIIUQtkBauEEIIUQsk4QohhBC1QBKuEEIIUQsk4RZ7++23adq0KZ6envTo0YNNmzbpHVKNiI+Pp1u3bvj6+tKwYUOGDh1KYmKiW5n8/Hzi4uIIDg7Gx8eH2267zTWkXInk5GQGDx6M1WqlYcOGPPnkkxQVFbmVWbNmDVdccQUWi4WWLVsyb968mj68GjFjxgw0TWPs2LGuZVJHcPToUe6++26Cg4Px8vKiY8eObNmyxbVeKcWUKVMIDw/Hy8uLfv36uUYfKpGWlsaIESPw8/MjICCABx54wPX4xxK7du3i6quvxtPTkyZNmvDSSy/VyvFdLLvdzuTJk13DELZo0YLnn3+e0t1mLsc6+umnnxgyZAgRERFomlbm9rXarJNFixbRpk0bPD096dix4wWHnKwW+o2bUHcsXLhQmc1m9eGHH6rffvtNPfTQQyogIECdOHFC79Cq3YABA9TcuXPVnj171I4dO9QNN9ygIiMjVXZ2tqvMI488opo0aaJWrVqltmzZov72t7+pXr16udYXFRWpDh06qH79+qnt27erJUuWqJCQENfIMkopdeDAAWW1WtW4cePU3r171ZtvvqmMRqNaunRprR7vxdq0aZNq2rSp6tSpk2t0HaWkjtLS0lRUVJQaOXKk2rhxozpw4IBatmyZ2r9/v6vMjBkzlL+/v/r666/Vzp071U033aSaNWum8vLyXGUGDhyoOnfurDZs2KB+/vln1bJlS9eoQUoplZGRoUJDQ9WIESPUnj171IIFC5SXl5d69913a/V4q2L69OkqODhYfffddyopKUktWrRI+fj4qNdff91V5nKsoyVLlqhJkyapr776SgFq8eLFbutrq07WrVunjEajeumll9TevXvVM888ozw8PFyjddUESbhKqe7du6u4uDjXvN1uVxERESo+Pl7HqGpHamqqAtTatWuVUkqlp6crDw8PtWjRIleZhIQEBaj169crpZx/MAaDQaWkpLjKzJ49W/n5+SmbzaaUUuqpp55S7du3d9vXnXfeqQYMGFDTh1RtsrKyVHR0tFqxYoXbcHZSR0pNmDChzPB4pTkcDhUWFqZefvll17L09HRlsVjUggULlFJK7d27VwFq8+bNrjI//PCD0jTNNVTgf/7zHxUYGOiqs5J9lx6OsK4aPHiwuv/++92W3XrrrWrEiBFKKakjpVSZhFubdTJs2DA1ePBgt3h69Oih/vnPf1brMZZ22Z9SLigoYOvWrfTr18+1zGAw0K9fP9avX69jZLUjIyMDODus3NatWyksLHSrjzZt2hAZGemqj/Xr19OxY0fXUHEAAwYMIDMzk99++81VpvQ2SsrUpzqNi4tj8ODBZY5D6gi++eYbunbtyh133EHDhg2JiYnhvffec61PSkoiJSXF7fj8/f3p0aOHWx0FBATQtWtXV5l+/fphMBjYuHGjq8w111yD2Wx2lRkwYACJiYmcOXOmpg/zovTq1YtVq1axb98+wDmgxC+//MKgQYMAqaPy1Gad6PH3d9kn3FOnTmG3292+GME5xui5D6a/1DgcDsaOHUvv3r1d46+mpKRgNpsJCAhwK1u6PlJSUsqtr5J1FyqTmZlJXl5eTRxOtVq4cCHbtm0jPj6+zDqpIzhw4ACzZ88mOjqaZcuW8eijjzJ69Gg++ugj4OwxXujvKiUlhYYNG7qtN5lMBAUFVaoe66qnn36au+66izZt2uDh4UFMTAxjx45lxIgRgNRReWqzTs5XpibrTAYvuIzFxcWxZ8+eah0W7lJw+PBhxowZw4oVK/D09NQ7nDrJ4XDQtWtXXnjhBcA5xN+ePXt45513iI2N1Tm6uuHzzz9n/vz5fPrpp7Rv354dO3YwduxYIiIipI4uU5d9CzckJASj0Vimh+mJEycICwvTKaqaN2rUKL777jtWr15N48aNXcvDwsIoKCggPT3drXzp+ggLCyu3vkrWXaiMn5+fa4zWumrr1q2kpqZyxRVXYDKZMJlMrF27ljfeeAOTyURoaOhlX0fh4eG0a9fObVnbtm1JTk4Gzh7jhf6uwsLCSE1NdVtfVFREWlpapeqxrnryySddrdyOHTtyzz338MQTT7jOmkgdlVWbdXK+MjVZZ5d9wjWbzVx55ZWsWrXKtczhcLBq1Sp69uypY2Q1QynFqFGjWLx4MT/++CPNmjVzW3/llVfi4eHhVh+JiYkkJye76qNnz57s3r3b7UO/YsUK/Pz8XF/CPXv2dNtGSZn6UKd9+/Zl9+7d7NixwzV17dqVESNGuF5f7nXUu3fvMreT7du3j6ioKACaNWtGWFiY2/FlZmayceNGtzpKT09n69atrjI//vgjDoeDHj16uMr89NNPFBYWusqsWLGC1q1bExgYWGPHVx1yc3PLDCdoNBpxOByA1FF5arNOdPn7q7HuWPXIwoULlcViUfPmzVN79+5VDz/8sAoICHDrYXqpePTRR5W/v79as2aNOn78uGvKzc11lXnkkUdUZGSk+vHHH9WWLVtUz549Vc+ePV3rS2556d+/v9qxY4daunSpatCgQbm3vDz55JMqISFBvf322/XmlpfylO6lrJTU0aZNm5TJZFLTp09Xf/zxh5o/f76yWq3qk08+cZWZMWOGCggIUP/73//Url271M0331zu7R0xMTFq48aN6pdfflHR0dFut3ekp6er0NBQdc8996g9e/aohQsXKqvVWmdveSktNjZWNWrUyHVb0FdffaVCQkLUU0895SpzOdZRVlaW2r59u9q+fbsC1Kuvvqq2b9+uDh06pJSqvTpZt26dMplMaubMmSohIUFNnTpVbguqLW+++aaKjIxUZrNZde/eXW3YsEHvkGoEUO40d+5cV5m8vDz12GOPqcDAQGW1WtUtt9yijh8/7radgwcPqkGDBikvLy8VEhKi/vWvf6nCwkK3MqtXr1ZdunRRZrNZNW/e3G0f9c25CVfqSKlvv/1WdejQQVksFtWmTRs1Z84ct/UOh0NNnjxZhYaGKovFovr27asSExPdypw+fVoNHz5c+fj4KD8/P3XfffeprKwstzI7d+5UV111lbJYLKpRo0ZqxowZNX5s1SEzM1ONGTNGRUZGKk9PT9W8eXM1adIkt1tVLsc6Wr16dbnfQbGxsUqp2q2Tzz//XLVq1UqZzWbVvn179f3339fYcSullIwWJIQQQtSCy/4arhBCCFEbJOEKIYQQtUASrhBCCFELJOEKIYQQtUASrhBCCFELJOEKIYQQtUASbjGbzca0adOw2Wx6h1JnSR1VjNTTX5M6+mtSR3+tvtWR3IdbLDMzE39/fzIyMvDz89M7nDpJ6qhipJ7+mtTRX5M6+mv1rY6khSuEEELUAkm4QgghRC2o1+PhFhUVsX37dkJDQ8uMylFZWVlZABw9epTMzMzqCO+SI3VUMVJPf03q6K9JHf21ulJHDoeDEydOEBMTg8l0/rRar6/hbt68me7du+sdhhBCCMGmTZvo1q3bedfX6xZuaGgo4DzI8PBwnaMRQghxOTp+/Djdu3d35aTzqdcJt+Q0cnh4OI0bN9Y5GiGEEJezv7q0KZ2mhBBCiFogCVcIIYSoBZJwhRBCiFpQr6/hCiHEhdjtdgoLC/UOQ9RzHh4eGI3Gi96OJFwhxCVHKUVKSgrp6el6hyIuEQEBAYSFhaFpWpW3IQm3xMlESD8MEV3AO0TvaIQQF6Ek2TZs2BCr1XpRX5Li8qaUIjc3l9TUVICLugVVEm6JxY/AsW1w1wJoc4Pe0Qghqshut7uSbXBwsN7hiEuAl5cXAKmpqTRs2LDKp5el01QJ3zDnv1nH9Y1DCHFRSq7ZWq1WnSMRl5KSz9PF9AmQhFuiJOFmn9A3DiFEtZDTyKI6VcfnSRJuCR9p4QohhKg5knBLuE4pSwtXCHFpaNq0KbNmzapw+TVr1qBpWo337p43bx4BAQE1uo+6SBJuCbmGK4TQiaZpF5ymTZtWpe1u3ryZhx9+uMLle/XqxfHjx/H396/S/sSFSS/lEj7FozzINVwhRC07fvzsD/3PPvuMKVOmkJiY6Frm4+Pjeq2Uwm63X3Dc1RINGjSoVBxms5mwsLBKvUdUnLRwS/gW31uVnQr2In1jEUJcVsLCwlyTv78/mqa55n///Xd8fX354YcfuPLKK7FYLPzyyy/8+eef3HzzzYSGhuLj40O3bt1YuXKl23bPPaWsaRrvv/8+t9xyC1arlejoaL755hvX+nNPKZec+l22bBlt27bFx8eHgQMHuv1AKCoqYvTo0QQEBBAcHMyECROIjY1l6NChlaqD2bNn06JFC8xmM61bt+a///2va51SimnTphEZGYnFYiEiIoLRo0e71v/nP/8hOjoaT09PQkNDuf322yu179oiCbeEdwhoBkBBzkm9oxFCVCOlFLkFRbU+KaWq7RiefvppZsyYQUJCAp06dSI7O5sbbriBVatWsX37dgYOHMiQIUNITk6+4HaeffZZhg0bxq5du7jhhhsYMWIEaWlp5y2fm5vLzJkz+e9//8tPP/1EcnIy48ePd61/8cUXmT9/PnPnzmXdunVkZmby9ddfV+rYFi9ezJgxY/jXv/7Fnj17+Oc//8l9993H6tWrAfjyyy957bXXePfdd/njjz/4+uuv6dixIwBbtmxh9OjRPPfccyQmJrJ06VKuueaaSu2/tsgp5RIGo/O0ctZx5+QnA9oLcanIK7TTbsqyWt/v3ucGYDVXz9fsc889x/XXX++aDwoKonPnzq75559/nsWLF/PNN98watSo825n5MiRDB8+HIAXXniBN954g02bNjFw4MByyxcWFvLOO+/QokULAEaNGsVzzz3nWv/mm28yceJEbrnlFgDeeustlixZUqljmzlzJiNHjuSxxx4DYNy4cWzYsIGZM2dy3XXXkZycTFhYGP369cPDw4PIyEi6d+8OQHJyMt7e3tx44434+voSFRVFTExMpfZfW6SFW5pcxxVC1FFdu3Z1m8/Ozmb8+PG0bduWgIAAfHx8SEhI+MsWbqdOnVyvvb298fPzcz22sDxWq9WVbMH5aMOS8hkZGZw4ccKV/ACMRiNXXnllpY4tISGB3r17uy3r3bs3CQkJANxxxx3k5eXRvHlzHnroIRYvXkxRkfPS3/XXX09UVBTNmzfnnnvuYf78+eTm5lZq/7VFWril+YbD8R3SU1mIS4yXh5G9zw3QZb/Vxdvb221+/PjxrFixgpkzZ9KyZUu8vLy4/fbbKSgouOB2PDw83OY1TcPhcFSqfHWeKq+IJk2akJiYyMqVK1mxYgWPPfYYL7/8MmvXrsXX15dt27axZs0ali9fzpQpU5g2bRqbN2+uc7ceSQu3NN/iFm7eGX3jEEJUK03TsJpNtT7V5NOu1q1bx8iRI7nlllvo2LEjYWFhHDx4sMb2Vx5/f39CQ0PZvHmza5ndbmfbtm2V2k7btm1Zt26d27J169bRrl0717yXlxdDhgzhjTfeYM2aNaxfv57du3cDYDKZ6NevHy+99BK7du3i4MGD/PjjjxdxZDVDWrilXf88DHoJTBa9IxFCiAuKjo7mq6++YsiQIWiaxuTJky/YUq0pjz/+OPHx8bRs2ZI2bdrw5ptvcubMmUr92HjyyScZNmwYMTEx9OvXj2+//ZavvvrK1et63rx52O12evTogdVq5ZNPPsHLy4uoqCi+++47Dhw4wDXXXENgYCBLlizB4XDQunXrmjrkKpOEW5qnn94RCCFEhbz66qvcf//99OrVi5CQECZMmEBmZmatxzFhwgRSUlK49957MRqNPPzwwwwYMKBSI+oMHTqU119/nZkzZzJmzBiaNWvG3Llz6dOnD+Aci3bGjBmMGzcOu91Ox44d+fbbbwkODiYgIICvvvqKadOmkZ+fT3R0NAsWLKB9+/Y1dMRVp6naPhlfyuzZs5k9e7brNEj79u2ZMmUKgwYNqtD7jxw5QpMmTTh8+DCNGzeuwUiFEPVFfn4+SUlJNGvWDE9PT73Duew4HA7atm3LsGHDeP755/UOp9pc6HNV0Vykawu3cePGzJgxg+joaJRSfPTRR9x8881s375dn18nmcdh5TQoyodhH9X+/oUQop45dOgQy5cv59prr8Vms/HWW2+RlJTEP/7xD71Dq3N0TbhDhgxxm58+fTqzZ89mw4YN+iRcTYNdC50PwHDYnffmCiGEOC+DwcC8efMYP348Sik6dOjAypUradu2rd6h1Tl15hqu3W5n0aJF5OTk0LNnz3LL2Gw2bDabaz4rK6t6g/BuAP2mOW8PUg5AEq4QQlxIkyZNyvQwFuXTPeHu3r2bnj17kp+fj4+PD4sXL3brCl5afHw8zz77bM0FYzDCVU/U3PaFEEJctnS/D7d169bs2LGDjRs38uijjxIbG8vevXvLLTtx4kQyMjJc0/nKCSGEEHWN7i1cs9lMy5YtAbjyyivZvHkzr7/+Ou+++26ZshaLBYvl7D2yNdIFPi0JTu+HgCho0Kr6ty+EEOKypHsL91wOh8PtOm2t++U1mH87/PaVfjEIIYS45Ojawp04cSKDBg0iMjKSrKwsPv30U9asWcOyZbU/qodLybi48jxlIYQQ1UjXhJuamsq9997L8ePH8ff3p1OnTixbtsxtCKpaV/I85SwZMUgIIUT10fWU8gcffMDBgwex2WykpqaycuVKfZMtnG3hZqfoG4cQQlRSnz59GDt2rGu+adOmzJo164Lv0TSt0gPG1+R2LmTatGl06dKlRvdRk+rcNVzd+YY5/82ShCuEqB1Dhgw57wDwP//8M5qmsWvXrkpvd/PmzTz88MMXG56b8yW948ePV/ixvJcrSbjn8ilOuNmpzqdNCSFEDXvggQdYsWIFR44cKbNu7ty5dO3a1W3g+Ipq0KABVqu1OkL8S2FhYW53kYiyJOGey7uB89GOyg45p/SORghxGbjxxhtp0KAB8+bNc1uenZ3NokWLeOCBBzh9+jTDhw+nUaNGWK1WOnbsyIIFCy643XNPKf/xxx9cc801eHp60q5dO1asWFHmPRMmTKBVq1ZYrVaaN2/O5MmTKSwsBJzD5D377LPs3LkTTdPQNM0V87mnlHfv3s3f//53vLy8CA4O5uGHHyY7O9u1fuTIkQwdOpSZM2cSHh5OcHAwcXFxrn1VhMPh4LnnnqNx48ZYLBa6dOnC0qVLXesLCgoYNWoU4eHheHp6EhUVRXx8PABKKaZNm0ZkZCQWi4WIiAhGjx5d4X1Xhe734dY5RpMz6WafcF7HLelEJYSo/wpyKv8eo8X5vQBgLwK7zfmj3MPrwts1e1d4FyaTiXvvvZd58+YxadIk11iyixYtwm63M3z4cLKzs7nyyiuZMGECfn5+fP/999xzzz20aNGC7t27/+U+HA4Ht956K6GhoWzcuJGMjAy3670lfH19mTdvHhEREezevZuHHnoIX19fnnrqKe6880727NnD0qVLXWPV+vv7l9lGTk4OAwYMoGfPnmzevJnU1FQefPBBRo0a5fajYvXq1YSHh7N69Wr279/PnXfeSZcuXXjooYcqVG+vv/46r7zyCu+++y4xMTF8+OGH3HTTTfz2229ER0fzxhtv8M033/D5558TGRnJ4cOHOXz4MABffvklr732GgsXLqR9+/akpKSwc+fOCu23qiThlsc3zJlws1IgvLPe0QghqssLEZV/zx3zoP0tzte/fwuLRkLUVXDf92fLzOoIuafd3zcto1K7uf/++3n55ZdZu3ataxzYuXPnctttt+Hv74+/vz/jx493lX/88cdZtmwZn3/+eYUS7sqVK/n9999ZtmwZERHOenjhhRfKXHd95plnXK+bNm3K+PHjWbhwIU899RReXl74+PhgMpkICws7774+/fRT8vPz+fjjj/H2dv7weOuttxgyZAgvvvgioaHOhkxgYCBvvfUWRqORNm3aMHjwYFatWlXhhDtz5kwmTJjAXXfdBcCLL77I6tWrmTVrFm+//TbJyclER0dz1VVXoWkaUVFRrvcmJycTFhZGv3798PDwIDIyskL1eDHklHJ5fKTjlBCidrVp04ZevXrx4YcfArB//35+/vlnHnjgAcA5wMvzzz9Px44dCQoKwsfHh2XLlpGcnFyh7SckJNCkSRNXsgXKHSjms88+o3fv3oSFheHj48MzzzxT4X2U3lfnzp1dyRagd+/eOBwOEhMTXcvat2/vNlB9eHg4qampFdpHZmYmx44do3fv3m7Le/fuTUJCAuA8bb1jxw5at27N6NGjWb58uavcHXfcQV5eHs2bN+ehhx5i8eLFFBUVVeo4K0tauOWRnspCXJr+fazy7zGW6gjUZohzG9o5bZWxuy8urmIPPPAAjz/+OG+//TZz586lRYsWXHvttQC8/PLLvP7668yaNYuOHTvi7e3N2LFjKSgoqJZ9A6xfv54RI0bw7LPPMmDAAPz9/Vm4cCGvvPJKte2jNA8PD7d5TdNwOBzVtv0rrriCpKQkfvjhB1auXMmwYcPo168fX3zxBU2aNCExMZGVK1eyYsUKHnvsMdcZhnPjqi7Swi3mcCiOpedRaHecTbhyL64Qlxazd+UnY6l2idHkXFb6+u35tlsFw4YNw2Aw8Omnn/Lxxx9z//33u67nrlu3jptvvpm7776bzp0707x5c/bt21fhbbdt25bDhw9z/PjZp+ht2LDBrcyvv/5KVFQUkyZNomvXrkRHR3Po0CH3QzWbsdsvfAdH27Zt2blzJzk5Z69tr1u3DoPBQOvWrSsc84X4+fkRERFRZmjAdevWuY045+fnx5133sl7773HZ599xpdffklaWhoAXl5eDBkyhDfeeIM1a9awfv16du+unh9P5ZEWbrEe8as4mWVj+RPX0Kok4Valg4UQQlSRj48Pd955JxMnTiQzM5ORI0e61kVHR/PFF1/w66+/EhgYyKuvvsqJEyfOO5zpufr160erVq2IjY3l5ZdfJjMzk0mTJrmViY6OJjk5mYULF9KtWze+//57Fi9e7FamadOmJCUlsWPHDho3boyvr2+Z24FGjBjB1KlTiY2NZdq0aZw8eZLHH3+ce+65x3X9tjo8+eSTTJ06lRYtWtClSxfmzp3Ljh07mD9/PgCvvvoq4eHhxMTEYDAYWLRoEWFhYQQEBDBv3jzsdjs9evTAarXyySef4OXl5Xadt7pJC7dYqJ/zA3PodC50/gc8cxJunaNzVEKIy80DDzzAmTNnGDBggNv11meeeYYrrriCAQMG0KdPH8LCwhg6dGiFt2swGFi8eDF5eXl0796dBx98kOnTp7uVuemmm3jiiScYNWoUXbp04ddff2Xy5MluZW677TYGDhzIddddR4MGDcq9NclqtbJs2TLS0tLo1q0bt99+O3379uWtt96qXGX8hdGjRzNu3Dj+9a9/0bFjR5YuXco333xDdHQ04Oxx/dJLL9G1a1e6devGwYMHWbJkCQaDgYCAAN577z169+5Np06dWLlyJd9++y3BwcHVGmNpmlJK1djWa9iRI0do0qQJhw8fpnHjxhe1rbhPt/H9ruM8M7gtD17dvJoiFELUtvz8fJKSkmjWrBmenp56hyMuERf6XFU0F0kLt1hUkPNpLIdO5+ociRBCiEuRJNxiUcHFCTctF5SCxY/AR0PkaVNCCCGqhXSaKhYZ5OxVmHw6BzQN9q+CnFTIPAreITpHJ4QQor6ThFuspIV75EweRXYHpn7TwOgBfhd3bVgIIYQASbguYX6emE0GCoocHM/Ip0nMCL1DEkIIcQmRa7jFDAaNyOKOUwdPy/23QtR31fnEIiGq4/MkLdxSooKs7E/N5tDpXK4OPQbHd4GnP0SVfd6oEKJuMpvNGAwGjh07RoMGDTCbza6nNQlRWUopCgoKOHnyJAaDAbPZXOVtScItJbL4Om5yWi4k/grfj4PWgyXhClGPGAwGmjVrxvHjxzl2rArPThaiHFarlcjISAyGqp8YloRbytl7cXOguTxPWYj6ymw2ExkZSVFR0V8+91eIv2I0GjGZTBd9pkQSbilRwc5bgw6dzpURg4So5zRNw8PDo8ZGfhGisqTTVCmlTykrn+IHbGefAOl8IYQQ4iJJwi2lcaAXBg1yC+ycVP6ABo4iyD2td2hCCCHqOUm4pVhMRsL9neNcJqcXnn3ClFzHFUIIcZEk4Z7D9UxluY4rhBCiGknCPYfbIAY+knCFEEJUD0m453AbxEBauEIIIaqJJNxzuLVwfeVeXCGEENVDEu455BquEEKImiAJ9xwlD79Iyykgz9LAuVASrhBCiIskCfccPhYTwd7Oh1MfswfgvBe3UNeYhBBC1H/yaMdyRAZbOZ1TQKKxBS0mn3QORC+EEEJcBGnhlsM1iEGaTZKtEEKIaiEJtxyRxddxk9NkIHohhBDVQxJuOZoW91Q+eCoXVj4L826EQ+t1jkoIIUR9Jgm3HFGlB6JP2QUHf4a0P3WOSgghRH0mnabKUfK0qWMZeRTc8k/Mne6CJt11jkoIIUR9Jgm3HCE+ZqxmI7kFdg4H9aJFAx+9QxJCCFHPySnlcmiaRmRxT+Xk07k6RyOEEOJSoGvCjY+Pp1u3bvj6+tKwYUOGDh1KYmKiniG5NC3uqXws5Tgk/gAJ3+kckRBCiPpM14S7du1a4uLi2LBhAytWrKCwsJD+/fuTk6P/7TglHadyjyfCgrtg6USdIxJCCFGf6XoNd+nSpW7z8+bNo2HDhmzdupVrrrlGp6icIosTbmKO81+yU0Ap0DQdoxJCCFFf1alOUxkZGQAEBQWVu95ms2Gz2VzzWVlZNRZLVHFP5d3pFucCewHknQFr+bEJIYQQF1JnOk05HA7Gjh1L79696dChQ7ll4uPj8ff3d03t2rWrsXhKTiknpRehrMHOhVnHa2x/QgghLm11JuHGxcWxZ88eFi5ceN4yEydOJCMjwzXt3bu3xuIJ9/fEZNAoKHJQZG3oXCgJVwghRBXViVPKo0aN4rvvvuOnn36icePG5y1nsViwWCyu+czMzBqLyWQ00CTIStKpHLLNIQQCZJ2osf0JIYS4tOnawlVKMWrUKBYvXsyPP/5Is2bN9AynjJJ7cc9oxddtpYUrhBCiiqqUcA8fPsyRI0dc85s2bWLs2LHMmTOnUtuJi4vjk08+4dNPP8XX15eUlBRSUlLIy8urSljVruQ6booKdC7IlhauEEKIqqlSwv3HP/7B6tWrAUhJSeH6669n06ZNTJo0ieeee67C25k9ezYZGRn06dOH8PBw1/TZZ59VJaxqV9LCPVLo51wgLVwhhBBVVKWEu2fPHrp3dz7M//PPP6dDhw78+uuvzJ8/n3nz5lV4O0qpcqeRI0dWJaxqF1X8tKk/8oqfpSzXcIUQQlRRlRJuYWGhq/PSypUruemmmwBo06YNx49fOq3AklPKCdnFD7/IStExGiGEEPVZlRJu+/bteeedd/j5559ZsWIFAwcOBODYsWMEBwdXa4B6KjmlnJTv61xQ8rQpIYQQopKqlHBffPFF3n33Xfr06cPw4cPp3LkzAN98843rVPOlwNPDSJifJycJQGlG8AoCW83diiSEEOLSVaX7cPv06cOpU6fIzMwkMDDQtfzhhx/GarVWW3B1QWSwlZTMfL65aQc3x0TqHY4QQoh6qkot3Ly8PGw2myvZHjp0iFmzZpGYmEjDhg2rNUC9RRWfVj6UZvuLkkIIIcT5VSnh3nzzzXz88ccApKen06NHD1555RWGDh3K7NmzqzVAvZV0nDokA9ELIYS4CFVKuNu2bePqq68G4IsvviA0NJRDhw7x8ccf88Ybb1RrgHqLLL41qPXhz2DuDbB1nr4BCSGEqJeqdA03NzcXX19nz93ly5dz6623YjAY+Nvf/sahQ4eqNUC9NS1u4ZpzjkHWOggtfyQjIYQQ4kKq1MJt2bIlX3/9NYcPH2bZsmX0798fgNTUVPz8/Ko1QL2VjIu7KK8btpvfg6736RyREEKI+qhKCXfKlCmMHz+epk2b0r17d3r27Ak4W7sxMTHVGqDe/K0e+Ht58JtqSlL4QGjYVu+QhBBC1ENVSri33347ycnJbNmyhWXLlrmW9+3bl9dee63agqsrpOOUEEKIi1Xl8XDDwsIICwtzjRrUuHHjS+qhF6VFBllJOHIa7ffvIRfoej9omt5hCSGEqEeq1MJ1OBw899xz+Pv7ExUVRVRUFAEBATz//PM4HI7qjlF3UcFWDDjov3scfD8O8s7oHZIQQoh6pkot3EmTJvHBBx8wY8YMevfuDcAvv/zCtGnTyM/PZ/r06dUapN6igr2xYSbL4IuvI8s5iIE1SO+whBBC1CNVSrgfffQR77//vmuUIIBOnTrRqFEjHnvssUsv4RY/beqkCsSXLOcgBqHtdI5KCCFEfVKlU8ppaWm0adOmzPI2bdqQlpZ20UHVNSXj4h61+zsXyLi4QgghKqlKCbdz58689dZbZZa/9dZbdOrU6aKDqmsa+lqwmAykqgDngqxLZ8xfIYQQtaNKp5RfeuklBg8ezMqVK1334K5fv57Dhw+zZMmSag2wLjAYNCKDrJxIKx4ZKVtauEIIISqnSi3ca6+9ln379nHLLbeQnp5Oeno6t956K7/99hv//e9/qzvGOiEq2CotXCGEEFVW5ftwIyIiynSO2rlzJx988AFz5sy56MDqmqhgb46p4hauXMMVQghRSVVq4V6OpIUrhBDiYkjCraDIICupBDhnsk+AUrrGI4QQon6RhFtBUcHepJacUi7Kh/x0XeMRQghRv1TqGu6tt956wfXp6ekXE0ud1ijAi0LNTIay4q/lOq/jegXqHZYQQoh6olIJ19/f/y/X33vvvRcVUF1lNhmICPDiWE4wXlZfzAXZeockhBCiHqlUwp07d25NxVEvNA32ZtCZGbx8U2fuaNxE73CEEELUI3INtxIig62ARnKajIsrhBCiciThVkLJIAYyEL0QQojKkoRbCVHBVvoYdvDPA3Hww9N6hyOEEKIeqfKTpi5HkUHeeJNP+8Lf4Lif3uEIIYSoR6SFWwlRwVa2OFoRVzCa7D7P6R2OEEKIekQSbiV4W0zYfcL53vE3ksyt9Q5HCCFEPSIJt5Kigos7TqXl6ByJEEKI+kQSbiVFBTk7TvnumgfZqXqHI4QQop6QTlOVFBls5VHTfKL3H4XUXuDTUO+QhBBC1APSwq0k92H6ZFxcIYQQFSMJt5Kigr05QclA9DIurhBCiIqRhFtJUUFnW7hFmZJwhRBCVIwk3EoK8jaTbgwGIO/0UZ2jEUIIUV/omnB/+uknhgwZQkREBJqm8fXXX+sZToVomgY+oYC0cIUQQlScrgk3JyeHzp078/bbb+sZRqV5BEQAYMyWTlNCCCEqRtfbggYNGsSgQYP0DKFKvEMawVHwtJ0EpUDT9A5JCCFEHVev7sO12WzYbDbXfFZWli5xBIVGAmB25IMtCzxlIAMhhBAXVq86TcXHx+Pv7++a2rVrp0scjRuGkKm8nDNZKbrEIIQQon6pVwl34sSJZGRkuKa9e/fqEkdksJVU5bwX1y4dp4QQQlRAvUq4FosFPz8/1+Tr66tLHOH+XpwkAID01GRdYhBCCFG/1KuEW1cYDRoZ5jCOqSBOZ+XrHY4QQoh6QNdOU9nZ2ezfv981n5SUxI4dOwgKCiIyMlLHyP7awkYTeSTxJC/4d6SV3sEIIYSo83RNuFu2bOG6665zzY8bNw6A2NhY5s2bp1NUFdM02Bs4KePiCiGEqBBdE26fPn1QSukZQpVFBjkHok8+natzJEIIIeqDenUfbl3S1uMYi8zT8DjoCfysdzhCCCHqOEm4VRQR5EuUYR+5hRaUUs5nLAshhBDnIQm3ikIbtySucDQpjkDezbYR4uupd0hCCCHqMEm4VeTpZWWbTx+OZ+RzKC1PEq4QQogLkvtwL0JUcHHHKempLIQQ4i9IC/ciXGvZTyvjZnIOKYhprHc4Qggh6jBJuBehb853tPJYyv+OeQM36h2OEEKIOkxOKV8Eo384AFqWDGAghBDiwiThXgRrYCMAzPkndY5ECCFEXScJ9yL4hzYBIMB+mmxbkc7RCCGEqMsk4V4Ea5CzhduAdHnEoxBCiAuShHsxfJ3XcEO1M3JrkBBCiAuShHsxfEMB8NHyOXZCruMKIYQ4P0m4F8PiS4HB+fCLM6mHdQ5GCCFEXSYJ9yLZvBoAkJt2VOdIhBBC1GWScC+S8gkDwJ4u9+IKIYQ4P0m4F8kc4Ow45ZF3goIih87RCCGEqKsk4V4kS+DZW4OOpufpHI0QQoi6ShLuRdICmpCqhZCHmUOn5dYgIYQQ5ZOEe7H+9iiTmn3Ga0V3sP7P03pHI4QQoo6ShFsNbuocAcCcnw/w8x9yP64QQoiyJOFWgyGdI7irWxOUgtELtsu1XCGEEGVIwr1YRTb4oD8vJI+ga4QHZ3ILeWz+NmxFdr0jE0IIUYdIwr1YJguk7MGQkcybN0bg52li5+F0pn+foHdkQggh6hBJuNVh2Edw/3LCm7Rg1l1dAPh4/SH+t0OePiWEEMJJEm51iL4eInuAhxd/bxPK439vCcDTX+5m34ksnYMTQghRF0jCrU4Hf4HMY4zt14qrWoaQV2jnkf9uJSu/UO/IhBBC6EwSbnU5tR8WDId3r8V4eAOv39WFcH9PDpzKYcKXu1BK6R2hEEIIHUnCrS4GI/g3gZxU+OhGgvd+zNv/iMHDqLFkdwof/JKkd4RCCCF0JAm3ugQ1gwdXQPtbwFEES8ZzxfbJTBnYAoD4H35nU1KazkEKIYTQiyTc6mT2htvnwvXPgWaAHZ9wd8Ij3NvOhN2hGPXpNlKz8vWOUgghhA4k4VY3TYPeY+DuL8ErEO3YNp5NiePWoIOkZtkYvWA7RXYZxk8IIS43knBrSou/w8NrILQjWu5JXsmfwkPmFWw4cJqZy/fpHZ0QQohaJgm3JgU2hQeWQ4fb0RxFTDLMZabHu8xdm8Cy31L0jk4IIUQtkoRb08xWuO196D8dNAO3G39ikflZJn++kYOnLm78XLtDkXA8k+TTuXLbkRBC1HEmvQO4LGga9BoFYR1Qi+7jOG1JPWPikU+2svix3niZjRXaTKHdwe6jGWw8kMampNNsOXiGLFsRAKF+Fro1DaJHsyC6NQuiVUNfDAatJo9KiLIcdjiyBfYtdU6n/oDGXZ2XWFr8HSJinLfQCXEZ0lQ9bhodOXKEJk2acPjwYRo3bqx3OBWTcYQTDj8Gv72RU9kF3NUlhPhh3dAMZU825Bfa2Xk4nY1JaWxKSmProTPkFbqPQuRjMWErslNod/9v9PfyoFvTQLo3C6Jb0yA6NPLHwygnNEQN++IB2PPF+dd7+kOza88m4MCo2otNiBpS0VwkLdza5t+YUODN4Vdw7/vruPG3J0j6oAXN7/+QXLvGtkPpbDpwig0Hz7DjcDoFRe49mgOsHnRvGkSP5sH0aBZE23A/Cooc7DiczqakNDYfTGNb8hky8gpZmZDKyoRUALw8jFwRFUC3pkF0bxZETJPACreshSjXlg8h4Tu48bWzibPpVfDHCojuB60GQmgHOLwB/lwNSWshPwMSvnFOAEEt4Lp/Q8fb9TsOIWqJJFyd9GwRzMs9bPxtewLJR9K49d1N7DqSQZFDscQ8kYe0VLKMXuSZvMHTH4tPIH4BQfj6B6N5+YPyg2N+kBaAV0AkPRu3o2eLaMB56vm3Y5lsTkpj00FnEk7PLWTd/tOs238aAA+jRsdG/sREBtKyoY9zauBDoLdZz2oRdZXDAacSoWHbs8t2fwGH1jlPHff4p3NZ5+EQczcYPc6WC20HXe8HexEc2w4HVsOfP8LhTZD2JxhKfQ2d+M2ZxKOvh0ZX1M6xCVFL6kTCffvtt3n55ZdJSUmhc+fOvPnmm3Tv3l3vsGrczTffzswTGTQ5/C3bktMBiPD3JMxhw7cwD1/ygDTIPwz5wKkLbKz3GOcDNwCPohy62LbQ5YrOPHRNcxwOxf6T2WxMSnMm4aQ0UjLz2Zac7tpviWBvMy1KJeCSZBzu74lWlA9nDjof8BEQ6XxDQQ5smgMeVvDwKvVvqdemc+c9oZxT6JXmcIDdBkX5UFT8r3dDZ0c1gIyjkJoA1kBodOXZ9/2xEnwaQki0M54appTiZLaNU1kFNAr0wt/L46/fpCelIOeUMxme/hOSf4V9yyH3FIz/A7xDnOW6PQitBkB0/7Pv9fA8/3aNJmjSzTld+xTkZzoH/IjqdbZMwnew5gVI/Q2Gfexclp0Kn9/r/PyYreDhXfyvFcw+pV57u/8bEXP2s5D6O5xMgICos4m8IMfZSrcXgL2w1L+FzuvMFt9yJj9o0Np5aryiHHawZTpb927TOcu6PeD8TALkpkHWcQiOBpP8CL5U6J5wP/vsM8aNG8c777xDjx49mDVrFgMGDCAxMZGGDRvqHV6N0jSNR0eO5KNf+zDTz5MezYJoHOiFlvsr5Kc7/whtmWf/MEten/vHe+oPCOt0dsNHt8IntzlvSxqzE4NBo1WoL61yd3BPq0aogE4cSbexKSmN345lsv9kNn+mZnM0PY/CnDPkHvydjEOpnNJSeNXen2yseJuNzLB+wpD8b9nWJJaTf/s3zUK8CSpKJWTltMof/AMroEnxj6r1b8OP/wedhsGQ153LCnLg5ejzv99R/AV5rru/gpZ9na/3r4RvR0PrG2D4AucypWDRSCjIAjTnD4cGrSGkFTRoc/a1V0ClDqfQ7uDomTyS03I5lJZL8ukcDp3OJTnNOeUWnL32HuJjoUUDb+cPmwY+tGjoQ4sG3kT4e9VuR7fcNEg74EyqJcn19H7nMltm2fIWP+cPmGZXO+c73Hpx+/f0gzY3uC8L7wRtb4I2N55dlp8Byesrv/1H1ztb1wB7v4Y18XDlfWcTbpENlj9T+e3es9h5/Rlg28ew9N/Q7iYY+h8Kihzk52bg+8kNaCV/q+XVZXma9zmbcPctha8fhWbXQOy3Z8vsX+k8DR8QVbUfrUU2ZyI3+5z94ZSa4Pz7Uw7wDHB+9kv/6+lfdtmFfliV5rBDYS4U5EJBdvmvHYVgDXb+CPYJBZ8w54+zc+QX2jmekc+x9DyOpudx9EweqVk2AqweNArwolGAFxEBXkQEeOLrWTd/1OqecF999VUeeugh7rvvPgDeeecdvv/+ez788EOefvppnaOreT4WE3HXtXRf6B3snCrDUepab2EeBLd0Xj8roRQsHAG2DDSLH01CO9AkrCO3eftDQRLYk1DaAbQ89+c9J/l3Z0W6NzkFdrbaA7nWZGVz0ini/9gKQAgZPGW6Fm9DAb7GQnwMhXgbCvCiAE/NhqeyYVY2TI58TI6zCfLAmSJytAxsRXZCUtNpWpjLkVMZbN5+BFuhA3t+NiMKK37blAMDdqOZDYkpZOQew2o2Ep7lSVRwe/LN4eSk5eLpYcSqcvAKbY92KhEt7wykH3JOfyx3257duyH2oGgKg6IpDGpFbrOB2H3CycovIjnNmUydiTWXQ2k5HEvPx+44t/+hc15DYdLA39PEmbwiTmXncyrbxsZznq3t6WGgecjZBNyigQ8tGvjQvIE3nh7nXG932M+2yBxFpV4Xt9B8w50JDZwtxKSfnF+c0dc7l+VnwkvNLlCjGvg3hqDmzs9Rq/4Q2avmW1utBzmn0nxC4Y6Pir+gc85+URfmFP97zvKSL3Ot1I+XwKYQdRUEtzi7zOQJne5yfrkbzcWTBxg8nHVakI0jL4PCvAzseZk48rPQbFms2pfHvv2JnM4p4Ioje7ijIIvle47yrx3LyMovQsPBn5YENM3981Bg8KTA5Euhhy/K4gee/hi8AvDwDsDiE4SpdAcyW3Zxa7pNqWVZzh/SgPLwRjVog6NBG+wh7SgMaUNhUCtUYR5a1jEMmUed/2YdJ/3qqSiDGYdSBC9/HJ/ELznVcxJnujyKQ4HHyXSa//5d5f6fTJ7w6K9n6/PXt+C3r5yXE7re71x24jeY3ev82ziP9f2+JIEWHEvPI+zID0SfWcuKwi58ktsDAAMOWmuHOakCSMMXM4X4k4Oflos/2fhpuYSa82niVUC4xUZDj3yCjbkkxUygQWgjIgK8CN3+BsaDa+G+JZWO72Lo2ku5oKAAq9XKF198wdChQ13LY2NjSU9P53//+59beZvNhs1mc80fPXqUdu3a1a9eyrXJ4Tj7KzjvDHw81Plr1m674Nvwbuj8og1qBr3HUBjcmkOnc9h/IpM/T+ay/2QO+1OzSU7LJTO/kIp+ggw48KQAL2xk4E1R8e89P3Lw03LIVZ6k4UwSGg4aaeWfQ9dQFCoTNjywYaYAE0UYgcq0DhXBZNJSO0ZLw1FaasWT4RjhWtlBJu6wTWGzcn75PW78ilGmr/nU3pdni2IBCCKTzZZH0TQwUPE/qdciXuGHnGiSTuVwOyuZavqYpY5ujC0c5Sqzy/IgaBomHJgoxIT9L/fxvPVpfrVcBcC1BWt5OudltplieMb3eVeZ+el3Y8fAMWMEx4wRHDU04pgxguPGCI4bwynUnMlVKec933aHwq4UDte/UORwYHeAQznXOxyKolLlHEphNGgYNA2jQcOoaRgMxa+L540GDYMB1zpTqfJaDTb4tQt8XnILikjLKeBM7oXHsraSTwMtnQLlwXHO/kjuafiNHOVJJlYylTdZWCn8i/aNt9noutxQ5FA4HA6MDhu5yozdoQh3HOdNw2u00I5h0So+xnbv/Nc5SgMAnjQt5EHjD8y2D2FWkbOjmjd53GL8BTsG/MnBX8spTmA5+JHrmvfXcvAjB2PxD4nrjR+SYwrAZDQwuvADbi/8lgXmW3nf4vybiFAp/DfbeW3fgUY+nuRpntg0T/LwJF/zJF+zUKSMWIvOEEQGDUjnatvrnCAIgH+b5vOw6XvmFA3mhaIReHkY6eSfy2fZ91X4+EsMsM0gUTkvhY0xLWaYx09MaPRfPnmwR6W3da560Uv51KlT2O12QkND3ZaHhoby+++/lykfHx/Ps88+W1vh1X+lTzl5BcI/1zpbP6f2QcpuOL7L2UoIbHY2wQY2A4uP22Y8gJYNfWnZ0LfMLhwORZatiMy8QjLyCl3/lkyZ+SWvi1zLsvIK0fKL8DBqWEwGLCZfLB4GIkxGLB6G4mVGLKYmxfPGs/+anOsL7Yq8Qjt5BUXkFdrJLbCTX/xv6dd5Bfbi9UXkF5bu8a1xGn9OK3822tu6HZMPubQ0HKOV4RgttaO00I5x2NQEq2bE08NIhMWMJbeImEY+vNytE1HB3jT1ysX4TuV/uz5xfWueaHY1RXYH6WsP4PlTIR3DrAwLbcyfJ3PYfyILP3L/cjsOpVGIkSKMFGLiSHo+CQ7nqcwQg5FNptZsyW/C3uyzpzd78DoFnO/UW37xdPEcdgWV+BFS12gaBFnNBHmbCfYxE+xjIdjbTLC3hSAfMyHezmVB3mZCfMx4ehhJz+1LWk6Bc8otIC3bRlpuIWdKlpVadyangCKHIqfATk6B/Zy9GwDnvfZ/0pAbiMeInaZaCq21w7Q2HKG1dphW2mGaaicowESKCuI4waQUT5qHBatmxKBpvK/dxbuMwGA2EGTRMGigaRaWcyMO5bw0Umh3UGRXZe6QAOcPYR/y8ddyOKrMqOLPyPtab5ZqrThgC+dApvPMVBLeXME75OCJDQ8q8oO4gY+ZiAZedA20EhHgSSPuILGgI/2aXMEdba8jwOqBdjIRPmrg7GdQ8rnSjMWnu/2xW/yxGX3JNviQoayk2a2cLPSkqdaE3GwvjqfnM7/o7yy298Jw5q//tqqTri3cY8eO0ahRI3799Vd69uzpWv7UU0+xdu1aNm7c6FZeWrjiYjgcivwiO0UOhUFzftkYNGcryjlf8gX0F18M+RnO03tmb+cPGefGIeek87WmAdrZf13LiimHs9moHM4vCZPFudyWBXnpzo5cxdfXlMNB+pHfOZqeg81hQBk8UJoHymBCaSbsRjNKM4HB6HamQZVKcOX9hZ/vj/58XwdlW6SlXmsaJuO5rVhc60pax65WsFLYHbgtc7WcS7Wky56irz5/tWWLyUBIcWINsJox1uC1daUUmflFnMkpICOv0Pk5NIDJYMBoAKPB4Gz1l2r9l553/d84ijCYPKiu0wJKOc9WFNodFBYpCkon4+LXpRM0uNdryUep9GcRRZlyBk0j3N+T8ABPLKZK3KpoL3KeufPwcv4tVvC47Q7FySwbR9PzsBXZ6dUipOL7PI960cINCQnBaDRy4sQJt+UnTpwgLCysTHmLxYLFYnHNZ2ZWsDOCEIDBoGE1V8NH3tO/bC9VgwF8Q8svX1ElPWFL0QwGAiPbERh5cZsWdZemafh7eVx873Vj9V5f1zQND6PmfGBOXewobTSBT4PKv82gEebvSZh/BTt+VSNdHz1kNpu58sorWbVqlWuZw+Fg1apVbi1eIYQQor7TvZfyuHHjiI2NpWvXrnTv3p1Zs2aRk5Pj6rUshBBCXAp0T7h33nknJ0+eZMqUKaSkpNClSxeWLl1apiOVEEIIUZ/pnnABRo0axahRo/66oBBCCFFPyfAxQgghRC2oEy3cqnIUP13p+PHjOkcihBDiclWSgxyOsvcul1avE27J7USXw0AHQggh6rYTJ04QGXn+e/jq9QD0RUVFbN++ndDQUAwXOfpMVlYW7dq1Y+/evfj6ln2iknAn9VU5Ul+VJ3VWOVJflVOd9eVwODhx4gQxMTGYTOdvx9brhFudMjMz8ff3JyMjAz8/P73DqfOkvipH6qvypM4qR+qrcvSoL+k0JYQQQtQCSbhCCCFELZCEW8xisTB16lS3ZzWL85P6qhypr8qTOqscqa/K0aO+5BquEEIIUQukhSuEEELUAkm4QgghRC2QhCuEEELUAkm4xd5++22aNm2Kp6cnPXr0YNOmTXqHVCfFx8fTrVs3fH19adiwIUOHDiUxMVHvsOqNGTNmoGkaY8eO1TuUOuvo0aPcfffdBAcH4+XlRceOHdmyZYveYdVJdrudyZMn06xZM7y8vGjRogXPP/880jXnrJ9++okhQ4YQERGBpml8/fXXbuuVUkyZMoXw8HC8vLzo168ff/zxR43EIgkX+Oyzzxg3bhxTp05l27ZtdO7cmQEDBpCamqp3aHXO2rVriYuLY8OGDaxYsYLCwkL69+9PTk6O3qHVeZs3b+bdd9+lU6dOeodSZ505c4bevXvj4eHBDz/8wN69e3nllVcIDAzUO7Q66cUXX2T27Nm89dZbJCQk8OKLL/LSSy/x5ptv6h1anZGTk0Pnzp15++23y13/0ksv8cYbb/DOO++wceNGvL29GTBgAPn5+dUfjBKqe/fuKi4uzjVvt9tVRESEio+P1zGq+iE1NVUBau3atXqHUqdlZWWp6OhotWLFCnXttdeqMWPG6B1SnTRhwgR11VVX6R1GvTF48GB1//33uy279dZb1YgRI3SKqG4D1OLFi13zDodDhYWFqZdfftm1LD09XVksFrVgwYJq3/9l38ItKChg69at9OvXz7XMYDDQr18/1q9fr2Nk9UNGRgYAQUFBOkdSt8XFxTF48GC3z5ko65tvvqFr167ccccdNGzYkJiYGN577z29w6qzevXqxapVq9i3bx8AO3fu5JdffmHQoEE6R1Y/JCUlkZKS4vZ36e/vT48ePWrk+79ejxZUHU6dOoXdbic0NNRteWhoKL///rtOUdUPDoeDsWPH0rt3bzp06KB3OHXWwoUL2bZtG5s3b9Y7lDrvwIEDzJ49m3HjxvHvf/+bzZs3M3r0aMxmM7GxsXqHV+c8/fTTZGZm0qZNG4xGI3a7nenTpzNixAi9Q6sXUlJSAMr9/i9ZV50u+4Qrqi4uLo49e/bwyy+/6B1KnXX48GHGjBnDihUr8PT01DucOs/hcNC1a1deeOEFAGJiYtizZw/vvPOOJNxyfP7558yfP59PP/2U9u3bs2PHDsaOHUtERITUVx102Z9SDgkJwWg0usbWLXHixAnCwsJ0iqruGzVqFN999x2rV6+mcePGeodTZ23dupXU1FSuuOIKTCYTJpOJtWvX8sYbb2AymbDb7XqHWKeEh4fTrl07t2Vt27YlOTlZp4jqtieffJKnn36au+66i44dO3LPPffwxBNPEB8fr3do9ULJd3xtff9f9gnXbDZz5ZVXsmrVKtcyh8PBqlWr6Nmzp46R1U1KKUaNGsXixYv58ccfadasmd4h1Wl9+/Zl9+7d7NixwzV17dqVESNGsGPHDoxGo94h1im9e/cuc5vZvn37iIqK0imiui03N7fMWOBGoxGHw6FTRPVLs2bNCAsLc/v+z8zMZOPGjTXy/S+nlIFx48YRGxtL165d6d69O7NmzSInJ4f77rtP79DqnLi4OD799FP+97//4evr67rO4e/vj5eXl87R1T2+vr5lrm97e3sTHBws173L8cQTT9CrVy9eeOEFhg0bxqZNm5gzZw5z5szRO7Q6aciQIUyfPp3IyEjat2/P9u3befXVV7n//vv1Dq3OyM7OZv/+/a75pKQkduzYQVBQEJGRkYwdO5b/+7//Izo6mmbNmjF58mQiIiIYOnRo9QdT7f2e66k333xTRUZGKrPZrLp37642bNigd0h1ElDuNHfuXL1DqzfktqAL+/bbb1WHDh2UxWJRbdq0UXPmzNE7pDorMzNTjRkzRkVGRipPT0/VvHlzNWnSJGWz2fQOrc5YvXp1ud9ZsbGxSinnrUGTJ09WoaGhymKxqL59+6rExMQaiUVGCxJCCCFqwWV/DVcIIYSoDZJwhRBCiFogCVcIIYSoBZJwhRBCiFogCVcIIYSoBZJwhRBCiFogCVcIIYSoBZJwhRBCiFogCVcIUSGapvH111/rHYYQ9ZYkXCHqgZEjR6JpWplp4MCBeocmhKggGbxAiHpi4MCBzJ07122ZxWLRKRohRGVJC1eIesJisRAWFuY2BQYGAs7TvbNnz2bQoEF4eXnRvHlzvvjiC7f37969m7///e94eXkRHBzMww8/THZ2tluZDz/8kPbt22OxWAgPD2fUqFFu60+dOsUtt9yC1WolOjqab775xrXuzJkzjBgxggYNGuDl5UV0dHSZHwhCXM4k4QpxiZg8eTK33XYbO3fuZMSIEdx1110kJCQAkJOTw4ABAwgMDGTz5s0sWrSIlStXuiXU2bNnExcXx8MPP8zu3bv55ptvaNmypds+nn32WYYNG8auXbu44YYbGDFiBGlpaa797927lx9++IGEhARmz55NSEhI7VWAEHVdjYxBJISoVrGxscpoNCpvb2+3afr06Uop57CJjzzyiNt7evTooR599FGllFJz5sxRgYGBKjs727X++++/VwaDQaWkpCillIqIiFCTJk06bwyAeuaZZ1zz2dnZClA//PCDUkqpIUOGqPvuu696DliIS5BcwxWinrjuuuuYPXu227KgoCDX6549e7qt69mzJzt27AAgISGBzp074+3t7Vrfu3dvHA4HiYmJaJrGsWPH6Nu37wVj6NSpk+u1t7c3fn5+pKamAvDoo49y2223sW3bNvr378/QoUPp1atXlY5ViEuRJFwh6glvb+8yp3iri5eXV4XKeXh4uM1rmobD4QBg0KBBHDp0iCVLlrBixQr69u1LXFwcM2fOrPZ4haiP5BquEJeIDRs2lJlv27YtAG3btmXnzp3k5OS41q9btw6DwUDr1q3x9fWladOmrFq16qJiaNCgAbGxsXzyySfMmjWLOXPmXNT2hLiUSAtXiHrCZrORkpLitsxkMrk6Ji1atIiuXbty1VVXMX/+fDZt2sQHH3wAwIgRI5g6dSqxsbFMmzaNkydP8vjjj3PPPfcQGhoKwLRp03jkkUdo2LAhgwYNIisri3Xr1vH4449XKL4pU6Zw5ZVX0r59e2w2G999950r4QshJOEKUW8sXbqU8PBwt2WtW7fm999/B5w9iBcuXMhjjz1GeHg4CxYsoF27dgBYrVaWLVvGmDFj6NatG1arldtuu41XX33Vta3Y2Fjy8/N57bXXGD9+PCEhIdx+++0Vjs9sNjNx4kQOHjyIl5cXV199NQsXLqyGIxfi0qAppZTeQQghLo6maSxevJihQ4fqHYoQ4jzkGq4QQghRCyThCiGEELVAruEKcQmQK0NC1H3SwhVCCCFqgSRcIYQQohZIwhVCCCFqgSRcIYQQohZIwhVCCCFqgSRcIYQQohZIwhVCCCFqgSRcIYQQohZIwhVCCCFqwf8D/+DH3JZVJVoAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T_T5GyCgSCte",
    "outputId": "f876e4a9-8ff5-4dbf-83ed-5a456d5bfce0"
   },
   "execution_count": 34,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training accuracy: 100.00%\n",
      "Validation accuracy: 94.63%\n",
      "Test accuracy: 94.33%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "rHbVtTJkSCLc"
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "9yV6agpMSCIk"
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "notebook_end_time = time.time()\n",
    "runtime_in_seconds = notebook_end_time - notebook_start_time\n",
    "\n",
    "# format as minutes and seconds\n",
    "minutes, seconds = divmod(runtime_in_seconds, 60)\n",
    "print(f\"Notebook runtime: {int(minutes)} min {seconds:.2f} sec\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PRKL96uxqKc0",
    "outputId": "fe5b0fec-91b9-407e-afaa-a948849d8aea"
   },
   "execution_count": 35,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Notebook runtime: 2 min 27.33 sec\n"
     ]
    }
   ]
  }
 ]
}
