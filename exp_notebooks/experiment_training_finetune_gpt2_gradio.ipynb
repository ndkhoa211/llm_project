{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "notebook_start_time = time.time()"
      ],
      "metadata": {
        "id": "hzYeRD3WWVkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "id": "n3kYeCGFXwPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Setup"
      ],
      "metadata": {
        "id": "6lKganyVPjGf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX1yai0zPaX7",
        "outputId": "2e4718e3-732a-4b60-e67c-ac47cf63094c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradio version: 5.31.0\n"
          ]
        }
      ],
      "source": [
        "# Import/install Gradio\n",
        "try:\n",
        "    import gradio as gr\n",
        "except:\n",
        "    !pip -q install gradio\n",
        "    import gradio as gr\n",
        "\n",
        "print(f\"Gradio version: {gr.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Create title, description and article strings\n",
        "title = \"GPT2 From Scratch\"\n",
        "description = \"Finetune GPT2 for classification task and instruction task.\"\n",
        "article = \"Learning LLM from scratch\"\n",
        "\n",
        "\n",
        "def greet(name, intensity):\n",
        "    return \"Hello, \" + name + \"!\" * int(intensity)\n",
        "\n",
        "# Create the Gradio demo\n",
        "demo = gr.Interface(fn=greet, # mapping function from input to output\n",
        "                    inputs=[\"text\", \"slider\"],\n",
        "                    outputs=[\"text\"],\n",
        "                    #examples=example_list,\n",
        "                    title=title,\n",
        "                    description=description,\n",
        "                    article=article,\n",
        "                    )\n",
        "\n",
        "# Launch the demo!\n",
        "demo.launch(debug=False, # print errors locally?\n",
        "            share=True) # generate a publically shareable URL?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "bXj42eH9Pkrr",
        "outputId": "0f980a68-548b-4c1b-edcd-871ef3f10c80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://76a5dfc1d5019f4fc1.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://76a5dfc1d5019f4fc1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
        "INPUT_PROMPT = \"Every effort moves\"\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"drop_rate\": 0.0,        # Dropout rate\n",
        "    \"qkv_bias\": True         # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
      ],
      "metadata": {
        "id": "FakFzGZJbBNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Preparing pretrained GPT2\n",
        "\n",
        "\n",
        "- Let's (2) train GPT2 with:\n",
        "  1. LoRA\n",
        "  2. add warmup rate, cosine decay, gradient clipping\n",
        "\n",
        "- And compare with (1) baseline pretrain and (3) loaded OpenAI weight"
      ],
      "metadata": {
        "id": "s1QZel_VRVNh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.0 Preparing architecture"
      ],
      "metadata": {
        "id": "xrsA5yYLSXPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.0.1 GPT2 architecture"
      ],
      "metadata": {
        "id": "xJPCWESbTM4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let's load:\n",
        "  1. `MultiHeadAttention` class\n",
        "  2. `LayerNorm` class\n",
        "  3. `GELU` class\n",
        "  4. `FeedForward` class\n",
        "  5. `TransformerBlock` class\n",
        "  6. `GPT2Model` class"
      ],
      "metadata": {
        "id": "PF1_4drVTTN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_embedding_dim,\n",
        "                 output_embedding_dim,\n",
        "                 context_length,\n",
        "                 dropout,\n",
        "                 num_heads,\n",
        "                 qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (output_embedding_dim % num_heads == 0), \\\n",
        "            \"output_embedding_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.output_embedding_dim = output_embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = output_embedding_dim // num_heads\n",
        "        self.W_query = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
        "                                 bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
        "                               bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
        "                                 bias=qkv_bias)\n",
        "        self.output_projection = nn.Linear(output_embedding_dim,\n",
        "                                           output_embedding_dim)  # to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        batch, num_tokens, input_embedding_dim = inputs.shape\n",
        "\n",
        "        # qkv shapes : (batch, num_tokens, output_embedding_dim)\n",
        "        keys = self.W_key(inputs)\n",
        "        values = self.W_value(inputs)\n",
        "        queries = self.W_query(inputs)\n",
        "\n",
        "        # qkv shapes : (batch, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # qkv shapes : (batch, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "\n",
        "        # compute attention scores for each head\n",
        "        attention_scores = queries @ keys.transpose(3, 2)\n",
        "        attention_scores.masked_fill_(\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], - torch.inf)\n",
        "\n",
        "        # compute attention weights + dropout\n",
        "        masked_attention_weight = torch.softmax(\n",
        "            attention_scores / (keys.shape[-1] ** 0.5),\n",
        "            dim=-1)\n",
        "        masked_attention_dropout_weight = self.dropout(masked_attention_weight)\n",
        "\n",
        "        # compute context vectors\n",
        "        # shape : (batch, num_tokens, num_heads, head_dim)\n",
        "        context_vector = (masked_attention_dropout_weight @ values).transpose(1, 2)\n",
        "\n",
        "        # combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        # shape : (batch, num_tokens, output_embedding_dim)\n",
        "        context_vector = context_vector.contiguous().view(\n",
        "            batch, num_tokens, self.output_embedding_dim)\n",
        "\n",
        "        # linear projection (optional)\n",
        "        context_vector = self.output_projection(context_vector)\n",
        "\n",
        "        return context_vector\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.epsilon = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1,\n",
        "                    unbiased=False,  # Bessel's correction (n-1)\n",
        "                    keepdim=True)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.epsilon)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(config[\"emb_dim\"],  # 768\n",
        "                      4 * config[\"emb_dim\"]),  # 3072\n",
        "            GELU(),  # 3072\n",
        "            nn.Linear(4 * config[\"emb_dim\"],  # 3072\n",
        "                      config[\"emb_dim\"])  # 768\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(input_embedding_dim=config[\"emb_dim\"],\n",
        "                                            output_embedding_dim=config[\"emb_dim\"],\n",
        "                                            context_length=config[\"context_length\"],\n",
        "                                            dropout=config[\"drop_rate\"],\n",
        "                                            num_heads=config[\"n_heads\"],\n",
        "                                            qkv_bias=config[\"qkv_bias\"])\n",
        "        self.feed_forward = FeedForward(config)\n",
        "        self.layer_norm1 = LayerNorm(config[\"emb_dim\"])\n",
        "        self.layer_norm2 = LayerNorm(config[\"emb_dim\"])\n",
        "        self.drop_skip = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # skip connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.layer_norm1(x)\n",
        "        x = self.attention(x)  # shape: [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_skip(x)\n",
        "        x = shortcut + x  # skip connection\n",
        "\n",
        "        # skip connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.layer_norm2(x)\n",
        "        x = self.feed_forward(x)\n",
        "        x = self.drop_skip(x)\n",
        "        x = shortcut + x  # skip connection\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT2Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(config[\"vocab_size\"],\n",
        "                                      config[\"emb_dim\"])\n",
        "        self.position_emb = nn.Embedding(config[\"context_length\"],\n",
        "                                         config[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "        self.transformer_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(config) for _ in range(config[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(config[\"emb_dim\"])\n",
        "\n",
        "        self.out_head = nn.Linear(config[\"emb_dim\"],\n",
        "                                  config[\"vocab_size\"],\n",
        "                                  bias=False)\n",
        "\n",
        "    def forward(self, input_token):\n",
        "        batch_size, sequence_length = input_token.shape\n",
        "        token_embeds = self.token_emb(input_token)\n",
        "        position_embeds = self.position_emb(\n",
        "            torch.arange(sequence_length,\n",
        "                         device=input_token.device))\n",
        "        embeds = token_embeds + position_embeds\n",
        "        x = self.drop_emb(embeds)\n",
        "        x = self.transformer_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GPT2DatasetV1(Dataset):\n",
        "  def __init__(self,\n",
        "               text,\n",
        "               tokenizer,\n",
        "               context_length, # length of each input vector\n",
        "               stride # chunk the text into overlapping sequence of context_length\n",
        "               ):\n",
        "    self.input_id_vectors = []\n",
        "    self.target_id_vectors = []\n",
        "\n",
        "    # tokenize the entire text\n",
        "    token_list = tokenizer.encode(text)\n",
        "\n",
        "    # append input and target vectors\n",
        "    for i in range(0, len(token_list) - context_length, stride):\n",
        "      input_vector = token_list[i:i+context_length]\n",
        "      target_vector = token_list[i+1:i+context_length+1]\n",
        "      self.input_id_vectors.append(torch.tensor(input_vector))\n",
        "      self.target_id_vectors.append(torch.tensor(target_vector))\n",
        "\n",
        "  # get the number of input vectors\n",
        "  def __len__(self):\n",
        "    return len(self.input_id_vectors)\n",
        "\n",
        "  # return the (input vector, target vector) pair\n",
        "  def __getitem__(self, id):\n",
        "    return self.input_id_vectors[id], self.target_id_vectors[id]\n",
        "\n",
        "\n",
        "\n",
        "def create_dataloader_V1(text,\n",
        "                 batch_size=4,\n",
        "                 context_length=256,\n",
        "                 stride=128,\n",
        "                 shuffle=True, # shuffle dataset\n",
        "                 drop_last=True, # drop last batch if it not equal required size\n",
        "                 num_workers=0 # number of CPU processes for preprocessing\n",
        "                 ):\n",
        "\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "  dataset = GPT2DatasetV1(text=text,\n",
        "                          tokenizer=tokenizer,\n",
        "                          context_length=context_length,\n",
        "                          stride=stride)\n",
        "\n",
        "  dataloader = DataLoader(dataset=dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=shuffle,\n",
        "                          drop_last=drop_last,\n",
        "                          num_workers=num_workers)\n",
        "\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "4j4N_iwcQ8TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.0.2 training and evaluation function"
      ],
      "metadata": {
        "id": "jbzxiuZkTWvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let's load:\n",
        "  1. `train_model_simple`: without warmup rate, cosine decay, gradient clipping\n",
        "  2. `train_model`\n",
        "  3. `evaluate_model`"
      ],
      "metadata": {
        "id": "_rdv-I4cTdWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_model_simple(model,\n",
        "                       train_loader,\n",
        "                       val_loader,\n",
        "                       optimizer,\n",
        "                       device,\n",
        "                       num_epochs,\n",
        "                       eval_freq,\n",
        "                       eval_iter,\n",
        "                       start_context,\n",
        "                       tokenizer):\n",
        "\n",
        "  # initialize lists to track losses and tokens seen\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "  track_tokens_seen = []\n",
        "  token_seen = 0\n",
        "  global_step = -1\n",
        "\n",
        "  # main training loop - iterate over training epochs\n",
        "  for epoch in range(num_epochs):\n",
        "    # set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # iterate over batches in each training epoch\n",
        "    for input_batch, target_batch in train_loader:\n",
        "      # reset loss gradients from previous batch iteration\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # calculate loss on current batch\n",
        "      loss = calc_loss_batch(input_batch,\n",
        "                             target_batch,\n",
        "                             model,\n",
        "                             device)\n",
        "\n",
        "      # backward pass to calculate loss gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # update model weights using loss gradients\n",
        "      optimizer.step()\n",
        "      token_seen += input_batch.numel()\n",
        "      global_step += 1\n",
        "\n",
        "      # optional evaluation step\n",
        "      if global_step % eval_freq == 0:\n",
        "        train_loss, val_loss = evaluate_model(model,\n",
        "                                              train_loader,\n",
        "                                              val_loader,\n",
        "                                              device,\n",
        "                                              eval_iter)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        track_tokens_seen.append(token_seen)\n",
        "        # print training and evaluation set loss\n",
        "        print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "              f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "    # generative sample text for visual inspection\n",
        "    generate_and_print_sample(model,\n",
        "                              tokenizer,\n",
        "                              device,\n",
        "                              start_context)\n",
        "\n",
        "  return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ORIG_BOOK_VERSION = False\n",
        "def train_model(model,\n",
        "                train_loader,\n",
        "                val_loader,\n",
        "                optimizer,\n",
        "                device,\n",
        "                n_epochs,\n",
        "                eval_freq,\n",
        "                eval_iter,\n",
        "                start_context,\n",
        "                tokenizer,\n",
        "                warmup_steps,\n",
        "                initial_lr=3e-05,\n",
        "                min_lr=1e-6):\n",
        "\n",
        "  train_losses, val_losses = [], []\n",
        "  track_tokens_seen, track_lrs = [], []\n",
        "\n",
        "  token_seen = 0\n",
        "  global_step = -1\n",
        "\n",
        "  # retrieve the maximum/peak learning rate from the optimizer\n",
        "  peak_lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "  # calculate the total number of iterations in the training process\n",
        "  total_training_steps = len(train_loader) * n_epochs\n",
        "\n",
        "  # calculate the learning rate increment during the warmup phase\n",
        "  lr_increment = (peak_lr - initial_lr) / warmup_steps\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    for input_batch, target_batch in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      global_step += 1\n",
        "\n",
        "      # adjust the learning rate based on the current phase (warmup or cosine)\n",
        "      if global_step < warmup_steps:\n",
        "        lr = initial_lr + global_step * lr_increment\n",
        "      else:\n",
        "        # cosine annealing after warmup\n",
        "        progress = ((global_step - warmup_steps) /\n",
        "                    (total_training_steps - warmup_steps))\n",
        "        lr = (min_lr +\n",
        "         (peak_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress)))\n",
        "\n",
        "      # apply the calculated learning rate to the optimizer\n",
        "      for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "      track_lrs.append(lr) # store the current learning rate\n",
        "\n",
        "      # calculate and backpropagate the loss\n",
        "      loss = calc_loss_batch(input_batch,\n",
        "                             target_batch,\n",
        "                             model,\n",
        "                             device)\n",
        "      loss.backward()\n",
        "\n",
        "      # apply gradient clipping after the warmup phase to avoid exploding gradients\n",
        "      if ORIG_BOOK_VERSION:\n",
        "        if global_step > warmup_steps:\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "      else:\n",
        "        # the book originally used global_step > warmup_steps, which led to a skipped clipping step after warmup\n",
        "        if global_step >= warmup_steps:\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "      optimizer.step()\n",
        "      token_seen += input_batch.numel()\n",
        "\n",
        "      # periodically evaluate the model on the training and validation sets\n",
        "      if global_step % eval_freq == 0:\n",
        "        train_loss, val_loss = evaluate_model(model,\n",
        "                                              train_loader,\n",
        "                                              val_loader,\n",
        "                                              device,\n",
        "                                              eval_iter)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        track_tokens_seen.append(token_seen)\n",
        "        # print the current losses\n",
        "        print(f\"Ep {epoch+1} (Iter {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, \"\n",
        "                      f\"Val loss {val_loss:.3f}\")\n",
        "\n",
        "    # generate and print a sample from the model to monitor progess\n",
        "    generate_and_print_sample(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=device,\n",
        "        start_context=start_context\n",
        "    )\n",
        "\n",
        "  return train_losses, val_losses, track_tokens_seen, track_lrs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(model,\n",
        "                    train_loader,\n",
        "                    val_loader,\n",
        "                    device,\n",
        "                    eval_iter):\n",
        "  # set model to evaluation mode\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    # calculate loss\n",
        "    train_loss = calc_loss_loader(train_loader,\n",
        "                                  model,\n",
        "                                  device,\n",
        "                                  num_batches=eval_iter)\n",
        "    val_loss = calc_loss_loader(val_loader,\n",
        "                                model,\n",
        "                                device,\n",
        "                                num_batches=eval_iter)\n",
        "\n",
        "  # set model back to training mode\n",
        "  model.train()\n",
        "  return train_loss, val_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############################### utils functions #############################\n",
        "############################### utils functions #############################\n",
        "############################### utils functions #############################\n",
        "############################### utils functions #############################\n",
        "############################### utils functions #############################\n",
        "\n",
        "\n",
        "def calc_loss_batch(input_batch,\n",
        "                    target_batch,\n",
        "                    model,\n",
        "                    device):\n",
        "  input_batch = input_batch.to(device)\n",
        "  target_batch = target_batch.to(device)\n",
        "\n",
        "  logits = model(input_batch)\n",
        "  loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1),\n",
        "                                           target_batch.flatten())\n",
        "  return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(dataloader,\n",
        "                     model,\n",
        "                     device,\n",
        "                     num_batches=None):\n",
        "  total_loss = 0.\n",
        "  if len(dataloader) == 0:\n",
        "    return float(\"nan\")\n",
        "  elif num_batches is None:\n",
        "    num_batches = len(dataloader)\n",
        "  else:\n",
        "    # reduce the number of batches to match the total number of batches in the data loader\n",
        "    # if num_batches exceeds the number of batches in the data loader\n",
        "    num_batches = min(num_batches, len(dataloader))\n",
        "  for i, (input_batch, target_batch) in enumerate(dataloader):\n",
        "    if i < num_batches:\n",
        "      loss = calc_loss_batch(input_batch,\n",
        "                             target_batch,\n",
        "                             model,\n",
        "                             device)\n",
        "      total_loss += loss.item()\n",
        "    else:\n",
        "      break\n",
        "  return total_loss / num_batches\n",
        "\n",
        "\n",
        "\n",
        "def generate_and_print_sample(model,\n",
        "                              tokenizer,\n",
        "                              device,\n",
        "                              start_context):\n",
        "  # set model to evaluation mode\n",
        "  model.eval()\n",
        "  context_size = model.position_emb.weight.shape[0]\n",
        "  encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "  with torch.no_grad():\n",
        "    token_ids = generate_text_simple(model=model,\n",
        "                                     input_batch=encoded,\n",
        "                                     max_new_tokens=50,\n",
        "                                     context_size=context_size)\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \")) # compact print format\n",
        "  # set model back to training mode\n",
        "  model.train()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "  encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "  # turn the list of token IDs into tensor with batch dimension\n",
        "  encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "  return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(encoded_tensor, tokenizer):\n",
        "  # turn tensor without batch dimension to list\n",
        "  token_ids = encoded_tensor.squeeze(0).tolist()\n",
        "  text = tokenizer.decode(token_ids)\n",
        "  return text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_text_simple(model,\n",
        "                         input_batch,  # [batch, num_tokens]\n",
        "                         max_new_tokens,  # numbers of new tokens to be predicted\n",
        "                         context_size):\n",
        "    for _ in range(max_new_tokens):\n",
        "        # crop current context if it exceeds the supported context_size\n",
        "        crop_input_batch = input_batch[:, -context_size:]\n",
        "\n",
        "        # predict next token\n",
        "        with torch.no_grad():\n",
        "            logits = model(crop_input_batch)\n",
        "\n",
        "        # consider only logits of the last token\n",
        "        logits = logits[:, -1, :]  # (batch, n_tokens, vocab_size) -> (batch, vocab_size)\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "        predicted_tokens = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "        # update input_batch (append predicted tokens to the sequences)\n",
        "        input_batch = torch.cat([input_batch, predicted_tokens], dim=-1)  # [batch, num_tokens+1]\n",
        "\n",
        "    return input_batch\n",
        "\n",
        "\n",
        "\n",
        "def generate_text(model,\n",
        "                  input_batch,\n",
        "                  max_new_tokens,\n",
        "                  context_size,\n",
        "                  temperature=0.0,\n",
        "                  top_k=None,\n",
        "                  eos_id=None):\n",
        "  for _ in range(max_new_tokens):\n",
        "    # crop current context if it exceeds the supported context_size\n",
        "    crop_input_batch = input_batch[:, -context_size:]\n",
        "\n",
        "    # predict next token\n",
        "    with torch.no_grad():\n",
        "      logits = model(crop_input_batch)\n",
        "\n",
        "    # consider only logits of the last token\n",
        "    logits = logits[:, -1, :] # (batch, n_tokens, vocab_size) -> (batch, vocab_size)\n",
        "\n",
        "    # NEW: filter logits with top_k sampling\n",
        "    if top_k is not None:\n",
        "      # keep only top_k values\n",
        "      top_logits, _ = torch.topk(logits, top_k)\n",
        "      min_val = top_logits[:, -1] # min value among the top_k values\n",
        "      # all values other than top_k values will be set to -inf\n",
        "      logits = torch.where(logits < min_val,\n",
        "                           torch.tensor(-torch.inf).to(logits.device),\n",
        "                           logits)\n",
        "\n",
        "    # NEW: temperature scaling\n",
        "    if temperature > 0.0:\n",
        "      logits = logits / temperature\n",
        "\n",
        "      probas = torch.softmax(logits, dim=-1) # (batch, vocab_size)\n",
        "      predicted_tokens = torch.multinomial(probas, num_samples=1) # (batch, 1)\n",
        "\n",
        "    else: # same as before\n",
        "      #probas = torch.softmax(logits, dim=-1) # (batch, vocab_size)\n",
        "      predicted_tokens = torch.argmax(logits, dim=-1, keepdim=True) # (batch, 1)\n",
        "\n",
        "    if predicted_tokens == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "    # update input_batch (append predicted tokens to the sequences)\n",
        "    input_batch = torch.cat([input_batch, predicted_tokens], dim=1) # [batch, num_tokens+1]\n",
        "\n",
        "  return input_batch\n",
        "############################### utils functions #############################\n",
        "############################### utils functions #############################\n",
        "############################### utils functions #############################\n",
        "############################### utils functions #############################\n",
        "############################### utils functions #############################"
      ],
      "metadata": {
        "id": "-a_YJqyfTbOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.0.3 LoRA layer"
      ],
      "metadata": {
        "id": "tE3RWGDiVXK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "\n",
        "class LoRALayer(torch.nn.Module):\n",
        "  def __init__(self, in_dim, out_dim, rank, alpha):\n",
        "    super().__init__()\n",
        "    self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
        "    # Kaiming/He uniform initialization, similar to standard weight initialization\n",
        "    torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
        "    self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.alpha * (x @ self.A @ self.B)\n",
        "    return x\n",
        "\n",
        "\n",
        "class LinearLayerWithLoRA(torch.nn.Module):\n",
        "  def __init__(self, linear, rank, alpha):\n",
        "    super().__init__()\n",
        "    self.linear = linear\n",
        "    self.lora = LoRALayer(linear.in_features,\n",
        "                          linear.out_features,\n",
        "                          rank,\n",
        "                          alpha)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear(x) + self.lora(x)\n",
        "\n",
        "\n",
        "\n",
        "def replace_linear_with_lora(model, rank, alpha):\n",
        "  for name, module in model.named_children():\n",
        "    if isinstance(module, torch.nn.Linear):\n",
        "      print(f\"Replacing {name} with LinearLayerWithLoRA\")\n",
        "      setattr(model, name, LinearLayerWithLoRA(module, rank, alpha))\n",
        "    else:\n",
        "      # recursively apply the same function to child modules\n",
        "      replace_linear_with_lora(module, rank, alpha)"
      ],
      "metadata": {
        "id": "-3AroShTVZGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.0.4 Dataset"
      ],
      "metadata": {
        "id": "-jx5tDCMVjYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We'll use `war-and-peace.txt` dataset"
      ],
      "metadata": {
        "id": "DSRHTssSWHUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. App folder structure"
      ],
      "metadata": {
        "id": "wqLVmzFwW8go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "demos/\n",
        "└── gpt2/\n",
        "    ├── baseline_gpt2.pth\n",
        "    ├── stable_training_gpt2.pth\n",
        "    ├── stable_training_with_lora.pth\n",
        "    ├── openai_weights.pth\n",
        "    ├── instruction_finetune_gpt2.pth\n",
        "    ├── classification_finetune_gpt2.pth\n",
        "    ├── app.py\n",
        "    ├── examples/\n",
        "    │   ├── example_1\n",
        "    │   ├── example_2\n",
        "    │   └── example_3\n",
        "    ├── model.py\n",
        "    └── requirements.txt\n",
        "```"
      ],
      "metadata": {
        "id": "I40kbxjZXuSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.0 create demos/gpt2 folder"
      ],
      "metadata": {
        "id": "wtLHDHAIY1gN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Create gpt2 demo path\n",
        "gpt2_demo_path = Path(\"demos/gpt2/\")\n",
        "\n",
        "# Remove files that might already exist there and create new directory\n",
        "if gpt2_demo_path.exists():\n",
        "    shutil.rmtree(gpt2_demo_path)\n",
        "# If the file doesn't exist, create it anyway\n",
        "gpt2_demo_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Check what's in the folder\n",
        "!ls demos/gpt2/"
      ],
      "metadata": {
        "id": "9LftAfATY0Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1\n",
        "\n",
        "- We need:\n",
        "  1. `baseline_gpt2.pth`\n",
        "  2. `stable_training_gpt2.pth`\n",
        "  3. `stable_training_with_lora.pth`\n",
        "  4. `openai_weights.pth`\n",
        "  5. `instruction_finetune_gpt2.pth`\n",
        "  6. `classification_finetune_gpt2.pth`"
      ],
      "metadata": {
        "id": "vg-gyfARXKbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.1 creating `baseline_gpt2.pth`"
      ],
      "metadata": {
        "id": "DJfYXLjVad-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "\n",
        "torch.manual_seed(211)\n",
        "initialization_start_time = time.time()\n",
        "\n",
        "\n",
        "model = GPT2Model(BASE_CONFIG)\n",
        "\n",
        "\n",
        "initialization_end_time = time.time()\n",
        "runtime_in_seconds = initialization_end_time - initialization_start_time\n",
        "\n",
        "# format as minutes and seconds\n",
        "minutes, seconds = divmod(runtime_in_seconds, 60)\n",
        "print(f\"Initialization runtime: {int(minutes)} min {seconds:.2f} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twQwcP9aWagA",
        "outputId": "6e7a037f-860a-44c3-e264-7a9fda803d1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialization runtime: 0 min 1.33 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setup\n",
        "file_path = \"war_and_peace.txt\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "  text = f.read()\n",
        "\n",
        "total_characters = len(text)\n",
        "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "total_tokens = len(bpe_tokenizer.encode(text))\n",
        "print(f\"total characters: {total_characters}\")\n",
        "print(f\"total tokens: {total_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY_jBWrdWads",
        "outputId": "a0fd5a3c-3923-42cc-fe11-9fb916b5e6ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total characters: 3227520\n",
            "total tokens: 853923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train-test split\n",
        "train_ratio = 0.90\n",
        "split_id = int(train_ratio * len(text))\n",
        "train_text = text[:split_id]\n",
        "val_text = text[split_id:]"
      ],
      "metadata": {
        "id": "rSjBblUUWabK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataloaders\n",
        "torch.manual_seed(211)\n",
        "\n",
        "\n",
        "train_dataloader = create_dataloader_V1(text=train_text,\n",
        "                                        batch_size=8,\n",
        "                                        context_length=BASE_CONFIG[\"context_length\"],\n",
        "                                        stride=BASE_CONFIG['context_length'],\n",
        "                                        shuffle=True,\n",
        "                                        drop_last=True,\n",
        "                                        num_workers=0)\n",
        "\n",
        "val_dataloader = create_dataloader_V1(text=val_text,\n",
        "                                      batch_size=8,\n",
        "                                      context_length=BASE_CONFIG[\"context_length\"],\n",
        "                                      stride=BASE_CONFIG['context_length'],\n",
        "                                      shuffle=False,\n",
        "                                      drop_last=False,\n",
        "                                      num_workers=0)"
      ],
      "metadata": {
        "id": "14XuzOiYWaY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "\n",
        "if total_tokens * (train_ratio) < BASE_CONFIG[\"context_length\"]:\n",
        "    print(\"Not enough tokens for the training loader. \"\n",
        "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
        "          \"increase the `training_ratio`\")\n",
        "\n",
        "if total_tokens * (1-train_ratio) < BASE_CONFIG[\"context_length\"]:\n",
        "    print(\"Not enough tokens for the validation loader. \"\n",
        "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
        "          \"decrease the `training_ratio`\")"
      ],
      "metadata": {
        "id": "SkglIosTWaWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train loader:\")\n",
        "for x, y in train_dataloader:\n",
        "  print(x.shape, y.shape)\n",
        "\n",
        "print(\"\\nval loader:\")\n",
        "for x, y in val_dataloader:\n",
        "  print(x.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdeLKBiuWaT9",
        "outputId": "4f6a57e4-1b90-41bf-8047-5fb3b12441be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loader:\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "\n",
            "val loader:\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
            "torch.Size([4, 1024]) torch.Size([4, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's compute losses!!!\n",
        "import time\n",
        "compute_loss_start_time = time.time()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  train_loss = calc_loss_loader(train_dataloader,\n",
        "                                model,\n",
        "                                device)\n",
        "  val_loss = calc_loss_loader(val_dataloader,\n",
        "                              model,\n",
        "                              device)\n",
        "\n",
        "print(f\"train loss: {train_loss:.4f}\")\n",
        "print(f\"validation loss: {val_loss:.4f}\")\n",
        "\n",
        "\n",
        "compute_loss_end_time = time.time()\n",
        "runtime_in_seconds = compute_loss_end_time - compute_loss_start_time\n",
        "# format as minutes and seconds\n",
        "minutes, seconds = divmod(runtime_in_seconds, 60)\n",
        "print(f\"compute_loss runtime: {int(minutes)} min {seconds:.2f} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtBNaYgUWaRi",
        "outputId": "8d00361d-3091-4489-bab5-d7e43c95c7bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 10.9779\n",
            "validation loss: 10.9860\n",
            "compute_loss runtime: 0 min 21.26 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Clean up GPU memory ---\n",
        "import gc\n",
        "del train_loss, val_loss  # if you don't need them anymore\n",
        "#del train_dataloader, val_dataloader  # optional, if not reused\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()"
      ],
      "metadata": {
        "id": "cyy3gzPbWGyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "training_start_time = time.time()\n",
        "\n",
        "torch.manual_seed(211)\n",
        "model = GPT2Model(BASE_CONFIG).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                              lr=0.0004,\n",
        "                              weight_decay=0.1)\n",
        "num_epochs = 10\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(model=model,\n",
        "                                                             train_loader=train_dataloader,\n",
        "                                                             val_loader=val_dataloader,\n",
        "                                                             optimizer=optimizer,\n",
        "                                                             device=device,\n",
        "                                                             num_epochs=num_epochs,\n",
        "                                                             eval_freq=5,\n",
        "                                                             eval_iter=5,\n",
        "                                                             start_context=\"In the midst of winter, I found\",\n",
        "                                                             tokenizer=bpe_tokenizer)\n",
        "\n",
        "\n",
        "training_end_time = time.time()\n",
        "runtime_in_seconds = training_end_time - training_start_time\n",
        "# format as minutes and seconds\n",
        "minutes, seconds = divmod(runtime_in_seconds, 60)\n",
        "print(f\"device: {device}\")\n",
        "print(f\"training runtime: {int(minutes)} min {seconds:.2f} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kV55ZrkWaO5",
        "outputId": "8dc4185b-73f1-4dad-d93e-91710e0e26a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.567, Val loss 9.695\n",
            "Ep 1 (Step 000005): Train loss 7.888, Val loss 7.928\n",
            "Ep 1 (Step 000010): Train loss 6.825, Val loss 6.877\n",
            "Ep 1 (Step 000015): Train loss 6.525, Val loss 6.492\n",
            "Ep 1 (Step 000020): Train loss 6.415, Val loss 6.389\n",
            "Ep 1 (Step 000025): Train loss 6.188, Val loss 6.288\n",
            "Ep 1 (Step 000030): Train loss 6.117, Val loss 6.152\n",
            "Ep 1 (Step 000035): Train loss 5.982, Val loss 6.063\n",
            "Ep 1 (Step 000040): Train loss 5.944, Val loss 5.980\n",
            "Ep 1 (Step 000045): Train loss 5.666, Val loss 5.899\n",
            "Ep 1 (Step 000050): Train loss 5.698, Val loss 5.848\n",
            "Ep 1 (Step 000055): Train loss 5.685, Val loss 5.779\n",
            "Ep 1 (Step 000060): Train loss 5.596, Val loss 5.721\n",
            "Ep 1 (Step 000065): Train loss 5.525, Val loss 5.661\n",
            "Ep 1 (Step 000070): Train loss 5.493, Val loss 5.619\n",
            "Ep 1 (Step 000075): Train loss 5.452, Val loss 5.582\n",
            "Ep 1 (Step 000080): Train loss 5.393, Val loss 5.550\n",
            "Ep 1 (Step 000085): Train loss 5.270, Val loss 5.527\n",
            "Ep 1 (Step 000090): Train loss 5.314, Val loss 5.491\n",
            "In the midst of winter, I found the            the   the count, and the              the Emperor, and the         \n",
            "Ep 2 (Step 000095): Train loss 5.184, Val loss 5.455\n",
            "Ep 2 (Step 000100): Train loss 5.199, Val loss 5.439\n",
            "Ep 2 (Step 000105): Train loss 5.302, Val loss 5.403\n",
            "Ep 2 (Step 000110): Train loss 5.040, Val loss 5.363\n",
            "Ep 2 (Step 000115): Train loss 5.039, Val loss 5.344\n",
            "Ep 2 (Step 000120): Train loss 5.064, Val loss 5.338\n",
            "Ep 2 (Step 000125): Train loss 5.062, Val loss 5.306\n",
            "Ep 2 (Step 000130): Train loss 5.001, Val loss 5.290\n",
            "Ep 2 (Step 000135): Train loss 4.986, Val loss 5.284\n",
            "Ep 2 (Step 000140): Train loss 4.924, Val loss 5.255\n",
            "Ep 2 (Step 000145): Train loss 4.933, Val loss 5.246\n",
            "Ep 2 (Step 000150): Train loss 4.897, Val loss 5.224\n",
            "Ep 2 (Step 000155): Train loss 4.818, Val loss 5.208\n",
            "Ep 2 (Step 000160): Train loss 4.890, Val loss 5.202\n",
            "Ep 2 (Step 000165): Train loss 4.793, Val loss 5.179\n",
            "Ep 2 (Step 000170): Train loss 4.760, Val loss 5.172\n",
            "Ep 2 (Step 000175): Train loss 4.703, Val loss 5.157\n",
            "Ep 2 (Step 000180): Train loss 4.740, Val loss 5.137\n",
            "Ep 2 (Step 000185): Train loss 4.680, Val loss 5.131\n",
            "In the midst of winter, I found, and                       “I am I“I” said the   “I“I“I�\n",
            "Ep 3 (Step 000190): Train loss 4.618, Val loss 5.129\n",
            "Ep 3 (Step 000195): Train loss 4.649, Val loss 5.112\n",
            "Ep 3 (Step 000200): Train loss 4.632, Val loss 5.108\n",
            "Ep 3 (Step 000205): Train loss 4.630, Val loss 5.099\n",
            "Ep 3 (Step 000210): Train loss 4.592, Val loss 5.089\n",
            "Ep 3 (Step 000215): Train loss 4.569, Val loss 5.081\n",
            "Ep 3 (Step 000220): Train loss 4.545, Val loss 5.064\n",
            "Ep 3 (Step 000225): Train loss 4.532, Val loss 5.070\n",
            "Ep 3 (Step 000230): Train loss 4.610, Val loss 5.068\n",
            "Ep 3 (Step 000235): Train loss 4.389, Val loss 5.044\n",
            "Ep 3 (Step 000240): Train loss 4.501, Val loss 5.043\n",
            "Ep 3 (Step 000245): Train loss 4.421, Val loss 5.034\n",
            "Ep 3 (Step 000250): Train loss 4.458, Val loss 5.020\n",
            "Ep 3 (Step 000255): Train loss 4.519, Val loss 5.020\n",
            "Ep 3 (Step 000260): Train loss 4.308, Val loss 5.024\n",
            "Ep 3 (Step 000265): Train loss 4.329, Val loss 5.007\n",
            "Ep 3 (Step 000270): Train loss 4.460, Val loss 5.006\n",
            "Ep 3 (Step 000275): Train loss 4.391, Val loss 4.997\n",
            "Ep 3 (Step 000280): Train loss 4.393, Val loss 4.988\n",
            "In the midst of winter, I found  the one of the the army, and the the army, and the the army of the the army, and the the army of the the army of the the army of the the army of the \n",
            "Ep 4 (Step 000285): Train loss 4.323, Val loss 5.008\n",
            "Ep 4 (Step 000290): Train loss 4.264, Val loss 5.000\n",
            "Ep 4 (Step 000295): Train loss 4.272, Val loss 4.999\n",
            "Ep 4 (Step 000300): Train loss 4.235, Val loss 4.977\n",
            "Ep 4 (Step 000305): Train loss 4.310, Val loss 4.973\n",
            "Ep 4 (Step 000310): Train loss 4.304, Val loss 4.973\n",
            "Ep 4 (Step 000315): Train loss 4.185, Val loss 4.977\n",
            "Ep 4 (Step 000320): Train loss 4.147, Val loss 4.986\n",
            "Ep 4 (Step 000325): Train loss 4.099, Val loss 4.980\n",
            "Ep 4 (Step 000330): Train loss 4.226, Val loss 4.981\n",
            "Ep 4 (Step 000335): Train loss 4.215, Val loss 4.960\n",
            "Ep 4 (Step 000340): Train loss 4.056, Val loss 4.961\n",
            "Ep 4 (Step 000345): Train loss 4.125, Val loss 4.959\n",
            "Ep 4 (Step 000350): Train loss 4.130, Val loss 4.941\n",
            "Ep 4 (Step 000355): Train loss 4.041, Val loss 4.946\n",
            "Ep 4 (Step 000360): Train loss 4.066, Val loss 4.947\n",
            "Ep 4 (Step 000365): Train loss 4.061, Val loss 4.944\n",
            "Ep 4 (Step 000370): Train loss 4.063, Val loss 4.923\n",
            "Ep 4 (Step 000375): Train loss 4.058, Val loss 4.904\n",
            "In the midst of winter, I found it was the other, and the the one another, and the the one another, and the the the army, and the the army, and the the other, and the the army, and the of\n",
            "Ep 5 (Step 000380): Train loss 3.978, Val loss 4.920\n",
            "Ep 5 (Step 000385): Train loss 3.929, Val loss 4.943\n",
            "Ep 5 (Step 000390): Train loss 3.944, Val loss 4.954\n",
            "Ep 5 (Step 000395): Train loss 3.855, Val loss 4.945\n",
            "Ep 5 (Step 000400): Train loss 4.014, Val loss 4.955\n",
            "Ep 5 (Step 000405): Train loss 3.809, Val loss 4.963\n",
            "Ep 5 (Step 000410): Train loss 3.947, Val loss 4.952\n",
            "Ep 5 (Step 000415): Train loss 3.766, Val loss 4.948\n",
            "Ep 5 (Step 000420): Train loss 3.817, Val loss 4.947\n",
            "Ep 5 (Step 000425): Train loss 3.813, Val loss 4.952\n",
            "Ep 5 (Step 000430): Train loss 3.791, Val loss 4.954\n",
            "Ep 5 (Step 000435): Train loss 3.679, Val loss 4.951\n",
            "Ep 5 (Step 000440): Train loss 3.645, Val loss 4.964\n",
            "Ep 5 (Step 000445): Train loss 3.705, Val loss 4.950\n",
            "Ep 5 (Step 000450): Train loss 3.754, Val loss 4.930\n",
            "Ep 5 (Step 000455): Train loss 3.691, Val loss 4.938\n",
            "Ep 5 (Step 000460): Train loss 3.647, Val loss 4.930\n",
            "Ep 5 (Step 000465): Train loss 3.634, Val loss 4.926\n",
            "In the midst of winter, I found the Emperor’s  ’s,’s,’s,’s ’s a ’s ’s,’s,’s ’s,�\n",
            "Ep 6 (Step 000470): Train loss 3.540, Val loss 4.923\n",
            "Ep 6 (Step 000475): Train loss 3.434, Val loss 4.960\n",
            "Ep 6 (Step 000480): Train loss 3.489, Val loss 4.990\n",
            "Ep 6 (Step 000485): Train loss 3.388, Val loss 4.999\n",
            "Ep 6 (Step 000490): Train loss 3.518, Val loss 4.993\n",
            "Ep 6 (Step 000495): Train loss 3.482, Val loss 5.024\n",
            "Ep 6 (Step 000500): Train loss 3.402, Val loss 5.021\n",
            "Ep 6 (Step 000505): Train loss 3.355, Val loss 5.064\n",
            "Ep 6 (Step 000510): Train loss 3.383, Val loss 5.049\n",
            "Ep 6 (Step 000515): Train loss 3.267, Val loss 5.021\n",
            "Ep 6 (Step 000520): Train loss 3.225, Val loss 5.025\n",
            "Ep 6 (Step 000525): Train loss 3.236, Val loss 5.024\n",
            "Ep 6 (Step 000530): Train loss 3.210, Val loss 5.019\n",
            "Ep 6 (Step 000535): Train loss 3.205, Val loss 5.022\n",
            "Ep 6 (Step 000540): Train loss 3.258, Val loss 5.009\n",
            "Ep 6 (Step 000545): Train loss 3.231, Val loss 5.002\n",
            "Ep 6 (Step 000550): Train loss 3.291, Val loss 5.025\n",
            "Ep 6 (Step 000555): Train loss 3.111, Val loss 5.019\n",
            "Ep 6 (Step 000560): Train loss 3.059, Val loss 4.999\n",
            "In the midst of winter, I found, and the Emperor, and the Emperor’s and at the Emperor.  “I cannot do you,” “I can do you,” he. ’t you ’t\n",
            "Ep 7 (Step 000565): Train loss 3.000, Val loss 5.033\n",
            "Ep 7 (Step 000570): Train loss 3.026, Val loss 5.096\n",
            "Ep 7 (Step 000575): Train loss 3.025, Val loss 5.139\n",
            "Ep 7 (Step 000580): Train loss 2.905, Val loss 5.176\n",
            "Ep 7 (Step 000585): Train loss 2.855, Val loss 5.202\n",
            "Ep 7 (Step 000590): Train loss 2.942, Val loss 5.196\n",
            "Ep 7 (Step 000595): Train loss 2.790, Val loss 5.190\n",
            "Ep 7 (Step 000600): Train loss 2.734, Val loss 5.211\n",
            "Ep 7 (Step 000605): Train loss 2.729, Val loss 5.184\n",
            "Ep 7 (Step 000610): Train loss 2.594, Val loss 5.190\n",
            "Ep 7 (Step 000615): Train loss 2.702, Val loss 5.192\n",
            "Ep 7 (Step 000620): Train loss 2.710, Val loss 5.200\n",
            "Ep 7 (Step 000625): Train loss 2.563, Val loss 5.185\n",
            "Ep 7 (Step 000630): Train loss 2.629, Val loss 5.193\n",
            "Ep 7 (Step 000635): Train loss 2.476, Val loss 5.196\n",
            "Ep 7 (Step 000640): Train loss 2.545, Val loss 5.193\n",
            "Ep 7 (Step 000645): Train loss 2.569, Val loss 5.198\n",
            "Ep 7 (Step 000650): Train loss 2.543, Val loss 5.182\n",
            "Ep 7 (Step 000655): Train loss 2.489, Val loss 5.185\n",
            "In the midst of winter, I found meas a letter from the of the house, and the and the first time, the the Russians were to the army, and the and the general, and the other.  “The officer,” said\n",
            "Ep 8 (Step 000660): Train loss 2.425, Val loss 5.298\n",
            "Ep 8 (Step 000665): Train loss 2.402, Val loss 5.372\n",
            "Ep 8 (Step 000670): Train loss 2.296, Val loss 5.405\n",
            "Ep 8 (Step 000675): Train loss 2.288, Val loss 5.435\n",
            "Ep 8 (Step 000680): Train loss 2.233, Val loss 5.451\n",
            "Ep 8 (Step 000685): Train loss 2.216, Val loss 5.483\n",
            "Ep 8 (Step 000690): Train loss 2.127, Val loss 5.486\n",
            "Ep 8 (Step 000695): Train loss 1.984, Val loss 5.480\n",
            "Ep 8 (Step 000700): Train loss 2.046, Val loss 5.498\n",
            "Ep 8 (Step 000705): Train loss 2.217, Val loss 5.515\n",
            "Ep 8 (Step 000710): Train loss 1.995, Val loss 5.479\n",
            "Ep 8 (Step 000715): Train loss 2.103, Val loss 5.454\n",
            "Ep 8 (Step 000720): Train loss 2.001, Val loss 5.485\n",
            "Ep 8 (Step 000725): Train loss 1.916, Val loss 5.499\n",
            "Ep 8 (Step 000730): Train loss 1.843, Val loss 5.513\n",
            "Ep 8 (Step 000735): Train loss 1.844, Val loss 5.513\n",
            "Ep 8 (Step 000740): Train loss 1.891, Val loss 5.494\n",
            "Ep 8 (Step 000745): Train loss 1.821, Val loss 5.481\n",
            "Ep 8 (Step 000750): Train loss 1.831, Val loss 5.488\n",
            "In the midst of winter, I found,” said Pierre.  “I have just now,” said the Pierre, and at the room. “but I have not to know that is!”  ’s a little. “\n",
            "Ep 9 (Step 000755): Train loss 1.730, Val loss 5.614\n",
            "Ep 9 (Step 000760): Train loss 1.672, Val loss 5.667\n",
            "Ep 9 (Step 000765): Train loss 1.696, Val loss 5.681\n",
            "Ep 9 (Step 000770): Train loss 1.591, Val loss 5.759\n",
            "Ep 9 (Step 000775): Train loss 1.543, Val loss 5.764\n",
            "Ep 9 (Step 000780): Train loss 1.558, Val loss 5.812\n",
            "Ep 9 (Step 000785): Train loss 1.586, Val loss 5.796\n",
            "Ep 9 (Step 000790): Train loss 1.532, Val loss 5.792\n",
            "Ep 9 (Step 000795): Train loss 1.439, Val loss 5.802\n",
            "Ep 9 (Step 000800): Train loss 1.424, Val loss 5.812\n",
            "Ep 9 (Step 000805): Train loss 1.353, Val loss 5.812\n",
            "Ep 9 (Step 000810): Train loss 1.444, Val loss 5.845\n",
            "Ep 9 (Step 000815): Train loss 1.444, Val loss 5.851\n",
            "Ep 9 (Step 000820): Train loss 1.469, Val loss 5.847\n",
            "Ep 9 (Step 000825): Train loss 1.342, Val loss 5.818\n",
            "Ep 9 (Step 000830): Train loss 1.250, Val loss 5.827\n",
            "Ep 9 (Step 000835): Train loss 1.239, Val loss 5.831\n",
            "Ep 9 (Step 000840): Train loss 1.253, Val loss 5.830\n",
            "Ep 9 (Step 000845): Train loss 1.189, Val loss 5.865\n",
            "In the midst of winter, I found and so many clever or take part in the noted them. You shall do, and my body on my new and death!”  “I am very long been thinking about to have told them. You are your\n",
            "Ep 10 (Step 000850): Train loss 1.090, Val loss 6.024\n",
            "Ep 10 (Step 000855): Train loss 1.089, Val loss 6.067\n",
            "Ep 10 (Step 000860): Train loss 1.114, Val loss 6.092\n",
            "Ep 10 (Step 000865): Train loss 1.030, Val loss 6.122\n",
            "Ep 10 (Step 000870): Train loss 1.092, Val loss 6.084\n",
            "Ep 10 (Step 000875): Train loss 1.113, Val loss 6.210\n",
            "Ep 10 (Step 000880): Train loss 1.015, Val loss 6.202\n",
            "Ep 10 (Step 000885): Train loss 1.099, Val loss 6.136\n",
            "Ep 10 (Step 000890): Train loss 1.060, Val loss 6.183\n",
            "Ep 10 (Step 000895): Train loss 0.900, Val loss 6.193\n",
            "Ep 10 (Step 000900): Train loss 0.954, Val loss 6.188\n",
            "Ep 10 (Step 000905): Train loss 0.829, Val loss 6.207\n",
            "Ep 10 (Step 000910): Train loss 0.938, Val loss 6.237\n",
            "Ep 10 (Step 000915): Train loss 0.888, Val loss 6.212\n",
            "Ep 10 (Step 000920): Train loss 0.889, Val loss 6.187\n",
            "Ep 10 (Step 000925): Train loss 0.782, Val loss 6.172\n",
            "Ep 10 (Step 000930): Train loss 0.804, Val loss 6.197\n",
            "Ep 10 (Step 000935): Train loss 0.756, Val loss 6.243\n",
            "In the midst of winter, I found ! What is I cannot, the best position and the army and of seeking were not only did not wish to attain the army, but if it, but to besides, and of the same way essential for an army, for\n",
            "device: cuda\n",
            "training runtime: 15 min 38.69 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's plot a loss graph:\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_losses(epoch_seen,\n",
        "                tokens_seen,\n",
        "                train_losses,\n",
        "                val_losses):\n",
        "  \"\"\"Plot training and validation loss in xkcd style.\"\"\"\n",
        "\n",
        "  fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "  # plot training and validation loss against epochs\n",
        "  ax1.plot(epoch_seen, train_losses, label=\"Training Loss\")\n",
        "  ax1.plot(epoch_seen, val_losses, linestyle=\"-.\", label=\"Validation Loss\")\n",
        "  ax1.set_xlabel(\"Epoch\")\n",
        "  ax1.set_ylabel(\"Loss\")\n",
        "  ax1.legend(loc=\"upper right\")\n",
        "  ax1.xaxis.set_major_locator(MaxNLocator(integer=True)) # only show integer labels on x-axis\n",
        "\n",
        "  # create a second x-axis for token seen\n",
        "  ax2 = ax1.twiny() # create a second x-axis that shares the same y-axis\n",
        "  ax2.plot(tokens_seen, train_losses, alpha=0) # invisible plot for aligning ticks\n",
        "  ax2.set_xlabel(\"Tokens Seen\")\n",
        "\n",
        "  fig.tight_layout() # asjust layput to make room\n",
        "  plt.savefig(\"demos/gpt2/baseline_loss_plot.pdf\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "id": "xFvMO6hmWaMm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "1f59e3e6-f7e2-41d9-e9f0-df73e2e47ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(\"cpu\")\n",
        "model.eval()\n",
        "\n",
        "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(model=model,\n",
        "                                 input_batch=text_to_token_ids(\"In the midst of winter, I found\",\n",
        "                                                               bpe_tokenizer),\n",
        "                                 max_new_tokens=20,\n",
        "                                 context_size=BASE_CONFIG[\"context_length\"]\n",
        "                                 )\n",
        "\n",
        "\n",
        "print(\"output text: \\n\", token_ids_to_text(token_ids, bpe_tokenizer))"
      ],
      "metadata": {
        "id": "TvOUHbHAWaKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abb10609-2aa2-4da4-ebc8-08485b5b8426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output text: \n",
            " In the midst of winter, I found\n",
            "! What is\n",
            "I cannot, the best position and the army and of\n",
            "seeking were not\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model state_dict\n",
        "torch.save(model.state_dict(), \"demos/gpt2/baseline_gpt2.pth\")"
      ],
      "metadata": {
        "id": "g6WuQi5cWaHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model state_dict\n",
        "model = GPT2Model(BASE_CONFIG)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.load_state_dict(torch.load(\"demos/gpt2/baseline_gpt2.pth\", map_location=device, weights_only=True))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "lC5_8E87WaFc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cc2f639-a3e7-4ff8-c20b-ecb7bc58552f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Model(\n",
              "  (token_emb): Embedding(50257, 768)\n",
              "  (position_emb): Embedding(1024, 768)\n",
              "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
              "  (transformer_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.2 creating `stable_training_gpt2.pth`"
      ],
      "metadata": {
        "id": "Nlhh9T53pInt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Clean up GPU memory ---\n",
        "import gc\n",
        "# del train_loss, val_loss  # if you don't need them anymore\n",
        "#del train_dataloader, val_dataloader  # optional, if not reused\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()"
      ],
      "metadata": {
        "id": "3GJoZioufSbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "initial_lr = 0.0001\n",
        "peak_lr = 0.01\n",
        "\n",
        "\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "warmup_steps = int(0.2 * total_steps) # 20% warmup\n",
        "print(warmup_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKxo6RP_vwOy",
        "outputId": "8533d8bd-b602-44ab-98e6-a4c3c5ef2a1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "training_start_time = time.time()\n",
        "\n",
        "torch.manual_seed(211)\n",
        "model = GPT2Model(BASE_CONFIG).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                              lr=0.0004,\n",
        "                              weight_decay=0.1)\n",
        "\n",
        "train_losses, val_losses, tokens_seen, track_lrs = train_model(model=model,\n",
        "                                                    train_loader=train_dataloader,\n",
        "                                                    val_loader=val_dataloader,\n",
        "                                                    optimizer=optimizer,\n",
        "                                                    device=device,\n",
        "                                                    n_epochs=num_epochs,\n",
        "                                                    eval_freq=5,\n",
        "                                                    eval_iter=5,\n",
        "                                                    start_context=\"In the midst of winter, I found\",\n",
        "                                                    tokenizer=bpe_tokenizer,\n",
        "                                                    warmup_steps=warmup_steps,\n",
        "                                                    initial_lr=1e-5,\n",
        "                                                    min_lr=1e-5)\n",
        "\n",
        "\n",
        "training_end_time = time.time()\n",
        "runtime_in_seconds = training_end_time - training_start_time\n",
        "# format as minutes and seconds\n",
        "minutes, seconds = divmod(runtime_in_seconds, 60)\n",
        "print(f\"device: {device}\")\n",
        "print(f\"training runtime: {int(minutes)} min {seconds:.2f} sec\")"
      ],
      "metadata": {
        "id": "YrC8fFGeWaAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ff0ae9b-10cf-415f-81af-37cbd6b73d6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Iter 000000): Train loss 10.906, Val loss 10.922\n",
            "Ep 1 (Iter 000005): Train loss 10.361, Val loss 10.383\n",
            "Ep 1 (Iter 000010): Train loss 9.608, Val loss 9.636\n",
            "Ep 1 (Iter 000015): Train loss 9.151, Val loss 9.149\n",
            "Ep 1 (Iter 000020): Train loss 8.846, Val loss 8.826\n",
            "Ep 1 (Iter 000025): Train loss 8.458, Val loss 8.504\n",
            "Ep 1 (Iter 000030): Train loss 8.144, Val loss 8.175\n",
            "Ep 1 (Iter 000035): Train loss 7.774, Val loss 7.812\n",
            "Ep 1 (Iter 000040): Train loss 7.420, Val loss 7.445\n",
            "Ep 1 (Iter 000045): Train loss 6.967, Val loss 7.120\n",
            "Ep 1 (Iter 000050): Train loss 6.779, Val loss 6.853\n",
            "Ep 1 (Iter 000055): Train loss 6.585, Val loss 6.624\n",
            "Ep 1 (Iter 000060): Train loss 6.400, Val loss 6.460\n",
            "Ep 1 (Iter 000065): Train loss 6.351, Val loss 6.337\n",
            "Ep 1 (Iter 000070): Train loss 6.219, Val loss 6.251\n",
            "Ep 1 (Iter 000075): Train loss 6.173, Val loss 6.187\n",
            "Ep 1 (Iter 000080): Train loss 6.102, Val loss 6.136\n",
            "Ep 1 (Iter 000085): Train loss 5.947, Val loss 6.059\n",
            "Ep 1 (Iter 000090): Train loss 5.942, Val loss 6.001\n",
            "In the midst of winter, I found, and          ” ’s, and the    ’s the  ” ’s, and the ”  ’s, and\n",
            "Ep 2 (Iter 000095): Train loss 5.826, Val loss 5.931\n",
            "Ep 2 (Iter 000100): Train loss 5.797, Val loss 5.888\n",
            "Ep 2 (Iter 000105): Train loss 5.845, Val loss 5.826\n",
            "Ep 2 (Iter 000110): Train loss 5.612, Val loss 5.771\n",
            "Ep 2 (Iter 000115): Train loss 5.561, Val loss 5.734\n",
            "Ep 2 (Iter 000120): Train loss 5.548, Val loss 5.696\n",
            "Ep 2 (Iter 000125): Train loss 5.539, Val loss 5.639\n",
            "Ep 2 (Iter 000130): Train loss 5.416, Val loss 5.609\n",
            "Ep 2 (Iter 000135): Train loss 5.404, Val loss 5.576\n",
            "Ep 2 (Iter 000140): Train loss 5.336, Val loss 5.529\n",
            "Ep 2 (Iter 000145): Train loss 5.322, Val loss 5.506\n",
            "Ep 2 (Iter 000150): Train loss 5.268, Val loss 5.479\n",
            "Ep 2 (Iter 000155): Train loss 5.193, Val loss 5.446\n",
            "Ep 2 (Iter 000160): Train loss 5.246, Val loss 5.427\n",
            "Ep 2 (Iter 000165): Train loss 5.154, Val loss 5.412\n",
            "Ep 2 (Iter 000170): Train loss 5.104, Val loss 5.388\n",
            "Ep 2 (Iter 000175): Train loss 5.036, Val loss 5.363\n",
            "Ep 2 (Iter 000180): Train loss 5.076, Val loss 5.339\n",
            "Ep 2 (Iter 000185): Train loss 4.998, Val loss 5.313\n",
            "In the midst of winter, I found, and                       “I“Yes,“I“I“I“I“I“I\n",
            "Ep 3 (Iter 000190): Train loss 4.921, Val loss 5.306\n",
            "Ep 3 (Iter 000195): Train loss 4.956, Val loss 5.283\n",
            "Ep 3 (Iter 000200): Train loss 4.937, Val loss 5.270\n",
            "Ep 3 (Iter 000205): Train loss 4.940, Val loss 5.255\n",
            "Ep 3 (Iter 000210): Train loss 4.867, Val loss 5.236\n",
            "Ep 3 (Iter 000215): Train loss 4.860, Val loss 5.225\n",
            "Ep 3 (Iter 000220): Train loss 4.813, Val loss 5.199\n",
            "Ep 3 (Iter 000225): Train loss 4.805, Val loss 5.210\n",
            "Ep 3 (Iter 000230): Train loss 4.879, Val loss 5.201\n",
            "Ep 3 (Iter 000235): Train loss 4.651, Val loss 5.169\n",
            "Ep 3 (Iter 000240): Train loss 4.747, Val loss 5.159\n",
            "Ep 3 (Iter 000245): Train loss 4.671, Val loss 5.154\n",
            "Ep 3 (Iter 000250): Train loss 4.668, Val loss 5.142\n",
            "Ep 3 (Iter 000255): Train loss 4.772, Val loss 5.133\n",
            "Ep 3 (Iter 000260): Train loss 4.556, Val loss 5.120\n",
            "Ep 3 (Iter 000265): Train loss 4.557, Val loss 5.109\n",
            "Ep 3 (Iter 000270): Train loss 4.691, Val loss 5.100\n",
            "Ep 3 (Iter 000275): Train loss 4.602, Val loss 5.087\n",
            "Ep 3 (Iter 000280): Train loss 4.596, Val loss 5.075\n",
            "In the midst of winter, I found the whole  the old man, and the  the old man of the  the army, and the the army, and the the army, and the the French.     the army of the\n",
            "Ep 4 (Iter 000285): Train loss 4.525, Val loss 5.076\n",
            "Ep 4 (Iter 000290): Train loss 4.421, Val loss 5.058\n",
            "Ep 4 (Iter 000295): Train loss 4.474, Val loss 5.047\n",
            "Ep 4 (Iter 000300): Train loss 4.452, Val loss 5.038\n",
            "Ep 4 (Iter 000305): Train loss 4.531, Val loss 5.034\n",
            "Ep 4 (Iter 000310): Train loss 4.532, Val loss 5.042\n",
            "Ep 4 (Iter 000315): Train loss 4.404, Val loss 5.041\n",
            "Ep 4 (Iter 000320): Train loss 4.381, Val loss 5.051\n",
            "Ep 4 (Iter 000325): Train loss 4.305, Val loss 5.032\n",
            "Ep 4 (Iter 000330): Train loss 4.442, Val loss 5.031\n",
            "Ep 4 (Iter 000335): Train loss 4.422, Val loss 5.004\n",
            "Ep 4 (Iter 000340): Train loss 4.284, Val loss 5.002\n",
            "Ep 4 (Iter 000345): Train loss 4.341, Val loss 5.002\n",
            "Ep 4 (Iter 000350): Train loss 4.357, Val loss 4.993\n",
            "Ep 4 (Iter 000355): Train loss 4.274, Val loss 4.994\n",
            "Ep 4 (Iter 000360): Train loss 4.303, Val loss 4.991\n",
            "Ep 4 (Iter 000365): Train loss 4.294, Val loss 4.977\n",
            "Ep 4 (Iter 000370): Train loss 4.290, Val loss 4.958\n",
            "Ep 4 (Iter 000375): Train loss 4.287, Val loss 4.948\n",
            "In the midst of winter, I found it was the commander in the the commander in the the commander in the the army, and the the army, and the the army, and the the army, and the the army, and the the army\n",
            "Ep 5 (Iter 000380): Train loss 4.215, Val loss 4.950\n",
            "Ep 5 (Iter 000385): Train loss 4.201, Val loss 4.969\n",
            "Ep 5 (Iter 000390): Train loss 4.208, Val loss 4.971\n",
            "Ep 5 (Iter 000395): Train loss 4.101, Val loss 4.954\n",
            "Ep 5 (Iter 000400): Train loss 4.265, Val loss 4.948\n",
            "Ep 5 (Iter 000405): Train loss 4.072, Val loss 4.956\n",
            "Ep 5 (Iter 000410): Train loss 4.238, Val loss 4.973\n",
            "Ep 5 (Iter 000415): Train loss 4.056, Val loss 4.949\n",
            "Ep 5 (Iter 000420): Train loss 4.074, Val loss 4.938\n",
            "Ep 5 (Iter 000425): Train loss 4.083, Val loss 4.934\n",
            "Ep 5 (Iter 000430): Train loss 4.076, Val loss 4.947\n",
            "Ep 5 (Iter 000435): Train loss 3.979, Val loss 4.936\n",
            "Ep 5 (Iter 000440): Train loss 3.940, Val loss 4.944\n",
            "Ep 5 (Iter 000445): Train loss 3.982, Val loss 4.933\n",
            "Ep 5 (Iter 000450): Train loss 4.056, Val loss 4.923\n",
            "Ep 5 (Iter 000455): Train loss 3.965, Val loss 4.916\n",
            "Ep 5 (Iter 000460): Train loss 3.946, Val loss 4.914\n",
            "Ep 5 (Iter 000465): Train loss 3.925, Val loss 4.910\n",
            "In the midst of winter, I found it. I   “I have to the ” said the Emperor.     “I have to the “I am I” said the officer.    “Yes,\n",
            "Ep 6 (Iter 000470): Train loss 3.847, Val loss 4.905\n",
            "Ep 6 (Iter 000475): Train loss 3.766, Val loss 4.915\n",
            "Ep 6 (Iter 000480): Train loss 3.813, Val loss 4.918\n",
            "Ep 6 (Iter 000485): Train loss 3.724, Val loss 4.942\n",
            "Ep 6 (Iter 000490): Train loss 3.883, Val loss 4.942\n",
            "Ep 6 (Iter 000495): Train loss 3.856, Val loss 4.955\n",
            "Ep 6 (Iter 000500): Train loss 3.771, Val loss 4.951\n",
            "Ep 6 (Iter 000505): Train loss 3.742, Val loss 4.939\n",
            "Ep 6 (Iter 000510): Train loss 3.736, Val loss 4.937\n",
            "Ep 6 (Iter 000515): Train loss 3.583, Val loss 4.925\n",
            "Ep 6 (Iter 000520): Train loss 3.600, Val loss 4.928\n",
            "Ep 6 (Iter 000525): Train loss 3.622, Val loss 4.926\n",
            "Ep 6 (Iter 000530): Train loss 3.596, Val loss 4.928\n",
            "Ep 6 (Iter 000535): Train loss 3.601, Val loss 4.930\n",
            "Ep 6 (Iter 000540): Train loss 3.654, Val loss 4.922\n",
            "Ep 6 (Iter 000545): Train loss 3.612, Val loss 4.919\n",
            "Ep 6 (Iter 000550): Train loss 3.696, Val loss 4.925\n",
            "Ep 6 (Iter 000555): Train loss 3.537, Val loss 4.917\n",
            "Ep 6 (Iter 000560): Train loss 3.472, Val loss 4.924\n",
            "In the midst of winter, I found in and the more than the the commander in the and the same way.   The Emperor was the same time the Emperor of the same, and the the Emperor of the the Emperor of the more than the Emperor,\n",
            "Ep 7 (Iter 000565): Train loss 3.442, Val loss 4.930\n",
            "Ep 7 (Iter 000570): Train loss 3.508, Val loss 4.936\n",
            "Ep 7 (Iter 000575): Train loss 3.532, Val loss 4.960\n",
            "Ep 7 (Iter 000580): Train loss 3.442, Val loss 4.978\n",
            "Ep 7 (Iter 000585): Train loss 3.362, Val loss 4.991\n",
            "Ep 7 (Iter 000590): Train loss 3.429, Val loss 4.991\n",
            "Ep 7 (Iter 000595): Train loss 3.313, Val loss 4.997\n",
            "Ep 7 (Iter 000600): Train loss 3.319, Val loss 5.001\n",
            "Ep 7 (Iter 000605): Train loss 3.317, Val loss 4.992\n",
            "Ep 7 (Iter 000610): Train loss 3.156, Val loss 4.999\n",
            "Ep 7 (Iter 000615): Train loss 3.295, Val loss 4.993\n",
            "Ep 7 (Iter 000620): Train loss 3.270, Val loss 4.998\n",
            "Ep 7 (Iter 000625): Train loss 3.213, Val loss 5.005\n",
            "Ep 7 (Iter 000630): Train loss 3.207, Val loss 4.996\n",
            "Ep 7 (Iter 000635): Train loss 3.072, Val loss 5.003\n",
            "Ep 7 (Iter 000640): Train loss 3.175, Val loss 5.005\n",
            "Ep 7 (Iter 000645): Train loss 3.201, Val loss 5.011\n",
            "Ep 7 (Iter 000650): Train loss 3.190, Val loss 4.999\n",
            "Ep 7 (Iter 000655): Train loss 3.097, Val loss 5.003\n",
            "In the midst of winter, I found and my dear, and I have been my father is and that I am not of all that I have not to my own. I am not to my dear to the more than my heart. I have not  The only\n",
            "Ep 8 (Iter 000660): Train loss 3.117, Val loss 5.018\n",
            "Ep 8 (Iter 000665): Train loss 3.113, Val loss 5.055\n",
            "Ep 8 (Iter 000670): Train loss 3.050, Val loss 5.074\n",
            "Ep 8 (Iter 000675): Train loss 3.025, Val loss 5.098\n",
            "Ep 8 (Iter 000680): Train loss 2.985, Val loss 5.098\n",
            "Ep 8 (Iter 000685): Train loss 3.007, Val loss 5.115\n",
            "Ep 8 (Iter 000690): Train loss 2.876, Val loss 5.127\n",
            "Ep 8 (Iter 000695): Train loss 2.831, Val loss 5.138\n",
            "Ep 8 (Iter 000700): Train loss 2.855, Val loss 5.121\n",
            "Ep 8 (Iter 000705): Train loss 2.935, Val loss 5.133\n",
            "Ep 8 (Iter 000710): Train loss 2.845, Val loss 5.139\n",
            "Ep 8 (Iter 000715): Train loss 2.851, Val loss 5.135\n",
            "Ep 8 (Iter 000720): Train loss 2.827, Val loss 5.137\n",
            "Ep 8 (Iter 000725): Train loss 2.802, Val loss 5.146\n",
            "Ep 8 (Iter 000730): Train loss 2.750, Val loss 5.139\n",
            "Ep 8 (Iter 000735): Train loss 2.820, Val loss 5.142\n",
            "Ep 8 (Iter 000740): Train loss 2.757, Val loss 5.158\n",
            "Ep 8 (Iter 000745): Train loss 2.761, Val loss 5.157\n",
            "Ep 8 (Iter 000750): Train loss 2.734, Val loss 5.151\n",
            "In the midst of winter, I found in and the world, and the army, the and that the and the French army.   The Russians had been sent for the French.  The French in the French were  The French had been sent for the\n",
            "Ep 9 (Iter 000755): Train loss 2.730, Val loss 5.180\n",
            "Ep 9 (Iter 000760): Train loss 2.709, Val loss 5.218\n",
            "Ep 9 (Iter 000765): Train loss 2.605, Val loss 5.239\n",
            "Ep 9 (Iter 000770): Train loss 2.663, Val loss 5.246\n",
            "Ep 9 (Iter 000775): Train loss 2.624, Val loss 5.258\n",
            "Ep 9 (Iter 000780): Train loss 2.618, Val loss 5.279\n",
            "Ep 9 (Iter 000785): Train loss 2.635, Val loss 5.273\n",
            "Ep 9 (Iter 000790): Train loss 2.618, Val loss 5.288\n",
            "Ep 9 (Iter 000795): Train loss 2.572, Val loss 5.291\n",
            "Ep 9 (Iter 000800): Train loss 2.613, Val loss 5.296\n",
            "Ep 9 (Iter 000805): Train loss 2.564, Val loss 5.298\n",
            "Ep 9 (Iter 000810): Train loss 2.574, Val loss 5.304\n",
            "Ep 9 (Iter 000815): Train loss 2.591, Val loss 5.305\n",
            "Ep 9 (Iter 000820): Train loss 2.573, Val loss 5.304\n",
            "Ep 9 (Iter 000825): Train loss 2.529, Val loss 5.309\n",
            "Ep 9 (Iter 000830): Train loss 2.544, Val loss 5.312\n",
            "Ep 9 (Iter 000835): Train loss 2.517, Val loss 5.310\n",
            "Ep 9 (Iter 000840): Train loss 2.517, Val loss 5.314\n",
            "Ep 9 (Iter 000845): Train loss 2.445, Val loss 5.309\n",
            "In the midst of winter, I found and this.”  “And here,” said the door. “And I have I should not know what you and not, I have been in my dear friend. I know that you know what\n",
            "Ep 10 (Iter 000850): Train loss 2.446, Val loss 5.333\n",
            "Ep 10 (Iter 000855): Train loss 2.506, Val loss 5.356\n",
            "Ep 10 (Iter 000860): Train loss 2.485, Val loss 5.370\n",
            "Ep 10 (Iter 000865): Train loss 2.448, Val loss 5.378\n",
            "Ep 10 (Iter 000870): Train loss 2.497, Val loss 5.383\n",
            "Ep 10 (Iter 000875): Train loss 2.462, Val loss 5.386\n",
            "Ep 10 (Iter 000880): Train loss 2.521, Val loss 5.392\n",
            "Ep 10 (Iter 000885): Train loss 2.483, Val loss 5.391\n",
            "Ep 10 (Iter 000890): Train loss 2.483, Val loss 5.399\n",
            "Ep 10 (Iter 000895): Train loss 2.507, Val loss 5.397\n",
            "Ep 10 (Iter 000900): Train loss 2.426, Val loss 5.397\n",
            "Ep 10 (Iter 000905): Train loss 2.487, Val loss 5.401\n",
            "Ep 10 (Iter 000910): Train loss 2.411, Val loss 5.401\n",
            "Ep 10 (Iter 000915): Train loss 2.470, Val loss 5.404\n",
            "Ep 10 (Iter 000920): Train loss 2.463, Val loss 5.410\n",
            "Ep 10 (Iter 000925): Train loss 2.445, Val loss 5.411\n",
            "Ep 10 (Iter 000930): Train loss 2.423, Val loss 5.410\n",
            "Ep 10 (Iter 000935): Train loss 2.427, Val loss 5.413\n",
            "In the midst of winter, I found and this.”  “And here,” said the door. “And what I have a I can you, I have been here.”  “I am glad for, I,�\n",
            "device: cuda\n",
            "training runtime: 15 min 34.92 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(range(len(track_lrs)), track_lrs)\n",
        "plt.ylabel(\"Learning rate\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "No0wfwoDz_gd",
        "outputId": "a786d2b6-070e-4e81-e782-0d50a4531315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's plot a loss graph:\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_losses(epoch_seen,\n",
        "                tokens_seen,\n",
        "                train_losses,\n",
        "                val_losses):\n",
        "  \"\"\"Plot training and validation loss in xkcd style.\"\"\"\n",
        "\n",
        "  fig, ax1 = plt.subplots(figsize=(5, 5))\n",
        "\n",
        "  # plot training and validation loss against epochs\n",
        "  ax1.plot(epoch_seen, train_losses, label=\"Training Loss\")\n",
        "  ax1.plot(epoch_seen, val_losses, linestyle=\"-.\", label=\"Validation Loss\")\n",
        "  ax1.set_xlabel(\"Epoch\")\n",
        "  ax1.set_ylabel(\"Loss\")\n",
        "  ax1.legend(loc=\"upper right\")\n",
        "  ax1.xaxis.set_major_locator(MaxNLocator(integer=True)) # only show integer labels on x-axis\n",
        "\n",
        "  # create a second x-axis for token seen\n",
        "  ax2 = ax1.twiny() # create a second x-axis that shares the same y-axis\n",
        "  ax2.plot(tokens_seen, train_losses, alpha=0) # invisible plot for aligning ticks\n",
        "  ax2.set_xlabel(\"Tokens Seen\")\n",
        "\n",
        "  fig.tight_layout() # asjust layput to make room\n",
        "  plt.savefig(\"demos/gpt2/stable_training_loss_plot.pdf\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "id": "jh1HSbmCWZ-A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "outputId": "ccfc4bc6-3222-46c5-8831-c9e9783ce545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(\"cpu\")\n",
        "model.eval()\n",
        "\n",
        "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(model=model,\n",
        "                                 input_batch=text_to_token_ids(\"In the midst of winter, I found\",\n",
        "                                                               bpe_tokenizer),\n",
        "                                 max_new_tokens=20,\n",
        "                                 context_size=BASE_CONFIG[\"context_length\"]\n",
        "                                 )\n",
        "\n",
        "\n",
        "print(\"output text: \\n\", token_ids_to_text(token_ids, bpe_tokenizer))"
      ],
      "metadata": {
        "id": "fpwwkjZLWZ7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c41614c3-547d-4f75-e244-c01566f2ef56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output text: \n",
            " In the midst of winter, I found\n",
            "and this.”\n",
            "\n",
            "“And here,” said the door. �\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model state_dict\n",
        "torch.save(model.state_dict(), \"demos/gpt2/stable_training_gpt2.pth\")"
      ],
      "metadata": {
        "id": "NfNR_NP-WZ47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model state_dict\n",
        "model = GPT2Model(BASE_CONFIG)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.load_state_dict(torch.load(\"demos/gpt2/stable_training_gpt2.pth\", map_location=device, weights_only=True))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "J3CDnEjHWZ2s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50a27814-6417-47d8-f3b2-1d0c349df26f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Model(\n",
              "  (token_emb): Embedding(50257, 768)\n",
              "  (position_emb): Embedding(1024, 768)\n",
              "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
              "  (transformer_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.3 creating `stable_training_with_lora.pth`"
      ],
      "metadata": {
        "id": "SBJ4Zq8P3yeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Clean up GPU memory ---\n",
        "import gc\n",
        "# del train_loss, val_loss  # if you don't need them anymore\n",
        "#del train_dataloader, val_dataloader  # optional, if not reused\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()"
      ],
      "metadata": {
        "id": "HunJkZhDWZ0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2Model(BASE_CONFIG)"
      ],
      "metadata": {
        "id": "JD5ZuF45Oqio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters before: {total_params:,}\")\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters after: {total_params:,}\")"
      ],
      "metadata": {
        "id": "a4bpKrQXWZxl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f12d3712-b47a-4956-acef-bb24c1195016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total trainable parameters before: 163,037,184\n",
            "Total trainable parameters after: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "replace_linear_with_lora(model, rank=16, alpha=16)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable LoRA parameters: {total_params:,}\")"
      ],
      "metadata": {
        "id": "cc-koluAWZvP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e418b89-c219-4726-e5c7-8a38248e27f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing out_head with LinearLayerWithLoRA\n",
            "Total trainable LoRA parameters: 3,470,608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "dyzc_6f7WZsl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35191ce1-063f-48df-d4c1-103022a71a22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2Model(\n",
            "  (token_emb): Embedding(50257, 768)\n",
            "  (position_emb): Embedding(1024, 768)\n",
            "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
            "  (transformer_blocks): Sequential(\n",
            "    (0): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (1): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (2): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (3): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (4): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (5): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (6): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (7): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (8): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (9): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (10): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (11): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_norm): LayerNorm()\n",
            "  (out_head): LinearLayerWithLoRA(\n",
            "    (linear): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    (lora): LoRALayer()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "initial_lr = 0.0001\n",
        "peak_lr = 3e-5\n",
        "\n",
        "\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "warmup_steps = int(0.2 * total_steps) # 20% warmup\n",
        "print(warmup_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvPwTgfD5_VJ",
        "outputId": "6d9045c1-591e-481d-8299-119286fdfecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "training_start_time = time.time()\n",
        "\n",
        "torch.manual_seed(211)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                              lr=peak_lr,\n",
        "                              weight_decay=0.01)\n",
        "\n",
        "train_losses, val_losses, tokens_seen, track_lrs = train_model(model=model,\n",
        "                                                    train_loader=train_dataloader,\n",
        "                                                    val_loader=val_dataloader,\n",
        "                                                    optimizer=optimizer,\n",
        "                                                    device=device,\n",
        "                                                    n_epochs=num_epochs,\n",
        "                                                    eval_freq=5,\n",
        "                                                    eval_iter=5,\n",
        "                                                    start_context=\"In the midst of winter, I found\",\n",
        "                                                    tokenizer=bpe_tokenizer,\n",
        "                                                    warmup_steps=warmup_steps,\n",
        "                                                    initial_lr=1e-5,\n",
        "                                                    min_lr=1e-5)\n",
        "\n",
        "\n",
        "training_end_time = time.time()\n",
        "runtime_in_seconds = training_end_time - training_start_time\n",
        "# format as minutes and seconds\n",
        "minutes, seconds = divmod(runtime_in_seconds, 60)\n",
        "print(f\"device: {device}\")\n",
        "print(f\"training runtime: {int(minutes)} min {seconds:.2f} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SSvlBUx41TE",
        "outputId": "3e535271-2f5a-4c1b-cff7-e77365ab95be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Iter 000000): Train loss 10.927, Val loss 10.929\n",
            "Ep 1 (Iter 000005): Train loss 10.570, Val loss 10.579\n",
            "Ep 1 (Iter 000010): Train loss 9.989, Val loss 10.014\n",
            "Ep 1 (Iter 000015): Train loss 9.293, Val loss 9.318\n",
            "Ep 1 (Iter 000020): Train loss 8.760, Val loss 8.780\n",
            "Ep 1 (Iter 000025): Train loss 8.176, Val loss 8.187\n",
            "Ep 1 (Iter 000030): Train loss 7.582, Val loss 7.574\n",
            "Ep 1 (Iter 000035): Train loss 7.157, Val loss 7.091\n",
            "Ep 1 (Iter 000040): Train loss 6.782, Val loss 6.782\n",
            "Ep 1 (Iter 000045): Train loss 6.546, Val loss 6.593\n",
            "Ep 1 (Iter 000050): Train loss 6.412, Val loss 6.521\n",
            "Ep 1 (Iter 000055): Train loss 6.438, Val loss 6.497\n",
            "Ep 1 (Iter 000060): Train loss 6.538, Val loss 6.478\n",
            "Ep 1 (Iter 000065): Train loss 6.447, Val loss 6.456\n",
            "Ep 1 (Iter 000070): Train loss 6.377, Val loss 6.436\n",
            "Ep 1 (Iter 000075): Train loss 6.247, Val loss 6.411\n",
            "Ep 1 (Iter 000080): Train loss 6.314, Val loss 6.371\n",
            "Ep 1 (Iter 000085): Train loss 6.354, Val loss 6.337\n",
            "Ep 1 (Iter 000090): Train loss 6.194, Val loss 6.301\n",
            "In the midst of winter, I found        ”   “”    ” “”     ““    “The,”  �\n",
            "Ep 2 (Iter 000095): Train loss 6.175, Val loss 6.278\n",
            "Ep 2 (Iter 000100): Train loss 6.200, Val loss 6.260\n",
            "Ep 2 (Iter 000105): Train loss 6.135, Val loss 6.240\n",
            "Ep 2 (Iter 000110): Train loss 6.163, Val loss 6.219\n",
            "Ep 2 (Iter 000115): Train loss 6.155, Val loss 6.189\n",
            "Ep 2 (Iter 000120): Train loss 6.087, Val loss 6.154\n",
            "Ep 2 (Iter 000125): Train loss 6.027, Val loss 6.114\n",
            "Ep 2 (Iter 000130): Train loss 6.048, Val loss 6.077\n",
            "Ep 2 (Iter 000135): Train loss 5.935, Val loss 6.043\n",
            "Ep 2 (Iter 000140): Train loss 5.963, Val loss 6.017\n",
            "Ep 2 (Iter 000145): Train loss 5.969, Val loss 5.996\n",
            "Ep 2 (Iter 000150): Train loss 5.937, Val loss 5.982\n",
            "Ep 2 (Iter 000155): Train loss 5.867, Val loss 5.964\n",
            "Ep 2 (Iter 000160): Train loss 5.815, Val loss 5.950\n",
            "Ep 2 (Iter 000165): Train loss 5.850, Val loss 5.947\n",
            "Ep 2 (Iter 000170): Train loss 5.749, Val loss 5.918\n",
            "Ep 2 (Iter 000175): Train loss 5.754, Val loss 5.889\n",
            "Ep 2 (Iter 000180): Train loss 5.751, Val loss 5.867\n",
            "Ep 2 (Iter 000185): Train loss 5.784, Val loss 5.836\n",
            "In the midst of winter, I found      and, and the          ” said, and the     ” he had been, and the  “I, and the   \n",
            "Ep 3 (Iter 000190): Train loss 5.682, Val loss 5.818\n",
            "Ep 3 (Iter 000195): Train loss 5.786, Val loss 5.823\n",
            "Ep 3 (Iter 000200): Train loss 5.722, Val loss 5.825\n",
            "Ep 3 (Iter 000205): Train loss 5.749, Val loss 5.822\n",
            "Ep 3 (Iter 000210): Train loss 5.741, Val loss 5.816\n",
            "Ep 3 (Iter 000215): Train loss 5.709, Val loss 5.810\n",
            "Ep 3 (Iter 000220): Train loss 5.733, Val loss 5.806\n",
            "Ep 3 (Iter 000225): Train loss 5.723, Val loss 5.802\n",
            "Ep 3 (Iter 000230): Train loss 5.639, Val loss 5.797\n",
            "Ep 3 (Iter 000235): Train loss 5.653, Val loss 5.792\n",
            "Ep 3 (Iter 000240): Train loss 5.823, Val loss 5.787\n",
            "Ep 3 (Iter 000245): Train loss 5.785, Val loss 5.783\n",
            "Ep 3 (Iter 000250): Train loss 5.755, Val loss 5.780\n",
            "Ep 3 (Iter 000255): Train loss 5.720, Val loss 5.778\n",
            "Ep 3 (Iter 000260): Train loss 5.717, Val loss 5.776\n",
            "Ep 3 (Iter 000265): Train loss 5.661, Val loss 5.773\n",
            "Ep 3 (Iter 000270): Train loss 5.759, Val loss 5.771\n",
            "Ep 3 (Iter 000275): Train loss 5.720, Val loss 5.769\n",
            "Ep 3 (Iter 000280): Train loss 5.678, Val loss 5.767\n",
            "In the midst of winter, I found     ’s, and the  “I,” said the ” “I, and the  ” said the    “I,” said the \n",
            "Ep 4 (Iter 000285): Train loss 5.630, Val loss 5.766\n",
            "Ep 4 (Iter 000290): Train loss 5.762, Val loss 5.765\n",
            "Ep 4 (Iter 000295): Train loss 5.670, Val loss 5.764\n",
            "Ep 4 (Iter 000300): Train loss 5.578, Val loss 5.762\n",
            "Ep 4 (Iter 000305): Train loss 5.625, Val loss 5.760\n",
            "Ep 4 (Iter 000310): Train loss 5.693, Val loss 5.758\n",
            "Ep 4 (Iter 000315): Train loss 5.639, Val loss 5.758\n",
            "Ep 4 (Iter 000320): Train loss 5.624, Val loss 5.757\n",
            "Ep 4 (Iter 000325): Train loss 5.614, Val loss 5.755\n",
            "Ep 4 (Iter 000330): Train loss 5.710, Val loss 5.752\n",
            "Ep 4 (Iter 000335): Train loss 5.621, Val loss 5.750\n",
            "Ep 4 (Iter 000340): Train loss 5.618, Val loss 5.750\n",
            "Ep 4 (Iter 000345): Train loss 5.688, Val loss 5.749\n",
            "Ep 4 (Iter 000350): Train loss 5.696, Val loss 5.747\n",
            "Ep 4 (Iter 000355): Train loss 5.653, Val loss 5.746\n",
            "Ep 4 (Iter 000360): Train loss 5.743, Val loss 5.746\n",
            "Ep 4 (Iter 000365): Train loss 5.655, Val loss 5.745\n",
            "Ep 4 (Iter 000370): Train loss 5.648, Val loss 5.743\n",
            "Ep 4 (Iter 000375): Train loss 5.613, Val loss 5.740\n",
            "In the midst of winter, I found     ” said the “I, and the “I” said,” said the   ”  “I,” ” ” said the \n",
            "Ep 5 (Iter 000380): Train loss 5.582, Val loss 5.739\n",
            "Ep 5 (Iter 000385): Train loss 5.573, Val loss 5.737\n",
            "Ep 5 (Iter 000390): Train loss 5.534, Val loss 5.737\n",
            "Ep 5 (Iter 000395): Train loss 5.612, Val loss 5.737\n",
            "Ep 5 (Iter 000400): Train loss 5.583, Val loss 5.736\n",
            "Ep 5 (Iter 000405): Train loss 5.602, Val loss 5.735\n",
            "Ep 5 (Iter 000410): Train loss 5.576, Val loss 5.733\n",
            "Ep 5 (Iter 000415): Train loss 5.552, Val loss 5.732\n",
            "Ep 5 (Iter 000420): Train loss 5.637, Val loss 5.733\n",
            "Ep 5 (Iter 000425): Train loss 5.610, Val loss 5.733\n",
            "Ep 5 (Iter 000430): Train loss 5.668, Val loss 5.731\n",
            "Ep 5 (Iter 000435): Train loss 5.604, Val loss 5.729\n",
            "Ep 5 (Iter 000440): Train loss 5.623, Val loss 5.727\n",
            "Ep 5 (Iter 000445): Train loss 5.542, Val loss 5.725\n",
            "Ep 5 (Iter 000450): Train loss 5.577, Val loss 5.722\n",
            "Ep 5 (Iter 000455): Train loss 5.565, Val loss 5.721\n",
            "Ep 5 (Iter 000460): Train loss 5.615, Val loss 5.720\n",
            "Ep 5 (Iter 000465): Train loss 5.719, Val loss 5.717\n",
            "In the midst of winter, I found     ’s, and the ” ”  ” said the ” said the   ” ” ”  “I” ” \n",
            "Ep 6 (Iter 000470): Train loss 5.644, Val loss 5.715\n",
            "Ep 6 (Iter 000475): Train loss 5.710, Val loss 5.714\n",
            "Ep 6 (Iter 000480): Train loss 5.590, Val loss 5.713\n",
            "Ep 6 (Iter 000485): Train loss 5.538, Val loss 5.711\n",
            "Ep 6 (Iter 000490): Train loss 5.619, Val loss 5.709\n",
            "Ep 6 (Iter 000495): Train loss 5.621, Val loss 5.708\n",
            "Ep 6 (Iter 000500): Train loss 5.633, Val loss 5.707\n",
            "Ep 6 (Iter 000505): Train loss 5.537, Val loss 5.706\n",
            "Ep 6 (Iter 000510): Train loss 5.483, Val loss 5.706\n",
            "Ep 6 (Iter 000515): Train loss 5.602, Val loss 5.704\n",
            "Ep 6 (Iter 000520): Train loss 5.632, Val loss 5.703\n",
            "Ep 6 (Iter 000525): Train loss 5.498, Val loss 5.701\n",
            "Ep 6 (Iter 000530): Train loss 5.629, Val loss 5.701\n",
            "Ep 6 (Iter 000535): Train loss 5.594, Val loss 5.700\n",
            "Ep 6 (Iter 000540): Train loss 5.421, Val loss 5.698\n",
            "Ep 6 (Iter 000545): Train loss 5.536, Val loss 5.697\n",
            "Ep 6 (Iter 000550): Train loss 5.545, Val loss 5.696\n",
            "Ep 6 (Iter 000555): Train loss 5.522, Val loss 5.696\n",
            "Ep 6 (Iter 000560): Train loss 5.554, Val loss 5.695\n",
            "In the midst of winter, I found     “I” said the “I. ” said the ” said the ” said the ” said the  ” said the ” ” \n",
            "Ep 7 (Iter 000565): Train loss 5.609, Val loss 5.693\n",
            "Ep 7 (Iter 000570): Train loss 5.496, Val loss 5.692\n",
            "Ep 7 (Iter 000575): Train loss 5.657, Val loss 5.690\n",
            "Ep 7 (Iter 000580): Train loss 5.624, Val loss 5.689\n",
            "Ep 7 (Iter 000585): Train loss 5.431, Val loss 5.687\n",
            "Ep 7 (Iter 000590): Train loss 5.544, Val loss 5.685\n",
            "Ep 7 (Iter 000595): Train loss 5.430, Val loss 5.684\n",
            "Ep 7 (Iter 000600): Train loss 5.551, Val loss 5.683\n",
            "Ep 7 (Iter 000605): Train loss 5.630, Val loss 5.683\n",
            "Ep 7 (Iter 000610): Train loss 5.625, Val loss 5.682\n",
            "Ep 7 (Iter 000615): Train loss 5.606, Val loss 5.682\n",
            "Ep 7 (Iter 000620): Train loss 5.530, Val loss 5.682\n",
            "Ep 7 (Iter 000625): Train loss 5.488, Val loss 5.681\n",
            "Ep 7 (Iter 000630): Train loss 5.525, Val loss 5.680\n",
            "Ep 7 (Iter 000635): Train loss 5.628, Val loss 5.678\n",
            "Ep 7 (Iter 000640): Train loss 5.577, Val loss 5.677\n",
            "Ep 7 (Iter 000645): Train loss 5.549, Val loss 5.676\n",
            "Ep 7 (Iter 000650): Train loss 5.517, Val loss 5.675\n",
            "Ep 7 (Iter 000655): Train loss 5.573, Val loss 5.673\n",
            "In the midst of winter, I found     “I” said the “I. “I” ” said the ” “I,” ” ” ” ”\n",
            "Ep 8 (Iter 000660): Train loss 5.564, Val loss 5.673\n",
            "Ep 8 (Iter 000665): Train loss 5.483, Val loss 5.673\n",
            "Ep 8 (Iter 000670): Train loss 5.509, Val loss 5.672\n",
            "Ep 8 (Iter 000675): Train loss 5.520, Val loss 5.671\n",
            "Ep 8 (Iter 000680): Train loss 5.626, Val loss 5.670\n",
            "Ep 8 (Iter 000685): Train loss 5.432, Val loss 5.669\n",
            "Ep 8 (Iter 000690): Train loss 5.560, Val loss 5.668\n",
            "Ep 8 (Iter 000695): Train loss 5.597, Val loss 5.667\n",
            "Ep 8 (Iter 000700): Train loss 5.633, Val loss 5.667\n",
            "Ep 8 (Iter 000705): Train loss 5.497, Val loss 5.666\n",
            "Ep 8 (Iter 000710): Train loss 5.467, Val loss 5.664\n",
            "Ep 8 (Iter 000715): Train loss 5.507, Val loss 5.662\n",
            "Ep 8 (Iter 000720): Train loss 5.543, Val loss 5.661\n",
            "Ep 8 (Iter 000725): Train loss 5.485, Val loss 5.660\n",
            "Ep 8 (Iter 000730): Train loss 5.486, Val loss 5.659\n",
            "Ep 8 (Iter 000735): Train loss 5.552, Val loss 5.659\n",
            "Ep 8 (Iter 000740): Train loss 5.584, Val loss 5.658\n",
            "Ep 8 (Iter 000745): Train loss 5.554, Val loss 5.656\n",
            "Ep 8 (Iter 000750): Train loss 5.536, Val loss 5.655\n",
            "In the midst of winter, I found   the Emperor, and the  the French, and the the    the hand, and the    the Emperor, and the      the Emperor, and the and the \n",
            "Ep 9 (Iter 000755): Train loss 5.555, Val loss 5.653\n",
            "Ep 9 (Iter 000760): Train loss 5.516, Val loss 5.652\n",
            "Ep 9 (Iter 000765): Train loss 5.515, Val loss 5.650\n",
            "Ep 9 (Iter 000770): Train loss 5.505, Val loss 5.649\n",
            "Ep 9 (Iter 000775): Train loss 5.415, Val loss 5.650\n",
            "Ep 9 (Iter 000780): Train loss 5.473, Val loss 5.650\n",
            "Ep 9 (Iter 000785): Train loss 5.523, Val loss 5.648\n",
            "Ep 9 (Iter 000790): Train loss 5.523, Val loss 5.646\n",
            "Ep 9 (Iter 000795): Train loss 5.546, Val loss 5.646\n",
            "Ep 9 (Iter 000800): Train loss 5.482, Val loss 5.646\n",
            "Ep 9 (Iter 000805): Train loss 5.535, Val loss 5.646\n",
            "Ep 9 (Iter 000810): Train loss 5.416, Val loss 5.645\n",
            "Ep 9 (Iter 000815): Train loss 5.599, Val loss 5.644\n",
            "Ep 9 (Iter 000820): Train loss 5.525, Val loss 5.643\n",
            "Ep 9 (Iter 000825): Train loss 5.411, Val loss 5.642\n",
            "Ep 9 (Iter 000830): Train loss 5.458, Val loss 5.642\n",
            "Ep 9 (Iter 000835): Train loss 5.408, Val loss 5.642\n",
            "Ep 9 (Iter 000840): Train loss 5.407, Val loss 5.641\n",
            "Ep 9 (Iter 000845): Train loss 5.556, Val loss 5.640\n",
            "In the midst of winter, I found   the Emperor, and the  the French, and the the    the hand, and the    the Emperor, and the      the Emperor, and the    \n",
            "Ep 10 (Iter 000850): Train loss 5.423, Val loss 5.641\n",
            "Ep 10 (Iter 000855): Train loss 5.484, Val loss 5.639\n",
            "Ep 10 (Iter 000860): Train loss 5.449, Val loss 5.637\n",
            "Ep 10 (Iter 000865): Train loss 5.436, Val loss 5.634\n",
            "Ep 10 (Iter 000870): Train loss 5.456, Val loss 5.633\n",
            "Ep 10 (Iter 000875): Train loss 5.354, Val loss 5.635\n",
            "Ep 10 (Iter 000880): Train loss 5.564, Val loss 5.635\n",
            "Ep 10 (Iter 000885): Train loss 5.402, Val loss 5.634\n",
            "Ep 10 (Iter 000890): Train loss 5.532, Val loss 5.632\n",
            "Ep 10 (Iter 000895): Train loss 5.376, Val loss 5.632\n",
            "Ep 10 (Iter 000900): Train loss 5.444, Val loss 5.633\n",
            "Ep 10 (Iter 000905): Train loss 5.400, Val loss 5.632\n",
            "Ep 10 (Iter 000910): Train loss 5.475, Val loss 5.631\n",
            "Ep 10 (Iter 000915): Train loss 5.433, Val loss 5.632\n",
            "Ep 10 (Iter 000920): Train loss 5.512, Val loss 5.631\n",
            "Ep 10 (Iter 000925): Train loss 5.456, Val loss 5.630\n",
            "Ep 10 (Iter 000930): Train loss 5.521, Val loss 5.628\n",
            "Ep 10 (Iter 000935): Train loss 5.471, Val loss 5.628\n",
            "In the midst of winter, I found  the count, and the   the French, and the the    the hand, and the    the Emperor, and the      the Emperor, and the and the \n",
            "device: cuda\n",
            "training runtime: 15 min 17.84 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(range(len(track_lrs)), track_lrs)\n",
        "plt.ylabel(\"Learning rate\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "n0vt3GT241Qp",
        "outputId": "1e168e30-6407-436f-ca41-321f31e3961b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's plot a loss graph:\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_losses(epoch_seen,\n",
        "                tokens_seen,\n",
        "                train_losses,\n",
        "                val_losses):\n",
        "  \"\"\"Plot training and validation loss in xkcd style.\"\"\"\n",
        "\n",
        "  fig, ax1 = plt.subplots(figsize=(5, 5))\n",
        "\n",
        "  # plot training and validation loss against epochs\n",
        "  ax1.plot(epoch_seen, train_losses, label=\"Training Loss\")\n",
        "  ax1.plot(epoch_seen, val_losses, linestyle=\"-.\", label=\"Validation Loss\")\n",
        "  ax1.set_xlabel(\"Epoch\")\n",
        "  ax1.set_ylabel(\"Loss\")\n",
        "  ax1.legend(loc=\"upper right\")\n",
        "  ax1.xaxis.set_major_locator(MaxNLocator(integer=True)) # only show integer labels on x-axis\n",
        "\n",
        "  # create a second x-axis for token seen\n",
        "  ax2 = ax1.twiny() # create a second x-axis that shares the same y-axis\n",
        "  ax2.plot(tokens_seen, train_losses, alpha=0) # invisible plot for aligning ticks\n",
        "  ax2.set_xlabel(\"Tokens Seen\")\n",
        "\n",
        "  fig.tight_layout() # asjust layput to make room\n",
        "  plt.savefig(\"demos/gpt2/stable_training_with_lora_loss_plot.pdf\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "a-BZPtvP41OV",
        "outputId": "98a6bf7a-2a24-4558-a070-84db6fcfedc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(\"cpu\")\n",
        "model.eval()\n",
        "\n",
        "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(model=model,\n",
        "                                 input_batch=text_to_token_ids(\"In the midst of winter, I found\",\n",
        "                                                               bpe_tokenizer),\n",
        "                                 max_new_tokens=20,\n",
        "                                 context_size=BASE_CONFIG[\"context_length\"]\n",
        "                                 )\n",
        "\n",
        "\n",
        "print(\"output text: \\n\", token_ids_to_text(token_ids, bpe_tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnI6_wfS41MG",
        "outputId": "716a7aaa-c594-4d8e-950c-5fbde4873414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output text: \n",
            " In the midst of winter, I found\n",
            "\n",
            "the count, and the\n",
            "\n",
            "\n",
            "the French, and the\n",
            "the\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model state_dict\n",
        "torch.save(model.state_dict(), \"demos/gpt2/stable_training_with_lora_gpt2.pth\")"
      ],
      "metadata": {
        "id": "QUoMoe_S41KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model state_dict\n",
        "model = GPT2Model(BASE_CONFIG)\n",
        "replace_linear_with_lora(model, rank=16, alpha=16)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.load_state_dict(torch.load(\"demos/gpt2/stable_training_with_lora_gpt2.pth\", map_location=device, weights_only=True))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfR9y_zb41He",
        "outputId": "3fcb89f8-7b91-455f-e855-2f7028a6b681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing out_head with LinearLayerWithLoRA\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Model(\n",
              "  (token_emb): Embedding(50257, 768)\n",
              "  (position_emb): Embedding(1024, 768)\n",
              "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
              "  (transformer_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): LinearLayerWithLoRA(\n",
              "    (linear): Linear(in_features=768, out_features=50257, bias=False)\n",
              "    (lora): LoRALayer()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.4 creating `openai_weights.pth`"
      ],
      "metadata": {
        "id": "RbZY2GE-6rbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Clean up GPU memory ---\n",
        "import gc\n",
        "# del train_loss, val_loss  # if you don't need them anymore\n",
        "#del train_dataloader, val_dataloader  # optional, if not reused\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()"
      ],
      "metadata": {
        "id": "hmCLnAda41E5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "4038c6b0-5da4-4ac5-9794-4dbf8231d566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-1835142963.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipc_collect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mipc_collect\u001b[0;34m()\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0mtensors\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwant\u001b[0m \u001b[0mto\u001b[0m \u001b[0mrelease\u001b[0m \u001b[0munused\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m     \"\"\"\n\u001b[0;32m--> 997\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_ipc_collect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "print(\"TensorFlow version:\", version(\"tensorflow\"))\n",
        "print(\"tqdm version:\", version(\"tqdm\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boIOp0Vx7NqE",
        "outputId": "87a3f952-9e95-4055-cb19-8c2ba4e8bf8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "tqdm version: 4.67.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/\"\n",
        "    \"LLMs-from-scratch/main/ch05/\"\n",
        "    \"01_main-chapter-code/gpt_download.py\"\n",
        ")\n",
        "filename = url.split(\"/\")[-1]\n",
        "urllib.request.urlretrieve(url, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pRQiHrI7Nnl",
        "outputId": "e6f42610-c652-4d90-a762-15b41c83536b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gpt_download.py', <http.client.HTTPMessage at 0x7fbd33761c10>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_download import download_and_load_gpt2\n",
        "\n",
        "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvWbnsxt7Nlb",
        "outputId": "3ea2f2c7-bfcf-4091-d31f-f6e2845b499f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 115kiB/s]\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 3.40MiB/s]\n",
            "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 122kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [00:27<00:00, 18.0MiB/s]\n",
            "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 8.11MiB/s]\n",
            "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 2.35MiB/s]\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 2.24MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Settings:\", settings)\n",
        "print(\"Parameter dictionary keys:\", params.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbRCD2vv7NjA",
        "outputId": "3be521ba-49c8-43ba-cc96-ab608077c22e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
            "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_CONFIG"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXQK8anj8LXi",
        "outputId": "72c1d231-1d87-4636-f19d-8b07f1d8c7c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'vocab_size': 50257,\n",
              " 'context_length': 1024,\n",
              " 'drop_rate': 0.0,\n",
              " 'qkv_bias': True,\n",
              " 'emb_dim': 768,\n",
              " 'n_layers': 12,\n",
              " 'n_heads': 12}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model configurations in a dictionary for compactness\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "# Copy the base configuration and update with specific model settings\n",
        "model_name = \"gpt2-small (124M)\"  # Example model name\n",
        "NEW_CONFIG = BASE_CONFIG.copy()\n",
        "NEW_CONFIG.update(model_configs[model_name])\n",
        "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
        "\n",
        "gpt = GPT2Model(NEW_CONFIG)\n",
        "gpt.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sdEGG2F7Ngn",
        "outputId": "8da36167-506b-48b2-de79-16e206d8bc9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Model(\n",
              "  (token_emb): Embedding(50257, 768)\n",
              "  (position_emb): Embedding(1024, 768)\n",
              "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
              "  (transformer_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.position_emb.weight = assign(gpt.position_emb.weight, params['wpe'])\n",
        "    gpt.token_emb.weight = assign(gpt.token_emb.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.transformer_blocks[b].attention.W_query.weight = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_query.weight, q_w.T)\n",
        "        gpt.transformer_blocks[b].attention.W_key.weight = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_key.weight, k_w.T)\n",
        "        gpt.transformer_blocks[b].attention.W_value.weight = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.transformer_blocks[b].attention.W_query.bias = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_query.bias, q_b)\n",
        "        gpt.transformer_blocks[b].attention.W_key.bias = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_key.bias, k_b)\n",
        "        gpt.transformer_blocks[b].attention.W_value.bias = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_value.bias, v_b)\n",
        "\n",
        "        gpt.transformer_blocks[b].attention.output_projection.weight = assign(\n",
        "            gpt.transformer_blocks[b].attention.output_projection.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.transformer_blocks[b].attention.output_projection.bias = assign(\n",
        "            gpt.transformer_blocks[b].attention.output_projection.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.transformer_blocks[b].feed_forward.layers[0].weight = assign(\n",
        "            gpt.transformer_blocks[b].feed_forward.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.transformer_blocks[b].feed_forward.layers[0].bias = assign(\n",
        "            gpt.transformer_blocks[b].feed_forward.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.transformer_blocks[b].feed_forward.layers[2].weight = assign(\n",
        "            gpt.transformer_blocks[b].feed_forward.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.transformer_blocks[b].feed_forward.layers[2].bias = assign(\n",
        "            gpt.transformer_blocks[b].feed_forward.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.transformer_blocks[b].layer_norm1.scale = assign(\n",
        "            gpt.transformer_blocks[b].layer_norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.transformer_blocks[b].layer_norm1.shift = assign(\n",
        "            gpt.transformer_blocks[b].layer_norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.transformer_blocks[b].layer_norm2.scale = assign(\n",
        "            gpt.transformer_blocks[b].layer_norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.transformer_blocks[b].layer_norm2.shift = assign(\n",
        "            gpt.transformer_blocks[b].layer_norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
      ],
      "metadata": {
        "id": "mwnrg20T7Nb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "H5h8nOKejlKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_weights_into_gpt(gpt, params)\n",
        "gpt.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dfqhzy3Ssijy",
        "outputId": "b9af4555-c663-4b78-f7fa-213c4eb2ffa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Model(\n",
              "  (token_emb): Embedding(50257, 768)\n",
              "  (position_emb): Embedding(1024, 768)\n",
              "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
              "  (transformer_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(211)\n",
        "\n",
        "token_ids = generate_text(\n",
        "    model=gpt,\n",
        "    input_batch=text_to_token_ids(\"In the midst of winter, I found\", bpe_tokenizer).to(device),\n",
        "    max_new_tokens=100,\n",
        "    context_size=NEW_CONFIG[\"context_length\"],\n",
        "    top_k=50,\n",
        "    temperature=1.5\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, bpe_tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7GIXO2Q7NZr",
        "outputId": "24108d3e-7b78-4a34-d115-f7538668505f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " In the midst of winter, I found that this morning an Englishman of English descent arrived on his return home at his flat, about six in the morning.\n",
            "\n",
            "When suddenly my friend saw him in the garden by the door, he hurried after him, and went by him while I ran after him. Once or twice at length I heard from him that I heard a voice. The first one took up and whispered,—\"Wenkleben and Knoll?\" That night I was unable until nightstand for her or supper after\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model state_dict\n",
        "torch.save(gpt.state_dict(), \"demos/gpt2/openai_weight_gpt2.pth\")"
      ],
      "metadata": {
        "id": "CyyxlJYe7NXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model state_dict\n",
        "model = GPT2Model(BASE_CONFIG)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.load_state_dict(torch.load(\"demos/gpt2/openai_weight_gpt2.pth\", map_location=device, weights_only=True))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9O8O_nG7NVB",
        "outputId": "23b97c2b-611f-414d-9533-39b126b74f12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Model(\n",
              "  (token_emb): Embedding(50257, 768)\n",
              "  (position_emb): Embedding(1024, 768)\n",
              "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
              "  (transformer_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.5 Creating `instruction_finetune_gpt2.pth`"
      ],
      "metadata": {
        "id": "TKkowLepQwnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "8EloJxNPBFbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Clean up GPU memory ---\n",
        "import gc\n",
        "# del train_loss, val_loss  # if you don't need them anymore\n",
        "#del train_dataloader, val_dataloader  # optional, if not reused\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()"
      ],
      "metadata": {
        "id": "QitLWioG7NSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.position_emb.weight = assign(gpt.position_emb.weight, params['wpe'])\n",
        "    gpt.token_emb.weight = assign(gpt.token_emb.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.transformer_blocks[b].attention.W_query.weight = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_query.weight, q_w.T)\n",
        "        gpt.transformer_blocks[b].attention.W_key.weight = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_key.weight, k_w.T)\n",
        "        gpt.transformer_blocks[b].attention.W_value.weight = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.transformer_blocks[b].attention.W_query.bias = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_query.bias, q_b)\n",
        "        gpt.transformer_blocks[b].attention.W_key.bias = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_key.bias, k_b)\n",
        "        gpt.transformer_blocks[b].attention.W_value.bias = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_value.bias, v_b)\n",
        "\n",
        "        gpt.transformer_blocks[b].attention.output_projection.weight = assign(\n",
        "            gpt.transformer_blocks[b].attention.output_projection.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.transformer_blocks[b].attention.output_projection.bias = assign(\n",
        "            gpt.transformer_blocks[b].attention.output_projection.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.transformer_blocks[b].feed_forward.layers[0].weight = assign(\n",
        "            gpt.transformer_blocks[b].feed_forward.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.transformer_blocks[b].feed_forward.layers[0].bias = assign(\n",
        "            gpt.transformer_blocks[b].feed_forward.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.transformer_blocks[b].feed_forward.layers[2].weight = assign(\n",
        "            gpt.transformer_blocks[b].feed_forward.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.transformer_blocks[b].feed_forward.layers[2].bias = assign(\n",
        "            gpt.transformer_blocks[b].feed_forward.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.transformer_blocks[b].layer_norm1.scale = assign(\n",
        "            gpt.transformer_blocks[b].layer_norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.transformer_blocks[b].layer_norm1.shift = assign(\n",
        "            gpt.transformer_blocks[b].layer_norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.transformer_blocks[b].layer_norm2.scale = assign(\n",
        "            gpt.transformer_blocks[b].layer_norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.transformer_blocks[b].layer_norm2.shift = assign(\n",
        "            gpt.transformer_blocks[b].layer_norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
      ],
      "metadata": {
        "id": "AXKyYBLuKG6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import urllib\n",
        "\n",
        "\n",
        "def download_and_load_file(file_path, url):\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            text_data = response.read().decode(\"utf-8\")\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(text_data)\n",
        "\n",
        "    # The book originally contained this unnecessary \"else\" clause:\n",
        "    #else:\n",
        "    #    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    #        text_data = file.read()\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "file_path = \"instruction-data.json\"\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
        "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
        ")\n",
        "\n",
        "data = download_and_load_file(file_path, url)\n",
        "print(\"Number of entries:\", len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCCiklGU41Cj",
        "outputId": "5861be0d-a1e4-4399-a3d7-d3631d87f8fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of entries: 1100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Example entry:\\n\", data[999])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQB5LgMRBM3h",
        "outputId": "235c2665-6e2e-446e-d1a5-248cf4d953df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example entry:\n",
            " {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_input(entry):\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "    )\n",
        "\n",
        "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "\n",
        "    return instruction_text + input_text"
      ],
      "metadata": {
        "id": "D2RLMYYV41AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_input = format_input(data[999])\n",
        "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
        "\n",
        "print(model_input + desired_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVIUhUSFBX_9",
        "outputId": "d4ecabe0-22ba-4589-be7d-e88f9bec4013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is an antonym of 'complicated'?\n",
            "\n",
            "### Response:\n",
            "The correct spelling is 'Occasion.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_portion = int(len(data) * 0.85)  # 85% for training\n",
        "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
        "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
        "\n",
        "train_data = data[:train_portion]\n",
        "test_data = data[train_portion:train_portion + test_portion]\n",
        "val_data = data[train_portion + test_portion:]\n",
        "\n",
        "print(\"Training set length:\", len(train_data))\n",
        "print(\"Validation set length:\", len(val_data))\n",
        "print(\"Test set length:\", len(test_data))"
      ],
      "metadata": {
        "id": "l2nYtWBzWZqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd0f14ac-f696-4ba9-8445-ab0755c55b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set length: 935\n",
            "Validation set length: 55\n",
            "Test set length: 110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "  def __init__(self, data, tokenizer):\n",
        "    self.data = data\n",
        "\n",
        "    # pre-tokenizer texts\n",
        "    self.encoded_texts = []\n",
        "    for entry in data:\n",
        "      instruction_plus_input = format_input(entry)\n",
        "      response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "      full_text = instruction_plus_input + response_text\n",
        "      self.encoded_texts.append(tokenizer.encode(full_text))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.encoded_texts[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ],
      "metadata": {
        "id": "5-Qbl6mnRLDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-535mlTRLAy",
        "outputId": "f987edf6-d9be-4a8e-ae99-461172850cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50256]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch,\n",
        "                      pad_token_id=50256,\n",
        "                      ignore_index=-100,\n",
        "                      allowed_max_length=None,\n",
        "                      device=\"cpu\"):\n",
        "  # find the longest sequence in the batch and increase max_length\n",
        "  # by +1 for the padding token, which indicates the end of sequence/answer\n",
        "  batch_max_length = max(len(item) + 1 for item in batch)\n",
        "\n",
        "  # pad and prepare inputs\n",
        "  inputs_lst = []\n",
        "  targets_lst = []\n",
        "\n",
        "  for item in batch:\n",
        "    new_item = item.copy()\n",
        "    # Add an <|endoftext|> token\n",
        "    new_item += [pad_token_id]\n",
        "    # Pad sequences to batch_max_length\n",
        "    padded = (new_item + [pad_token_id] * (batch_max_length - len(new_item)))\n",
        "\n",
        "    inputs = torch.tensor(padded[:-1]) # Truncate the last token for inputs\n",
        "    targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "\n",
        "    # replace all but the first padding tokens in targets by `ignore_index`\n",
        "    mask = targets == pad_token_id\n",
        "    padding_tokens_indices = torch.nonzero(mask).squeeze() # indices of the padding tokens in `targets`\n",
        "    if padding_tokens_indices.numel() > 1:\n",
        "      targets[padding_tokens_indices[1:]] = ignore_index\n",
        "\n",
        "    # optionally truncate to maximum sequence length\n",
        "    if allowed_max_length is not None:\n",
        "      inputs = inputs[:allowed_max_length]\n",
        "      targets = targets[:allowed_max_length]\n",
        "\n",
        "    inputs_lst.append(inputs)\n",
        "    targets_lst.append(targets)\n",
        "\n",
        "  # convert list of inputs to tensor and transfer to target device\n",
        "  inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "  targets_tensor = torch.stack(targets_lst).to(device)\n",
        "\n",
        "  return inputs_tensor, targets_tensor"
      ],
      "metadata": {
        "id": "cYuZnoz_RK-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "63I-vuOLRK7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "customized_collate_fn = partial(\n",
        "    custom_collate_fn,\n",
        "    device=device,\n",
        "    allowed_max_length=1024\n",
        ")"
      ],
      "metadata": {
        "id": "nyb4s79ERK5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(211)\n",
        "\n",
        "train_dataset = InstructionDataset(train_data, tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "val_dataset = InstructionDataset(val_data, tokenizer)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "test_dataset = InstructionDataset(test_data, tokenizer)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")"
      ],
      "metadata": {
        "id": "5QfiwvpfRK2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train loader:\")\n",
        "for inputs, targets in train_loader:\n",
        "    print(inputs.shape, targets.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bo6JqkMeB59Y",
        "outputId": "729ee3e7-07eb-4e24-f741-1218cb839ffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 57]) torch.Size([8, 57])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 58]) torch.Size([8, 58])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 57]) torch.Size([8, 57])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 79]) torch.Size([8, 79])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 55]) torch.Size([8, 55])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CHOOSE_MODEL = \"gpt2-large (774M)\"\n",
        "INPUT_PROMPT = \"Every effort moves\"\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"drop_rate\": 0.0,        # Dropout rate\n",
        "    \"qkv_bias\": True         # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "BASE_CONFIG"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhtguLr-RK0d",
        "outputId": "eb971978-c855-4ba4-c4f7-690881882b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'vocab_size': 50257,\n",
              " 'context_length': 1024,\n",
              " 'drop_rate': 0.0,\n",
              " 'qkv_bias': True,\n",
              " 'emb_dim': 1280,\n",
              " 'n_layers': 36,\n",
              " 'n_heads': 20}"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/\"\n",
        "    \"LLMs-from-scratch/main/ch05/\"\n",
        "    \"01_main-chapter-code/gpt_download.py\"\n",
        ")\n",
        "filename = url.split(\"/\")[-1]\n",
        "urllib.request.urlretrieve(url, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66XUNSyTRKyB",
        "outputId": "f0912d34-0772-4854-abae-740deffc1df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gpt_download.py', <http.client.HTTPMessage at 0x78b4787652d0>)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "model_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "E-XCW5pQRKv2",
        "outputId": "0b8c7ab7-7729-4294-99c5-33a91dfded63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'774M'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_download import download_and_load_gpt2\n",
        "\n",
        "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
        "print(settings)\n",
        "print(params.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhhDXtdvRKtf",
        "outputId": "0006c8e9-739a-430a-bbe2-6dbcd92d4928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 155kiB/s]\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 5.04MiB/s]\n",
            "hparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 194kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 3.10G/3.10G [02:11<00:00, 23.6MiB/s]\n",
            "model.ckpt.index: 100%|██████████| 15.5k/15.5k [00:00<00:00, 25.5MiB/s]\n",
            "model.ckpt.meta: 100%|██████████| 1.38M/1.38M [00:00<00:00, 6.18MiB/s]\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 2.54MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 1280, 'n_head': 20, 'n_layer': 36}\n",
            "dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2Model(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval();"
      ],
      "metadata": {
        "id": "Ulw09q8ERKrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_1 = \"In the midst of winter\"\n",
        "\n",
        "token_ids = generate_text(\n",
        "    model=model,\n",
        "    input_batch=text_to_token_ids(text_1, tokenizer),\n",
        "    max_new_tokens=50,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        "    temperature = 2.0,\n",
        "    top_k = 10\n",
        ")\n",
        "\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6bv4taaRKpE",
        "outputId": "35d6f8cb-28c7-49ce-ba8c-4bf85bf924a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the midst of winter in Europe and in the Ural Mountains we are witnessing a major ice sheet advance,\" he said in Moscow last week. \"We are talking of a major event with an ice thickness approaching three kilometres. It means the end for this region and also the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(211)\n",
        "\n",
        "input_text = format_input(val_data[50])\n",
        "print(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGYkzi9mRKmk",
        "outputId": "8be96a36-bf37-4dd9-96d3-be49bd323c7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Label whether the following statements are true or false.\n",
            "\n",
            "### Input:\n",
            "The moon is a planet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = generate_text(\n",
        "    model=model,\n",
        "    input_batch=text_to_token_ids(input_text, tokenizer),\n",
        "    max_new_tokens=50,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        "    eos_id=50256,\n",
        ")\n",
        "generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "\n",
        "\n",
        "response_text = (\n",
        "    generated_text[len(input_text):]\n",
        "    .replace(\"### Response:\", \"\")\n",
        "    .strip()\n",
        ")\n",
        "print(response_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12xTenTbRKi0",
        "outputId": "9dfee980-d3f7-4b4a-cd8b-e936157021cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Output:\n",
            "\n",
            "The moon is a planet.\n",
            "\n",
            "### Label:\n",
            "\n",
            "The moon is a planet.\n",
            "\n",
            "### Input:\n",
            "\n",
            "The moon is a planet.\n",
            "\n",
            "### Output:\n",
            "\n",
            "The moon is a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "\n",
        "torch.manual_seed(211)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sR1gAjXFRKgD",
        "outputId": "79c48b3c-704d-423b-a20d-06cbec9ac1ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 3.6910677433013914\n",
            "Validation loss: 3.614051675796509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters before: {total_params:,}\")\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters after: {total_params:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe7948c7-001a-4566-9a00-6c0b73880749",
        "id": "dqdDo97gS1Zc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total trainable parameters before: 838,359,040\n",
            "Total trainable parameters after: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "replace_linear_with_lora(model, rank=16, alpha=16)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable LoRA parameters: {total_params:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a73c742f-faf7-45b5-b5fe-e1051aa2dd2a",
        "id": "piQXn7huS1Zf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing out_head with LinearLayerWithLoRA\n",
            "Total trainable LoRA parameters: 14,095,632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3f9491e-3cf2-4229-d4ee-4c02150e6faf",
        "id": "P-uxzJ4CS1Zg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2Model(\n",
            "  (token_emb): Embedding(50257, 1280)\n",
            "  (position_emb): Embedding(1024, 1280)\n",
            "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
            "  (transformer_blocks): Sequential(\n",
            "    (0): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (1): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (2): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (3): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (4): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (5): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (6): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (7): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (8): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (9): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (10): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (11): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (12): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (13): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (14): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (15): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (16): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (17): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (18): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (19): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (20): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (21): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (22): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (23): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (24): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (25): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (26): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (27): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (28): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (29): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (30): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (31): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (32): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (33): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (34): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (35): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_key): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (W_value): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (output_projection): LinearLayerWithLoRA(\n",
            "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (lora): LoRALayer()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "          (1): GELU()\n",
            "          (2): LinearLayerWithLoRA(\n",
            "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "            (lora): LoRALayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_norm): LayerNorm()\n",
            "  (out_head): LinearLayerWithLoRA(\n",
            "    (linear): Linear(in_features=1280, out_features=50257, bias=False)\n",
            "    (lora): LoRALayer()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "initial_lr = 0.0001\n",
        "peak_lr = 3e-5\n",
        "\n",
        "\n",
        "total_steps = len(train_loader) * num_epochs\n",
        "warmup_steps = int(0.2 * total_steps) # 20% warmup\n",
        "print(warmup_steps)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McE6XU_bbrM7",
        "outputId": "54fe6548-e439-4543-f091-ce2d55fc1f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "training_start_time = time.time()\n",
        "\n",
        "torch.manual_seed(211)\n",
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                              lr=peak_lr,\n",
        "                              weight_decay=0.01)\n",
        "#num_epochs = 5\n",
        "\n",
        "\n",
        "# train_losses, val_losses, tokens_seen = train_model_simple(model,\n",
        "#                                                            train_loader,\n",
        "#                                                            val_loader,\n",
        "#                                                            optimizer,\n",
        "#                                                            device,\n",
        "#                                                            num_epochs=num_epochs,\n",
        "#                                                            eval_freq=5,\n",
        "#                                                            eval_iter=5,\n",
        "#                                                            start_context=format_input(val_data[50]),\n",
        "#                                                            tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_losses, val_losses, tokens_seen, track_lrs = train_model(model=model,\n",
        "                                                    train_loader=train_loader,\n",
        "                                                    val_loader=val_loader,\n",
        "                                                    optimizer=optimizer,\n",
        "                                                    device=device,\n",
        "                                                    n_epochs=num_epochs,\n",
        "                                                    eval_freq=5,\n",
        "                                                    eval_iter=5,\n",
        "                                                    start_context=format_input(val_data[50]),\n",
        "                                                    tokenizer=tokenizer,\n",
        "                                                    warmup_steps=warmup_steps,\n",
        "                                                    initial_lr=1e-5,\n",
        "                                                    min_lr=1e-5)\n",
        "\n",
        "\n",
        "training_end_time = time.time()\n",
        "runtime_in_seconds = training_end_time - training_start_time\n",
        "# format as minutes and seconds\n",
        "minutes, seconds = divmod(runtime_in_seconds, 60)\n",
        "print(f\"device: {device}\")\n",
        "print(f\"training runtime: {int(minutes)} min {seconds:.2f} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2160d835-e59b-41f1-8273-9919d9d9d87b",
        "id": "jhBBfNBxS1Zh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Iter 000000): Train loss 2.805, Val loss 2.752\n",
            "Ep 1 (Iter 000005): Train loss 1.360, Val loss 1.326\n",
            "Ep 1 (Iter 000010): Train loss 0.936, Val loss 0.971\n",
            "Ep 1 (Iter 000015): Train loss 0.794, Val loss 0.891\n",
            "Ep 1 (Iter 000020): Train loss 0.806, Val loss 0.842\n",
            "Ep 1 (Iter 000025): Train loss 0.693, Val loss 0.829\n",
            "Ep 1 (Iter 000030): Train loss 0.749, Val loss 0.809\n",
            "Ep 1 (Iter 000035): Train loss 0.638, Val loss 0.777\n",
            "Ep 1 (Iter 000040): Train loss 0.703, Val loss 0.767\n",
            "Ep 1 (Iter 000045): Train loss 0.604, Val loss 0.741\n",
            "Ep 1 (Iter 000050): Train loss 0.708, Val loss 0.736\n",
            "Ep 1 (Iter 000055): Train loss 0.587, Val loss 0.726\n",
            "Ep 1 (Iter 000060): Train loss 0.628, Val loss 0.717\n",
            "Ep 1 (Iter 000065): Train loss 0.586, Val loss 0.718\n",
            "Ep 1 (Iter 000070): Train loss 0.681, Val loss 0.712\n",
            "Ep 1 (Iter 000075): Train loss 0.545, Val loss 0.695\n",
            "Ep 1 (Iter 000080): Train loss 0.579, Val loss 0.680\n",
            "Ep 1 (Iter 000085): Train loss 0.480, Val loss 0.666\n",
            "Ep 1 (Iter 000090): Train loss 0.501, Val loss 0.666\n",
            "Ep 1 (Iter 000095): Train loss 0.562, Val loss 0.656\n",
            "Ep 1 (Iter 000100): Train loss 0.534, Val loss 0.649\n",
            "Ep 1 (Iter 000105): Train loss 0.488, Val loss 0.644\n",
            "Ep 1 (Iter 000110): Train loss 0.511, Val loss 0.639\n",
            "Ep 1 (Iter 000115): Train loss 0.543, Val loss 0.645\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Label whether the following statements are true or false.  ### Input: The moon is a planet.  ### Response: The statement \"The moon is a planet\" is false.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the boiling point of\n",
            "Ep 2 (Iter 000120): Train loss 0.447, Val loss 0.634\n",
            "Ep 2 (Iter 000125): Train loss 0.492, Val loss 0.631\n",
            "Ep 2 (Iter 000130): Train loss 0.415, Val loss 0.628\n",
            "Ep 2 (Iter 000135): Train loss 0.428, Val loss 0.635\n",
            "Ep 2 (Iter 000140): Train loss 0.434, Val loss 0.629\n",
            "Ep 2 (Iter 000145): Train loss 0.391, Val loss 0.618\n",
            "Ep 2 (Iter 000150): Train loss 0.431, Val loss 0.624\n",
            "Ep 2 (Iter 000155): Train loss 0.383, Val loss 0.622\n",
            "Ep 2 (Iter 000160): Train loss 0.481, Val loss 0.621\n",
            "Ep 2 (Iter 000165): Train loss 0.392, Val loss 0.603\n",
            "Ep 2 (Iter 000170): Train loss 0.413, Val loss 0.613\n",
            "Ep 2 (Iter 000175): Train loss 0.427, Val loss 0.608\n",
            "Ep 2 (Iter 000180): Train loss 0.372, Val loss 0.601\n",
            "Ep 2 (Iter 000185): Train loss 0.405, Val loss 0.599\n",
            "Ep 2 (Iter 000190): Train loss 0.403, Val loss 0.609\n",
            "Ep 2 (Iter 000195): Train loss 0.385, Val loss 0.607\n",
            "Ep 2 (Iter 000200): Train loss 0.385, Val loss 0.607\n",
            "Ep 2 (Iter 000205): Train loss 0.335, Val loss 0.605\n",
            "Ep 2 (Iter 000210): Train loss 0.318, Val loss 0.618\n",
            "Ep 2 (Iter 000215): Train loss 0.372, Val loss 0.613\n",
            "Ep 2 (Iter 000220): Train loss 0.359, Val loss 0.616\n",
            "Ep 2 (Iter 000225): Train loss 0.344, Val loss 0.615\n",
            "Ep 2 (Iter 000230): Train loss 0.339, Val loss 0.617\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Label whether the following statements are true or false.  ### Input: The moon is a planet.  ### Response: The moon is a planet.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Input: The sun is shining.  ### Response: The\n",
            "Ep 3 (Iter 000235): Train loss 0.330, Val loss 0.627\n",
            "Ep 3 (Iter 000240): Train loss 0.333, Val loss 0.641\n",
            "Ep 3 (Iter 000245): Train loss 0.343, Val loss 0.650\n",
            "Ep 3 (Iter 000250): Train loss 0.340, Val loss 0.653\n",
            "Ep 3 (Iter 000255): Train loss 0.327, Val loss 0.652\n",
            "Ep 3 (Iter 000260): Train loss 0.347, Val loss 0.651\n",
            "Ep 3 (Iter 000265): Train loss 0.334, Val loss 0.649\n",
            "Ep 3 (Iter 000270): Train loss 0.291, Val loss 0.646\n",
            "Ep 3 (Iter 000275): Train loss 0.306, Val loss 0.643\n",
            "Ep 3 (Iter 000280): Train loss 0.342, Val loss 0.638\n",
            "Ep 3 (Iter 000285): Train loss 0.281, Val loss 0.636\n",
            "Ep 3 (Iter 000290): Train loss 0.278, Val loss 0.634\n",
            "Ep 3 (Iter 000295): Train loss 0.291, Val loss 0.633\n",
            "Ep 3 (Iter 000300): Train loss 0.286, Val loss 0.632\n",
            "Ep 3 (Iter 000305): Train loss 0.269, Val loss 0.632\n",
            "Ep 3 (Iter 000310): Train loss 0.260, Val loss 0.632\n",
            "Ep 3 (Iter 000315): Train loss 0.293, Val loss 0.632\n",
            "Ep 3 (Iter 000320): Train loss 0.265, Val loss 0.632\n",
            "Ep 3 (Iter 000325): Train loss 0.265, Val loss 0.631\n",
            "Ep 3 (Iter 000330): Train loss 0.272, Val loss 0.630\n",
            "Ep 3 (Iter 000335): Train loss 0.279, Val loss 0.629\n",
            "Ep 3 (Iter 000340): Train loss 0.268, Val loss 0.627\n",
            "Ep 3 (Iter 000345): Train loss 0.263, Val loss 0.625\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Label whether the following statements are true or false.  ### Input: The moon is a planet.  ### Response: The statement 'The moon is a planet' is false.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Input: What is the opposite of '\n",
            "Ep 4 (Iter 000350): Train loss 0.233, Val loss 0.623\n",
            "Ep 4 (Iter 000355): Train loss 0.260, Val loss 0.624\n",
            "Ep 4 (Iter 000360): Train loss 0.275, Val loss 0.625\n",
            "Ep 4 (Iter 000365): Train loss 0.232, Val loss 0.626\n",
            "Ep 4 (Iter 000370): Train loss 0.234, Val loss 0.628\n",
            "Ep 4 (Iter 000375): Train loss 0.256, Val loss 0.629\n",
            "Ep 4 (Iter 000380): Train loss 0.264, Val loss 0.630\n",
            "Ep 4 (Iter 000385): Train loss 0.241, Val loss 0.631\n",
            "Ep 4 (Iter 000390): Train loss 0.228, Val loss 0.631\n",
            "Ep 4 (Iter 000395): Train loss 0.234, Val loss 0.631\n",
            "Ep 4 (Iter 000400): Train loss 0.237, Val loss 0.631\n",
            "Ep 4 (Iter 000405): Train loss 0.239, Val loss 0.631\n",
            "Ep 4 (Iter 000410): Train loss 0.224, Val loss 0.631\n",
            "Ep 4 (Iter 000415): Train loss 0.231, Val loss 0.630\n",
            "Ep 4 (Iter 000420): Train loss 0.213, Val loss 0.629\n",
            "Ep 4 (Iter 000425): Train loss 0.214, Val loss 0.629\n",
            "Ep 4 (Iter 000430): Train loss 0.228, Val loss 0.628\n",
            "Ep 4 (Iter 000435): Train loss 0.204, Val loss 0.627\n",
            "Ep 4 (Iter 000440): Train loss 0.229, Val loss 0.627\n",
            "Ep 4 (Iter 000445): Train loss 0.227, Val loss 0.626\n",
            "Ep 4 (Iter 000450): Train loss 0.248, Val loss 0.626\n",
            "Ep 4 (Iter 000455): Train loss 0.200, Val loss 0.624\n",
            "Ep 4 (Iter 000460): Train loss 0.209, Val loss 0.624\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Label whether the following statements are true or false.  ### Input: The moon is a planet.  ### Response: The statement 'The moon is a planet' is false.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Input: What is the capital of the\n",
            "Ep 5 (Iter 000465): Train loss 0.216, Val loss 0.623\n",
            "Ep 5 (Iter 000470): Train loss 0.212, Val loss 0.625\n",
            "Ep 5 (Iter 000475): Train loss 0.216, Val loss 0.628\n",
            "Ep 5 (Iter 000480): Train loss 0.195, Val loss 0.631\n",
            "Ep 5 (Iter 000485): Train loss 0.191, Val loss 0.635\n",
            "Ep 5 (Iter 000490): Train loss 0.206, Val loss 0.636\n",
            "Ep 5 (Iter 000495): Train loss 0.189, Val loss 0.637\n",
            "Ep 5 (Iter 000500): Train loss 0.192, Val loss 0.637\n",
            "Ep 5 (Iter 000505): Train loss 0.217, Val loss 0.637\n",
            "Ep 5 (Iter 000510): Train loss 0.212, Val loss 0.638\n",
            "Ep 5 (Iter 000515): Train loss 0.215, Val loss 0.638\n",
            "Ep 5 (Iter 000520): Train loss 0.217, Val loss 0.636\n",
            "Ep 5 (Iter 000525): Train loss 0.216, Val loss 0.635\n",
            "Ep 5 (Iter 000530): Train loss 0.184, Val loss 0.636\n",
            "Ep 5 (Iter 000535): Train loss 0.176, Val loss 0.636\n",
            "Ep 5 (Iter 000540): Train loss 0.187, Val loss 0.636\n",
            "Ep 5 (Iter 000545): Train loss 0.189, Val loss 0.636\n",
            "Ep 5 (Iter 000550): Train loss 0.198, Val loss 0.636\n",
            "Ep 5 (Iter 000555): Train loss 0.190, Val loss 0.636\n",
            "Ep 5 (Iter 000560): Train loss 0.200, Val loss 0.636\n",
            "Ep 5 (Iter 000565): Train loss 0.189, Val loss 0.635\n",
            "Ep 5 (Iter 000570): Train loss 0.198, Val loss 0.633\n",
            "Ep 5 (Iter 000575): Train loss 0.192, Val loss 0.632\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Label whether the following statements are true or false.  ### Input: The moon is a planet.  ### Response: The statement 'The moon is a planet' is false.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Input: What is the capital of the\n",
            "Ep 6 (Iter 000580): Train loss 0.191, Val loss 0.632\n",
            "Ep 6 (Iter 000585): Train loss 0.210, Val loss 0.635\n",
            "Ep 6 (Iter 000590): Train loss 0.183, Val loss 0.640\n",
            "Ep 6 (Iter 000595): Train loss 0.199, Val loss 0.645\n",
            "Ep 6 (Iter 000600): Train loss 0.192, Val loss 0.649\n",
            "Ep 6 (Iter 000605): Train loss 0.183, Val loss 0.652\n",
            "Ep 6 (Iter 000610): Train loss 0.192, Val loss 0.654\n",
            "Ep 6 (Iter 000615): Train loss 0.181, Val loss 0.655\n",
            "Ep 6 (Iter 000620): Train loss 0.192, Val loss 0.655\n",
            "Ep 6 (Iter 000625): Train loss 0.174, Val loss 0.654\n",
            "Ep 6 (Iter 000630): Train loss 0.171, Val loss 0.653\n",
            "Ep 6 (Iter 000635): Train loss 0.171, Val loss 0.652\n",
            "Ep 6 (Iter 000640): Train loss 0.179, Val loss 0.650\n",
            "Ep 6 (Iter 000645): Train loss 0.168, Val loss 0.649\n",
            "Ep 6 (Iter 000650): Train loss 0.175, Val loss 0.650\n",
            "Ep 6 (Iter 000655): Train loss 0.181, Val loss 0.652\n",
            "Ep 6 (Iter 000660): Train loss 0.177, Val loss 0.653\n",
            "Ep 6 (Iter 000665): Train loss 0.170, Val loss 0.653\n",
            "Ep 6 (Iter 000670): Train loss 0.174, Val loss 0.653\n",
            "Ep 6 (Iter 000675): Train loss 0.160, Val loss 0.654\n",
            "Ep 6 (Iter 000680): Train loss 0.171, Val loss 0.654\n",
            "Ep 6 (Iter 000685): Train loss 0.168, Val loss 0.653\n",
            "Ep 6 (Iter 000690): Train loss 0.171, Val loss 0.652\n",
            "Ep 6 (Iter 000695): Train loss 0.168, Val loss 0.651\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Label whether the following statements are true or false.  ### Input: The moon is a planet.  ### Response: The statement 'The moon is a planet' is false.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Input: What is the capital of the\n",
            "Ep 7 (Iter 000700): Train loss 0.160, Val loss 0.652\n",
            "Ep 7 (Iter 000705): Train loss 0.173, Val loss 0.655\n",
            "Ep 7 (Iter 000710): Train loss 0.165, Val loss 0.658\n",
            "Ep 7 (Iter 000715): Train loss 0.160, Val loss 0.660\n",
            "Ep 7 (Iter 000720): Train loss 0.178, Val loss 0.661\n",
            "Ep 7 (Iter 000725): Train loss 0.167, Val loss 0.662\n",
            "Ep 7 (Iter 000730): Train loss 0.161, Val loss 0.664\n",
            "Ep 7 (Iter 000735): Train loss 0.160, Val loss 0.664\n",
            "Ep 7 (Iter 000740): Train loss 0.167, Val loss 0.666\n",
            "Ep 7 (Iter 000745): Train loss 0.170, Val loss 0.667\n",
            "Ep 7 (Iter 000750): Train loss 0.161, Val loss 0.668\n",
            "Ep 7 (Iter 000755): Train loss 0.159, Val loss 0.669\n",
            "Ep 7 (Iter 000760): Train loss 0.171, Val loss 0.669\n",
            "Ep 7 (Iter 000765): Train loss 0.169, Val loss 0.669\n",
            "Ep 7 (Iter 000770): Train loss 0.160, Val loss 0.668\n",
            "Ep 7 (Iter 000775): Train loss 0.155, Val loss 0.668\n",
            "Ep 7 (Iter 000780): Train loss 0.169, Val loss 0.667\n",
            "Ep 7 (Iter 000785): Train loss 0.164, Val loss 0.668\n",
            "Ep 7 (Iter 000790): Train loss 0.156, Val loss 0.667\n",
            "Ep 7 (Iter 000795): Train loss 0.156, Val loss 0.667\n",
            "Ep 7 (Iter 000800): Train loss 0.149, Val loss 0.667\n",
            "Ep 7 (Iter 000805): Train loss 0.159, Val loss 0.666\n",
            "Ep 7 (Iter 000810): Train loss 0.156, Val loss 0.666\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Label whether the following statements are true or false.  ### Input: The moon is a planet.  ### Response: The statement 'The moon is a planet' is false.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Input: What is the capital of the\n",
            "Ep 8 (Iter 000815): Train loss 0.148, Val loss 0.665\n",
            "Ep 8 (Iter 000820): Train loss 0.156, Val loss 0.666\n",
            "Ep 8 (Iter 000825): Train loss 0.152, Val loss 0.667\n",
            "Ep 8 (Iter 000830): Train loss 0.158, Val loss 0.668\n",
            "Ep 8 (Iter 000835): Train loss 0.160, Val loss 0.670\n",
            "Ep 8 (Iter 000840): Train loss 0.160, Val loss 0.672\n",
            "Ep 8 (Iter 000845): Train loss 0.155, Val loss 0.674\n",
            "Ep 8 (Iter 000850): Train loss 0.161, Val loss 0.677\n",
            "Ep 8 (Iter 000855): Train loss 0.163, Val loss 0.678\n",
            "Ep 8 (Iter 000860): Train loss 0.165, Val loss 0.678\n",
            "Ep 8 (Iter 000865): Train loss 0.153, Val loss 0.677\n",
            "Ep 8 (Iter 000870): Train loss 0.160, Val loss 0.677\n",
            "Ep 8 (Iter 000875): Train loss 0.156, Val loss 0.678\n",
            "Ep 8 (Iter 000880): Train loss 0.152, Val loss 0.677\n",
            "Ep 8 (Iter 000885): Train loss 0.149, Val loss 0.676\n",
            "Ep 8 (Iter 000890): Train loss 0.148, Val loss 0.675\n",
            "Ep 8 (Iter 000895): Train loss 0.155, Val loss 0.675\n",
            "Ep 8 (Iter 000900): Train loss 0.151, Val loss 0.674\n",
            "Ep 8 (Iter 000905): Train loss 0.153, Val loss 0.674\n",
            "Ep 8 (Iter 000910): Train loss 0.155, Val loss 0.674\n",
            "Ep 8 (Iter 000915): Train loss 0.148, Val loss 0.675\n",
            "Ep 8 (Iter 000920): Train loss 0.149, Val loss 0.675\n",
            "Ep 8 (Iter 000925): Train loss 0.151, Val loss 0.675\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Label whether the following statements are true or false.  ### Input: The moon is a planet.  ### Response: The statement 'The moon is a planet' is false.<|endoftext|>The following blog post, unless otherwise noted, was written by a member of Gamasutra's community.  The thoughts and opinions expressed are those of\n",
            "Ep 9 (Iter 000930): Train loss 0.146, Val loss 0.675\n",
            "Ep 9 (Iter 000935): Train loss 0.153, Val loss 0.677\n",
            "Ep 9 (Iter 000940): Train loss 0.154, Val loss 0.679\n",
            "Ep 9 (Iter 000945): Train loss 0.155, Val loss 0.680\n",
            "Ep 9 (Iter 000950): Train loss 0.154, Val loss 0.681\n",
            "Ep 9 (Iter 000955): Train loss 0.148, Val loss 0.682\n",
            "Ep 9 (Iter 000960): Train loss 0.149, Val loss 0.684\n",
            "Ep 9 (Iter 000965): Train loss 0.149, Val loss 0.685\n",
            "Ep 9 (Iter 000970): Train loss 0.150, Val loss 0.687\n",
            "Ep 9 (Iter 000975): Train loss 0.153, Val loss 0.688\n",
            "Ep 9 (Iter 000980): Train loss 0.147, Val loss 0.688\n",
            "Ep 9 (Iter 000985): Train loss 0.157, Val loss 0.688\n",
            "Ep 9 (Iter 000990): Train loss 0.150, Val loss 0.688\n",
            "Ep 9 (Iter 000995): Train loss 0.149, Val loss 0.690\n",
            "Ep 9 (Iter 001000): Train loss 0.146, Val loss 0.689\n",
            "Ep 9 (Iter 001005): Train loss 0.147, Val loss 0.688\n",
            "Ep 9 (Iter 001010): Train loss 0.150, Val loss 0.688\n",
            "Ep 9 (Iter 001015): Train loss 0.148, Val loss 0.688\n",
            "Ep 9 (Iter 001020): Train loss 0.149, Val loss 0.687\n",
            "Ep 9 (Iter 001025): Train loss 0.144, Val loss 0.687\n",
            "Ep 9 (Iter 001030): Train loss 0.151, Val loss 0.686\n",
            "Ep 9 (Iter 001035): Train loss 0.137, Val loss 0.687\n",
            "Ep 9 (Iter 001040): Train loss 0.145, Val loss 0.686\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Label whether the following statements are true or false.  ### Input: The moon is a planet.  ### Response: The statement 'The moon is a planet' is false.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Input: What is the capital of the\n",
            "Ep 10 (Iter 001045): Train loss 0.148, Val loss 0.686\n",
            "Ep 10 (Iter 001050): Train loss 0.149, Val loss 0.686\n",
            "Ep 10 (Iter 001055): Train loss 0.141, Val loss 0.687\n",
            "Ep 10 (Iter 001060): Train loss 0.142, Val loss 0.688\n",
            "Ep 10 (Iter 001065): Train loss 0.150, Val loss 0.690\n",
            "Ep 10 (Iter 001070): Train loss 0.146, Val loss 0.693\n",
            "Ep 10 (Iter 001075): Train loss 0.146, Val loss 0.694\n",
            "Ep 10 (Iter 001080): Train loss 0.153, Val loss 0.696\n",
            "Ep 10 (Iter 001085): Train loss 0.140, Val loss 0.699\n",
            "Ep 10 (Iter 001090): Train loss 0.142, Val loss 0.701\n",
            "Ep 10 (Iter 001095): Train loss 0.144, Val loss 0.703\n",
            "Ep 10 (Iter 001100): Train loss 0.149, Val loss 0.703\n",
            "Ep 10 (Iter 001105): Train loss 0.142, Val loss 0.702\n",
            "Ep 10 (Iter 001110): Train loss 0.145, Val loss 0.700\n",
            "Ep 10 (Iter 001115): Train loss 0.148, Val loss 0.697\n",
            "Ep 10 (Iter 001120): Train loss 0.145, Val loss 0.696\n",
            "Ep 10 (Iter 001125): Train loss 0.147, Val loss 0.694\n",
            "Ep 10 (Iter 001130): Train loss 0.149, Val loss 0.693\n",
            "Ep 10 (Iter 001135): Train loss 0.136, Val loss 0.691\n",
            "Ep 10 (Iter 001140): Train loss 0.146, Val loss 0.690\n",
            "Ep 10 (Iter 001145): Train loss 0.144, Val loss 0.689\n",
            "Ep 10 (Iter 001150): Train loss 0.140, Val loss 0.688\n",
            "Ep 10 (Iter 001155): Train loss 0.143, Val loss 0.687\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Label whether the following statements are true or false.  ### Input: The moon is a planet.  ### Response: The statement 'The moon is a planet' is false.<|endoftext|>The following blog post, unless otherwise noted, was written by a member of Gamasutra's community.  The thoughts and opinions expressed are those of\n",
            "device: cuda\n",
            "training runtime: 8 min 17.30 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(range(len(track_lrs)), track_lrs)\n",
        "plt.ylabel(\"Learning rate\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "cd0a0126-bb91-4a64-a2df-310a1e5070f1",
        "id": "vpGyM7wSS1Zh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's plot a loss graph:\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_losses(epoch_seen,\n",
        "                tokens_seen,\n",
        "                train_losses,\n",
        "                val_losses):\n",
        "  \"\"\"Plot training and validation loss in xkcd style.\"\"\"\n",
        "\n",
        "  fig, ax1 = plt.subplots(figsize=(5, 5))\n",
        "\n",
        "  # plot training and validation loss against epochs\n",
        "  ax1.plot(epoch_seen, train_losses, label=\"Training Loss\")\n",
        "  ax1.plot(epoch_seen, val_losses, linestyle=\"-.\", label=\"Validation Loss\")\n",
        "  ax1.set_xlabel(\"Epoch\")\n",
        "  ax1.set_ylabel(\"Loss\")\n",
        "  ax1.legend(loc=\"upper right\")\n",
        "  ax1.xaxis.set_major_locator(MaxNLocator(integer=True)) # only show integer labels on x-axis\n",
        "\n",
        "  # create a second x-axis for token seen\n",
        "  ax2 = ax1.twiny() # create a second x-axis that shares the same y-axis\n",
        "  ax2.plot(tokens_seen, train_losses, alpha=0) # invisible plot for aligning ticks\n",
        "  ax2.set_xlabel(\"Tokens Seen\")\n",
        "\n",
        "  fig.tight_layout() # asjust layput to make room\n",
        "  plt.savefig(\"demos/gpt2/instruction_finetune_gpt2_plot.pdf\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "c7999ad7-6346-49c2-8f01-3334f1f3c45c",
        "id": "UT_o_FeVS1Zi"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(211)\n",
        "\n",
        "\n",
        "for entry in test_data[55:64]:\n",
        "\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "    token_ids = generate_text(\n",
        "        model=model,\n",
        "        input_batch=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    response_text = (\n",
        "        generated_text[len(input_text):]\n",
        "        .replace(\"### Response:\", \"\")\n",
        "        .strip()\n",
        ")\n",
        "\n",
        "    print(input_text)\n",
        "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
        "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
        "    print(\"-------------------------------------\")\n",
        "    print(\"-------------------------------------\")\n",
        "    print(\"-------------------------------------\")\n",
        "    print(\"-------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j_Tr4ccvGqb",
        "outputId": "3bce8e77-822e-45ff-8833-abfe5777d7aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Explain the primary function of the human heart.\n",
            "\n",
            "Correct response:\n",
            ">> The primary function of the human heart is to pump blood throughout the body, delivering oxygen and nutrients to tissues and removing carbon dioxide and other wastes.\n",
            "\n",
            "Model response:\n",
            ">> The primary function of the human heart is to pump blood around the body, supplying all the tissues with oxygen and nutrients. It also pumps blood away from the body and into the lungs, where it is used for respiration.\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Reword the following sentence to the future tense.\n",
            "\n",
            "### Input:\n",
            "He is reading a novel inspired by his grandmother.\n",
            "\n",
            "Correct response:\n",
            ">> He will be reading a novel inspired by his grandmother.\n",
            "\n",
            "Model response:\n",
            ">> He will be reading a novel inspired by his grandmother.\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert the given sentence into active voice.\n",
            "\n",
            "### Input:\n",
            "The law was passed by the government.\n",
            "\n",
            "Correct response:\n",
            ">> The government passed the law.\n",
            "\n",
            "Model response:\n",
            ">> The government passed the law.\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Create a sentence using the word 'inevitable'.\n",
            "\n",
            "Correct response:\n",
            ">> The confrontation was inevitable given the circumstances.\n",
            "\n",
            "Model response:\n",
            ">> The collapse of the company was inevitable due to poor management.\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Categorize the following sentence as either factual or opinion-based.\n",
            "\n",
            "### Input:\n",
            "Chocolate is the best dessert.\n",
            "\n",
            "Correct response:\n",
            ">> Opinion-based.\n",
            "\n",
            "Model response:\n",
            ">> The best dessert is chocolate.\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is an antonym of 'old'?\n",
            "\n",
            "Correct response:\n",
            ">> young.\n",
            "\n",
            "Model response:\n",
            ">> An antonym of 'old' is 'young'.\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Provide a synonym for 'hardworking'.\n",
            "\n",
            "Correct response:\n",
            ">> A synonym for 'hardworking' is 'diligent'.\n",
            "\n",
            "Model response:\n",
            ">> A synonym for 'hardworking' is 'thrifty'.\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the boiling point of sulfur in Celsius?\n",
            "\n",
            "Correct response:\n",
            ">> The boiling point of sulfur is 444.6 degrees Celsius.\n",
            "\n",
            "Model response:\n",
            ">> The boiling point of sulfur is 674 degrees Celsius.\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the plural form of 'child'?\n",
            "\n",
            "Correct response:\n",
            ">> The plural form of 'child' is 'children'.\n",
            "\n",
            "Model response:\n",
            ">> The plural form of 'child' is 'cousins'.\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Example entry:\\n\", test_data[50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJLbQWAINzYN",
        "outputId": "8c423bd8-c87d-42be-d1f0-6cc0044d48d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example entry:\n",
            " {'instruction': 'Edit the given text to ensure all plural nouns are spelled correctly.', 'input': 'The birds sings beautiful songs.', 'output': 'The birds sing beautiful songs.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# generate responses for whole dataset\n",
        "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
        "\n",
        "  input_text = format_input(entry)\n",
        "\n",
        "  token_ids = generate_text(\n",
        "      model=model,\n",
        "      input_batch=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "      max_new_tokens=256,\n",
        "      context_size=BASE_CONFIG[\"context_length\"],\n",
        "      eos_id=50256)\n",
        "  generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "  response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
        "\n",
        "  # add \"model_response\" field into `test_data`\n",
        "  test_data[i][\"model_response\"] = response_text\n",
        "\n",
        "\n",
        "# save `test_data` into .json file\n",
        "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
        "  json.dump(test_data, file, indent=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo564hjjNzVP",
        "outputId": "6a1c19ba-7fd5-402e-badb-7ae90ea3360e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 110/110 [02:33<00:00,  1.40s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_data[50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8liiiw9NzSY",
        "outputId": "cb3cd83b-88ca-4b4b-f7e2-e00e4023cf68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'instruction': 'Edit the given text to ensure all plural nouns are spelled correctly.', 'input': 'The birds sings beautiful songs.', 'output': 'The birds sing beautiful songs.', 'model_response': 'The birds sing beautiful songs.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAhbaAbJax8Z",
        "outputId": "53de8a4b-48a6-42a3-8686-422a30da3a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dotenv\n",
            "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
            "Collecting python-dotenv (from dotenv)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, dotenv\n",
            "Successfully installed dotenv-0.9.9 python-dotenv-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "pkgs = [\"openai\",  # OpenAI API\n",
        "        \"tqdm\",    # Progress bar\n",
        "        \"dotenv\"\n",
        "        ]\n",
        "\n",
        "for p in pkgs:\n",
        "    print(f\"{p} version: {version(p)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qs1SJJKiNzPw",
        "outputId": "5d127c08-6b52-4b15-90ae-0bc25212d349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openai version: 1.93.0\n",
            "tqdm version: 4.67.1\n",
            "dotenv version: 0.9.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
        "    print(\"API key looks good so far\")\n",
        "else:\n",
        "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
        "\n",
        "MODEL = \"gpt-4.1-mini\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1g5n7-c7aAXZ",
        "outputId": "2061ab91-c728-4db0-f790-02f0edd2e897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API key looks good so far\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_chatgpt(prompt, client, model=MODEL):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.0,\n",
        "        seed=123,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "prompt = \"Respond with 'hello world' if you got this message.\"\n",
        "run_chatgpt(prompt, client)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dhiFjEBxaAUc",
        "outputId": "5e516681-e24a-4e54-8d94-439f23be1273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello world'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for entry in test_data[55:64]:\n",
        "    prompt = (\n",
        "        f\"Given the input `{format_input(entry)}` \"\n",
        "        f\"and correct output `{entry['output']}`, \"\n",
        "        f\"score the model response `{entry['model_response']}`\"\n",
        "        f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
        "        f\"Respond with the integer number only.\"\n",
        "    )\n",
        "    print(\"\\nDataset response:\")\n",
        "    print(\">>\", entry['output'])\n",
        "    print(\"\\nModel response:\")\n",
        "    print(\">>\", entry[\"model_response\"])\n",
        "    print(\"\\nScore:\")\n",
        "    print(\">>\", run_chatgpt(prompt, client))\n",
        "    print(\"\\n-------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoQYlixtaARp",
        "outputId": "1d9934e5-3bc1-47bd-e2a0-7755338c801d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset response:\n",
            ">> The primary function of the human heart is to pump blood throughout the body, delivering oxygen and nutrients to tissues and removing carbon dioxide and other wastes.\n",
            "\n",
            "Model response:\n",
            ">> The primary function of the human heart is to pump blood around the body, supplying all the tissues with oxygen and nutrients. It also pumps blood away from the body and into the lungs, where it is used for respiration.\n",
            "\n",
            "Score:\n",
            ">> 85\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> He will be reading a novel inspired by his grandmother.\n",
            "\n",
            "Model response:\n",
            ">> He will be reading a novel inspired by his grandmother.\n",
            "\n",
            "Score:\n",
            ">> 100\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> The government passed the law.\n",
            "\n",
            "Model response:\n",
            ">> The government passed the law.\n",
            "\n",
            "Score:\n",
            ">> 100\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> The confrontation was inevitable given the circumstances.\n",
            "\n",
            "Model response:\n",
            ">> The collapse of the company was inevitable due to poor management.\n",
            "\n",
            "Score:\n",
            ">> 100\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> Opinion-based.\n",
            "\n",
            "Model response:\n",
            ">> The best dessert is chocolate.\n",
            "\n",
            "Score:\n",
            ">> 80\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> young.\n",
            "\n",
            "Model response:\n",
            ">> An antonym of 'old' is 'young'.\n",
            "\n",
            "Score:\n",
            ">> 100\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> A synonym for 'hardworking' is 'diligent'.\n",
            "\n",
            "Model response:\n",
            ">> A synonym for 'hardworking' is 'thrifty'.\n",
            "\n",
            "Score:\n",
            ">> 20\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> The boiling point of sulfur is 444.6 degrees Celsius.\n",
            "\n",
            "Model response:\n",
            ">> The boiling point of sulfur is 674 degrees Celsius.\n",
            "\n",
            "Score:\n",
            ">> 0\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> The plural form of 'child' is 'children'.\n",
            "\n",
            "Model response:\n",
            ">> The plural form of 'child' is 'cousins'.\n",
            "\n",
            "Score:\n",
            ">> 0\n",
            "\n",
            "-------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_model_scores(json_data, json_key, model=MODEL):\n",
        "    scores = []\n",
        "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
        "        prompt = (\n",
        "            f\"Given the input `{format_input(entry)}` \"\n",
        "            f\"and correct output `{entry['output']}`, \"\n",
        "            f\"score the model response `{entry[json_key]}`\"\n",
        "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
        "            f\"Respond with the integer number only.\"\n",
        "        )\n",
        "        score = run_chatgpt(prompt, client, model)\n",
        "        try:\n",
        "            scores.append(int(score))\n",
        "        except ValueError:\n",
        "            print(f\"Could not convert score: {score}\")\n",
        "            continue\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "scores = generate_model_scores(test_data, \"model_response\")\n",
        "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
        "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0JM_a4YNzM9",
        "outputId": "399be91d-07e9-48d4-fe3f-4422423fb56f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scoring entries: 100%|██████████| 110/110 [00:39<00:00,  2.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of scores: 110 of 110\n",
            "Average score: 47.45\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model state_dict\n",
        "torch.save(model.state_dict(), \"demos/gpt2/instruction_finetune_gpt2.pth\")"
      ],
      "metadata": {
        "id": "yqodyZdXS1Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model state_dict\n",
        "model = GPT2Model(BASE_CONFIG)\n",
        "replace_linear_with_lora(model, rank=16, alpha=16)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable LoRA parameters: {total_params:,}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.load_state_dict(torch.load(\"demos/gpt2/instruction_finetune_gpt2.pth\", map_location=device, weights_only=True))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3c64565-aa22-4d63-8fdd-a761d9810635",
        "id": "nQXcWsfpS1Zj"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing W_query with LinearLayerWithLoRA\n",
            "Replacing W_key with LinearLayerWithLoRA\n",
            "Replacing W_value with LinearLayerWithLoRA\n",
            "Replacing output_projection with LinearLayerWithLoRA\n",
            "Replacing 0 with LinearLayerWithLoRA\n",
            "Replacing 2 with LinearLayerWithLoRA\n",
            "Replacing out_head with LinearLayerWithLoRA\n",
            "Total trainable LoRA parameters: 852,454,672\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Model(\n",
              "  (token_emb): Embedding(50257, 1280)\n",
              "  (position_emb): Embedding(1024, 1280)\n",
              "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
              "  (transformer_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (12): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (13): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (14): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (15): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (16): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (17): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (18): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (19): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (20): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (21): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (22): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (23): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (24): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (25): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (26): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (27): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (28): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (29): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (30): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (31): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (32): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (33): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (34): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (35): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_key): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (W_value): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (output_projection): LinearLayerWithLoRA(\n",
              "          (linear): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (lora): LoRALayer()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "          (1): GELU()\n",
              "          (2): LinearLayerWithLoRA(\n",
              "            (linear): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "            (lora): LoRALayer()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): LinearLayerWithLoRA(\n",
              "    (linear): Linear(in_features=1280, out_features=50257, bias=False)\n",
              "    (lora): LoRALayer()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.6 creating `classification_finetune.pth`"
      ],
      "metadata": {
        "id": "4qXRbg97bt01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "HV7YELiWbzJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Clean up GPU memory ---\n",
        "import gc\n",
        "# del train_loss, val_loss  # if you don't need them anymore\n",
        "#del train_dataloader, val_dataloader  # optional, if not reused\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()"
      ],
      "metadata": {
        "id": "4T1pNqZobzJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.position_emb.weight = assign(gpt.position_emb.weight, params['wpe'])\n",
        "    gpt.token_emb.weight = assign(gpt.token_emb.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.transformer_blocks[b].attention.W_query.weight = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_query.weight, q_w.T)\n",
        "        gpt.transformer_blocks[b].attention.W_key.weight = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_key.weight, k_w.T)\n",
        "        gpt.transformer_blocks[b].attention.W_value.weight = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.transformer_blocks[b].attention.W_query.bias = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_query.bias, q_b)\n",
        "        gpt.transformer_blocks[b].attention.W_key.bias = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_key.bias, k_b)\n",
        "        gpt.transformer_blocks[b].attention.W_value.bias = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_value.bias, v_b)\n",
        "\n",
        "        gpt.transformer_blocks[b].attention.output_projection.weight = assign(\n",
        "            gpt.transformer_blocks[b].attention.output_projection.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.transformer_blocks[b].attention.output_projection.bias = assign(\n",
        "            gpt.transformer_blocks[b].attention.output_projection.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.transformer_blocks[b].feed_forward.layers[0].weight = assign(\n",
        "            gpt.transformer_blocks[b].feed_forward.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.transformer_blocks[b].feed_forward.layers[0].bias = assign(\n",
        "            gpt.transformer_blocks[b].feed_forward.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.transformer_blocks[b].feed_forward.layers[2].weight = assign(\n",
        "            gpt.transformer_blocks[b].feed_forward.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.transformer_blocks[b].feed_forward.layers[2].bias = assign(\n",
        "            gpt.transformer_blocks[b].feed_forward.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.transformer_blocks[b].layer_norm1.scale = assign(\n",
        "            gpt.transformer_blocks[b].layer_norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.transformer_blocks[b].layer_norm1.shift = assign(\n",
        "            gpt.transformer_blocks[b].layer_norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.transformer_blocks[b].layer_norm2.scale = assign(\n",
        "            gpt.transformer_blocks[b].layer_norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.transformer_blocks[b].layer_norm2.shift = assign(\n",
        "            gpt.transformer_blocks[b].layer_norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
      ],
      "metadata": {
        "id": "_4QklZBKbzJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "zip_path = \"sms_spam_collection.zip\"\n",
        "extracted_path = \"sms_spam_collection\"\n",
        "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
        "\n",
        "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
        "    if data_file_path.exists():\n",
        "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
        "        return\n",
        "\n",
        "    # Downloading the file\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        with open(zip_path, \"wb\") as out_file:\n",
        "            out_file.write(response.read())\n",
        "\n",
        "    # Unzipping the file\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extracted_path)\n",
        "\n",
        "    # Add .tsv file extension\n",
        "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
        "    os.rename(original_file_path, data_file_path)\n",
        "    print(f\"File downloaded and saved as {data_file_path}\")\n",
        "\n",
        "try:\n",
        "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
        "except (urllib.error.HTTPError, urllib.error.URLError, TimeoutError) as e:\n",
        "    print(f\"Primary URL failed: {e}. Trying backup URL...\")\n",
        "    url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/sms%2Bspam%2Bcollection.zip\"\n",
        "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydnaPF5OUL8Q",
        "outputId": "b86c2e4c-1d98-48f3-91a9-4b8c568b3df3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded and saved as sms_spam_collection/SMSSpamCollection.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "qv2_mxWvUL5R",
        "outputId": "98b2c498-f2a5-4344-fda4-b45db955c787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Label                                               Text\n",
              "0      ham  Go until jurong point, crazy.. Available only ...\n",
              "1      ham                      Ok lar... Joking wif u oni...\n",
              "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      ham  U dun say so early hor... U c already then say...\n",
              "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
              "...    ...                                                ...\n",
              "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
              "5568   ham               Will ü b going to esplanade fr home?\n",
              "5569   ham  Pity, * was in mood for that. So...any other s...\n",
              "5570   ham  The guy did some bitching but I acted like i'd...\n",
              "5571   ham                         Rofl. Its true to its name\n",
              "\n",
              "[5572 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-153f2e34-c85e-40b1-a860-662de7aebf3b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>spam</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>ham</td>\n",
              "      <td>Will ü b going to esplanade fr home?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>ham</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>ham</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>ham</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-153f2e34-c85e-40b1-a860-662de7aebf3b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-153f2e34-c85e-40b1-a860-662de7aebf3b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-153f2e34-c85e-40b1-a860-662de7aebf3b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d8824334-3c1c-41b6-8b15-b2981f672924\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d8824334-3c1c-41b6-8b15-b2981f672924')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d8824334-3c1c-41b6-8b15-b2981f672924 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_d1125a9b-99c1-465f-a075-38db7572766c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d1125a9b-99c1-465f-a075-38db7572766c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5572,\n  \"fields\": [\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5169,\n        \"samples\": [\n          \"K, makes sense, btw carlos is being difficult so you guys are gonna smoke while I go pick up the second batch and get gas\",\n          \"URGENT! Your mobile No *********** WON a \\u00a32,000 Bonus Caller Prize on 02/06/03! This is the 2nd attempt to reach YOU! Call 09066362220 ASAP! BOX97N7QP, 150ppm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"Label\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIqVTnNQUL2C",
        "outputId": "480dcb16-332d-40dd-9a34-ae9de1d3bc38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "ham     4825\n",
            "spam     747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_balanced_dataset(df):\n",
        "  # count the instances of `spam`\n",
        "  num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
        "\n",
        "  # randomly sub-sample \"ham\" with `num_spam` rows\n",
        "  ham_sub = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
        "\n",
        "  # concatenate subsets\n",
        "  balanced_df = pd.concat([ham_sub, df[df[\"Label\"] == \"spam\"]])\n",
        "\n",
        "  return balanced_df\n",
        "\n",
        "\n",
        "balanced_df = create_balanced_dataset(df)\n",
        "\n",
        "# show first 5 and last 5 rows\n",
        "print(balanced_df.iloc[np.r_[0:5, -5:0]])\n",
        "\n",
        "# check label/class distribution\n",
        "print(balanced_df[\"Label\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTgrcfrmULzI",
        "outputId": "e6598b62-a0b0-4f1e-cd65-d9794803faaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Label                                               Text\n",
            "4307   ham  Awww dat is sweet! We can think of something t...\n",
            "4138   ham                             Just got to  &lt;#&gt;\n",
            "4831   ham  The word \"Checkmate\" in chess comes from the P...\n",
            "4461   ham  This is wishing you a great day. Moji told me ...\n",
            "5440   ham      Thank you. do you generally date the brothas?\n",
            "5537  spam  Want explicit SEX in 30 secs? Ring 02073162414...\n",
            "5540  spam  ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...\n",
            "5547  spam  Had your contract mobile 11 Mnths? Latest Moto...\n",
            "5566  spam  REMINDER FROM O2: To get 2.50 pounds free call...\n",
            "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
            "Label\n",
            "ham     747\n",
            "spam    747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
        "\n",
        "# show first 5 and last 5 rows\n",
        "print(balanced_df.iloc[np.r_[0:5, -5:0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGDg1WV_ULvw",
        "outputId": "09ab051d-8559-429e-d6a4-6e4b3a61d844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Label                                               Text\n",
            "4307      0  Awww dat is sweet! We can think of something t...\n",
            "4138      0                             Just got to  &lt;#&gt;\n",
            "4831      0  The word \"Checkmate\" in chess comes from the P...\n",
            "4461      0  This is wishing you a great day. Moji told me ...\n",
            "5440      0      Thank you. do you generally date the brothas?\n",
            "5537      1  Want explicit SEX in 30 secs? Ring 02073162414...\n",
            "5540      1  ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...\n",
            "5547      1  Had your contract mobile 11 Mnths? Latest Moto...\n",
            "5566      1  REMINDER FROM O2: To get 2.50 pounds free call...\n",
            "5567      1  This is the 2nd time we have tried 2 contact u...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def random_split(df, train_frac, validation_frac):\n",
        "\n",
        "  # shuffle dataset\n",
        "  df = df.sample(frac=1, random_state=211).reset_index(drop=True)\n",
        "\n",
        "  # compute split range\n",
        "  train_end = int(len(df) * train_frac)\n",
        "  validation_end = train_end + int(len(df) * validation_frac)\n",
        "\n",
        "  # split\n",
        "  train_df = df[:train_end]\n",
        "  validation_df = df[train_end:validation_end]\n",
        "  test_df = df[validation_end:]\n",
        "\n",
        "  return train_df, validation_df, test_df\n",
        "\n",
        "\n",
        "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
        "\n",
        "\n",
        "# save splitted dataframe into csv files\n",
        "train_df.to_csv(\"train.csv\", index=None)\n",
        "validation_df.to_csv(\"validation.csv\", index=None)\n",
        "test_df.to_csv(\"test.csv\", index=None)"
      ],
      "metadata": {
        "id": "7NEkiIldULss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7088583b-f8bc-41e5-87ce-c425116525be",
        "id": "NuWHUSc3bzJD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50256]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SpamDataset(Dataset):\n",
        "  def __init__(self,\n",
        "               csv_file,\n",
        "               tokenizer,\n",
        "               max_length=None,\n",
        "               pad_token_id=50256):\n",
        "    self.data = pd.read_csv(csv_file)\n",
        "\n",
        "    # pre-tokenizer texts\n",
        "    self.encoded_texts = [tokenizer.encode(text) for text in self.data[\"Text\"]]\n",
        "\n",
        "    if max_length is None:\n",
        "      self.max_length = self._longest_encoded_length()\n",
        "    else:\n",
        "      self.max_length = max_length\n",
        "      # truncate sequences if they're longer than max_length\n",
        "      self.encoded_texts = [encoded_text[:self.max_length] for encoded_text in self.encoded_texts]\n",
        "\n",
        "    # pad tokens\n",
        "    self.encoded_texts = [\n",
        "        encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
        "        for encoded_text in self.encoded_texts\n",
        "    ]\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    encoded_sequence = self.encoded_texts[index]\n",
        "    label = self.data.iloc[index][\"Label\"]\n",
        "    return (\n",
        "        torch.tensor(encoded_sequence, dtype=torch.long),\n",
        "        torch.tensor(label, dtype=torch.long)\n",
        "    )\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def _longest_encoded_length(self):\n",
        "    return max(len(encoded_text) for encoded_text in self.encoded_texts)"
      ],
      "metadata": {
        "id": "OfEXoYXwUg5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SpamDataset(\n",
        "    csv_file=\"train.csv\",\n",
        "    max_length=None,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(train_dataset.max_length)\n",
        "\n",
        "val_dataset = SpamDataset(\n",
        "    csv_file=\"validation.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "test_dataset = SpamDataset(\n",
        "    csv_file=\"test.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxuuwll2Ug2V",
        "outputId": "277fe0db-05a7-429f-8789-f591aef9365d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(211)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")"
      ],
      "metadata": {
        "id": "akCTJZrgUgzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train loader:\")\n",
        "for input_batch, target_batch in train_loader:\n",
        "    pass\n",
        "\n",
        "print(\"Input batch dimensions:\", input_batch.shape)\n",
        "print(\"Label batch dimensions\", target_batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su_Sn96LUgvy",
        "outputId": "6313ef96-bff0-4d52-df59-a7e4973faddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "Input batch dimensions: torch.Size([8, 120])\n",
            "Label batch dimensions torch.Size([8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{len(train_loader)} training batches\")\n",
        "print(f\"{len(val_loader)} validation batches\")\n",
        "print(f\"{len(test_loader)} test batches\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fl3X7iYUgs7",
        "outputId": "8f434400-b203-4a0e-d806-b4859afa3332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130 training batches\n",
            "19 validation batches\n",
            "38 test batches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
        "INPUT_PROMPT = \"Every effort moves\"\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"drop_rate\": 0.0,        # Dropout rate\n",
        "    \"qkv_bias\": True         # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "\n",
        "\n",
        "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
        "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
        "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
        "    f\"`max_length={BASE_CONFIG['context_length']}`\")\n",
        "\n",
        "\n",
        "BASE_CONFIG"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d4d756e-6d00-4938-a8cf-1ff4b26fbaad",
        "id": "-c2QbVVTbzJE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'vocab_size': 50257,\n",
              " 'context_length': 1024,\n",
              " 'drop_rate': 0.0,\n",
              " 'qkv_bias': True,\n",
              " 'emb_dim': 768,\n",
              " 'n_layers': 12,\n",
              " 'n_heads': 12}"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/\"\n",
        "    \"LLMs-from-scratch/main/ch05/\"\n",
        "    \"01_main-chapter-code/gpt_download.py\"\n",
        ")\n",
        "filename = url.split(\"/\")[-1]\n",
        "urllib.request.urlretrieve(url, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loHo5uACUgqB",
        "outputId": "306c0636-5cc8-4caf-c00e-31d0fc38bc31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gpt_download.py', <http.client.HTTPMessage at 0x78b532ce7690>)"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "9mIjFwNYbzJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "model_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "81e07b00-3092-43fd-f01c-72daa669a4ae",
        "id": "TxLhqXxIbzJE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'124M'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_download import download_and_load_gpt2\n",
        "\n",
        "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
        "print(settings)\n",
        "print(params.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2731d86e-7607-44d1-d64f-5c163d127e60",
        "id": "Z5CD4XrfbzJE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
            "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
            "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
            "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n",
            "{'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
            "dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2Model(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval();"
      ],
      "metadata": {
        "id": "BNk2RSFvbzJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_1 = \"In the midst of winter\"\n",
        "\n",
        "token_ids = generate_text(\n",
        "    model=model,\n",
        "    input_batch=text_to_token_ids(text_1, tokenizer),\n",
        "    max_new_tokens=50,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        "    temperature = 2.0,\n",
        "    top_k = 10\n",
        ")\n",
        "\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7300e5e5-539b-4cff-fb2f-363127197f19",
        "id": "1UVEc3ANbzJE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the midst of winter break I was sitting with two friends who I would call friends and would say 'I love you so so much. It's been really nice to see that you guys care.' It's been really nice for the rest of us.\" She's not exactly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_2 = (\n",
        "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
        "    \" 'You are a winner you have been specially\"\n",
        "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
        ")\n",
        "\n",
        "token_ids = generate_text(\n",
        "    model=model,\n",
        "    input_batch=text_to_token_ids(text_2, tokenizer),\n",
        "    max_new_tokens=23,\n",
        "    context_size=BASE_CONFIG[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fN3sUWUWMSw",
        "outputId": "2930b73c-6fab-4511-994f-8dac1ac70298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
            "\n",
            "The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC2dzslTXQyn",
        "outputId": "2555c62b-2dbb-41d9-e52b-b383a17a5a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2Model(\n",
            "  (token_emb): Embedding(50257, 768)\n",
            "  (position_emb): Embedding(1024, 768)\n",
            "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
            "  (transformer_blocks): Sequential(\n",
            "    (0): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (1): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (2): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (3): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (4): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (5): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (6): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (7): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (8): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (9): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (10): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (11): TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm1): LayerNorm()\n",
            "      (layer_norm2): LayerNorm()\n",
            "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_norm): LayerNorm()\n",
            "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "Oyvunz4KXQtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(211)\n",
        "\n",
        "num_classes = 2\n",
        "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)"
      ],
      "metadata": {
        "id": "Ej6EL3hiXQo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.transformer_blocks[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in model.final_norm.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "P_kiH83-XQim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo\n",
        "\n",
        "from torchinfo import summary\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=(1, BASE_CONFIG[\"context_length\"]),      # (batch, seq_len)\n",
        "    dtypes=[torch.long],                                # token IDs are int64\n",
        "    col_names=(\"input_size\", \"output_size\", \"num_params\", \"trainable\"),\n",
        "    row_settings=(\"depth\", \"var_names\"),                # valid row options\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4jZIJY0XbYL",
        "outputId": "77a3cf6a-ac01-421f-9eb4-91808eb57f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===========================================================================================================================================================\n",
              "Layer (type (var_name):depth-idx)                       Input Shape               Output Shape              Param #                   Trainable\n",
              "===========================================================================================================================================================\n",
              "GPT2Model (GPT2Model)                                   [1, 1024]                 [1, 1024, 2]              --                        Partial\n",
              "├─Embedding (token_emb): 1-1                            [1, 1024]                 [1, 1024, 768]            (38,597,376)              False\n",
              "├─Embedding (position_emb): 1-2                         [1024]                    [1024, 768]               (786,432)                 False\n",
              "├─Dropout (drop_emb): 1-3                               [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "├─Sequential (transformer_blocks): 1-4                  [1, 1024, 768]            [1, 1024, 768]            --                        Partial\n",
              "│    └─TransformerBlock (0): 2-1                        [1, 1024, 768]            [1, 1024, 768]            --                        False\n",
              "│    │    └─LayerNorm (layer_norm1): 3-1                [1, 1024, 768]            [1, 1024, 768]            (1,536)                   False\n",
              "│    │    └─MultiHeadAttention (attention): 3-2         [1, 1024, 768]            [1, 1024, 768]            (2,362,368)               False\n",
              "│    │    └─Dropout (drop_skip): 3-3                    [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "│    │    └─LayerNorm (layer_norm2): 3-4                [1, 1024, 768]            [1, 1024, 768]            (1,536)                   False\n",
              "│    │    └─FeedForward (feed_forward): 3-5             [1, 1024, 768]            [1, 1024, 768]            (4,722,432)               False\n",
              "│    │    └─Dropout (drop_skip): 3-6                    [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "│    └─TransformerBlock (1): 2-2                        [1, 1024, 768]            [1, 1024, 768]            --                        False\n",
              "│    │    └─LayerNorm (layer_norm1): 3-7                [1, 1024, 768]            [1, 1024, 768]            (1,536)                   False\n",
              "│    │    └─MultiHeadAttention (attention): 3-8         [1, 1024, 768]            [1, 1024, 768]            (2,362,368)               False\n",
              "│    │    └─Dropout (drop_skip): 3-9                    [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "│    │    └─LayerNorm (layer_norm2): 3-10               [1, 1024, 768]            [1, 1024, 768]            (1,536)                   False\n",
              "│    │    └─FeedForward (feed_forward): 3-11            [1, 1024, 768]            [1, 1024, 768]            (4,722,432)               False\n",
              "│    │    └─Dropout (drop_skip): 3-12                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "│    └─TransformerBlock (2): 2-3                        [1, 1024, 768]            [1, 1024, 768]            --                        False\n",
              "│    │    └─LayerNorm (layer_norm1): 3-13               [1, 1024, 768]            [1, 1024, 768]            (1,536)                   False\n",
              "│    │    └─MultiHeadAttention (attention): 3-14        [1, 1024, 768]            [1, 1024, 768]            (2,362,368)               False\n",
              "│    │    └─Dropout (drop_skip): 3-15                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "│    │    └─LayerNorm (layer_norm2): 3-16               [1, 1024, 768]            [1, 1024, 768]            (1,536)                   False\n",
              "│    │    └─FeedForward (feed_forward): 3-17            [1, 1024, 768]            [1, 1024, 768]            (4,722,432)               False\n",
              "│    │    └─Dropout (drop_skip): 3-18                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "│    └─TransformerBlock (3): 2-4                        [1, 1024, 768]            [1, 1024, 768]            --                        False\n",
              "│    │    └─LayerNorm (layer_norm1): 3-19               [1, 1024, 768]            [1, 1024, 768]            (1,536)                   False\n",
              "│    │    └─MultiHeadAttention (attention): 3-20        [1, 1024, 768]            [1, 1024, 768]            (2,362,368)               False\n",
              "│    │    └─Dropout (drop_skip): 3-21                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "│    │    └─LayerNorm (layer_norm2): 3-22               [1, 1024, 768]            [1, 1024, 768]            (1,536)                   False\n",
              "│    │    └─FeedForward (feed_forward): 3-23            [1, 1024, 768]            [1, 1024, 768]            (4,722,432)               False\n",
              "│    │    └─Dropout (drop_skip): 3-24                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "│    └─TransformerBlock (4): 2-5                        [1, 1024, 768]            [1, 1024, 768]            --                        False\n",
              "│    │    └─LayerNorm (layer_norm1): 3-25               [1, 1024, 768]            [1, 1024, 768]            (1,536)                   False\n",
              "│    │    └─MultiHeadAttention (attention): 3-26        [1, 1024, 768]            [1, 1024, 768]            (2,362,368)               False\n",
              "│    │    └─Dropout (drop_skip): 3-27                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "│    │    └─LayerNorm (layer_norm2): 3-28               [1, 1024, 768]            [1, 1024, 768]            (1,536)                   False\n",
              "│    │    └─FeedForward (feed_forward): 3-29            [1, 1024, 768]            [1, 1024, 768]            (4,722,432)               False\n",
              "│    │    └─Dropout (drop_skip): 3-30                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "│    └─TransformerBlock (5): 2-6                        [1, 1024, 768]            [1, 1024, 768]            --                        False\n",
              "│    │    └─LayerNorm (layer_norm1): 3-31               [1, 1024, 768]            [1, 1024, 768]            (1,536)                   False\n",
              "│    │    └─MultiHeadAttention (attention): 3-32        [1, 1024, 768]            [1, 1024, 768]            (2,362,368)               False\n",
              "│    │    └─Dropout (drop_skip): 3-33                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "│    │    └─LayerNorm (layer_norm2): 3-34               [1, 1024, 768]            [1, 1024, 768]            (1,536)                   False\n",
              "│    │    └─FeedForward (feed_forward): 3-35            [1, 1024, 768]            [1, 1024, 768]            (4,722,432)               False\n",
              "│    │    └─Dropout (drop_skip): 3-36                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "│    └─TransformerBlock (6): 2-7                        [1, 1024, 768]            [1, 1024, 768]            --                        False\n",
              "│    │    └─LayerNorm (layer_norm1): 3-37               [1, 1024, 768]            [1, 1024, 768]            (1,536)                   False\n",
              "│    │    └─MultiHeadAttention (attention): 3-38        [1, 1024, 768]            [1, 1024, 768]            (2,362,368)               False\n",
              "│    │    └─Dropout (drop_skip): 3-39                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "│    │    └─LayerNorm (layer_norm2): 3-40               [1, 1024, 768]            [1, 1024, 768]            (1,536)                   False\n",
              "│    │    └─FeedForward (feed_forward): 3-41            [1, 1024, 768]            [1, 1024, 768]            (4,722,432)               False\n",
              "│    │    └─Dropout (drop_skip): 3-42                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "│    └─TransformerBlock (7): 2-8                        [1, 1024, 768]            [1, 1024, 768]            --                        False\n",
              "│    │    └─LayerNorm (layer_norm1): 3-43               [1, 1024, 768]            [1, 1024, 768]            (1,536)                   False\n",
              "│    │    └─MultiHeadAttention (attention): 3-44        [1, 1024, 768]            [1, 1024, 768]            (2,362,368)               False\n",
              "│    │    └─Dropout (drop_skip): 3-45                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "│    │    └─LayerNorm (layer_norm2): 3-46               [1, 1024, 768]            [1, 1024, 768]            (1,536)                   False\n",
              "│    │    └─FeedForward (feed_forward): 3-47            [1, 1024, 768]            [1, 1024, 768]            (4,722,432)               False\n",
              "│    │    └─Dropout (drop_skip): 3-48                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "│    └─TransformerBlock (8): 2-9                        [1, 1024, 768]            [1, 1024, 768]            --                        False\n",
              "│    │    └─LayerNorm (layer_norm1): 3-49               [1, 1024, 768]            [1, 1024, 768]            (1,536)                   False\n",
              "│    │    └─MultiHeadAttention (attention): 3-50        [1, 1024, 768]            [1, 1024, 768]            (2,362,368)               False\n",
              "│    │    └─Dropout (drop_skip): 3-51                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "│    │    └─LayerNorm (layer_norm2): 3-52               [1, 1024, 768]            [1, 1024, 768]            (1,536)                   False\n",
              "│    │    └─FeedForward (feed_forward): 3-53            [1, 1024, 768]            [1, 1024, 768]            (4,722,432)               False\n",
              "│    │    └─Dropout (drop_skip): 3-54                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "│    └─TransformerBlock (9): 2-10                       [1, 1024, 768]            [1, 1024, 768]            --                        False\n",
              "│    │    └─LayerNorm (layer_norm1): 3-55               [1, 1024, 768]            [1, 1024, 768]            (1,536)                   False\n",
              "│    │    └─MultiHeadAttention (attention): 3-56        [1, 1024, 768]            [1, 1024, 768]            (2,362,368)               False\n",
              "│    │    └─Dropout (drop_skip): 3-57                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "│    │    └─LayerNorm (layer_norm2): 3-58               [1, 1024, 768]            [1, 1024, 768]            (1,536)                   False\n",
              "│    │    └─FeedForward (feed_forward): 3-59            [1, 1024, 768]            [1, 1024, 768]            (4,722,432)               False\n",
              "│    │    └─Dropout (drop_skip): 3-60                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "│    └─TransformerBlock (10): 2-11                      [1, 1024, 768]            [1, 1024, 768]            --                        False\n",
              "│    │    └─LayerNorm (layer_norm1): 3-61               [1, 1024, 768]            [1, 1024, 768]            (1,536)                   False\n",
              "│    │    └─MultiHeadAttention (attention): 3-62        [1, 1024, 768]            [1, 1024, 768]            (2,362,368)               False\n",
              "│    │    └─Dropout (drop_skip): 3-63                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "│    │    └─LayerNorm (layer_norm2): 3-64               [1, 1024, 768]            [1, 1024, 768]            (1,536)                   False\n",
              "│    │    └─FeedForward (feed_forward): 3-65            [1, 1024, 768]            [1, 1024, 768]            (4,722,432)               False\n",
              "│    │    └─Dropout (drop_skip): 3-66                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "│    └─TransformerBlock (11): 2-12                      [1, 1024, 768]            [1, 1024, 768]            --                        True\n",
              "│    │    └─LayerNorm (layer_norm1): 3-67               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
              "│    │    └─MultiHeadAttention (attention): 3-68        [1, 1024, 768]            [1, 1024, 768]            2,362,368                 True\n",
              "│    │    └─Dropout (drop_skip): 3-69                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "│    │    └─LayerNorm (layer_norm2): 3-70               [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
              "│    │    └─FeedForward (feed_forward): 3-71            [1, 1024, 768]            [1, 1024, 768]            4,722,432                 True\n",
              "│    │    └─Dropout (drop_skip): 3-72                   [1, 1024, 768]            [1, 1024, 768]            --                        --\n",
              "├─LayerNorm (final_norm): 1-5                           [1, 1024, 768]            [1, 1024, 768]            1,536                     True\n",
              "├─Linear (out_head): 1-6                                [1, 1024, 768]            [1, 1024, 2]              1,538                     True\n",
              "===========================================================================================================================================================\n",
              "Total params: 124,441,346\n",
              "Trainable params: 7,090,946\n",
              "Non-trainable params: 117,350,400\n",
              "Total mult-adds (Units.MEGABYTES): 928.92\n",
              "===========================================================================================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 849.36\n",
              "Params size (MB): 497.77\n",
              "Estimated Total Size (MB): 1347.14\n",
              "==========================================================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "inputs = tokenizer.encode(\"I open myself to\")\n",
        "inputs = torch.tensor(inputs, dtype=torch.long).unsqueeze(0).to(device)\n",
        "print(\"Inputs:\", inputs)\n",
        "print(\"Inputs dimensions:\", inputs.shape) # shape: (batch_size, num_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rim2zo8oXh_T",
        "outputId": "ca835d2a-5587-4760-dd95-4412d03b1f01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: tensor([[  40, 1280, 3589,  284]], device='cuda:0')\n",
            "Inputs dimensions: torch.Size([1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy_loader(data_loader,\n",
        "                         model,\n",
        "                         device,\n",
        "                         num_batches=None):\n",
        "  model.eval()\n",
        "  correct_predictions, num_examples = 0, 0\n",
        "\n",
        "  if num_batches is None:\n",
        "    num_batches = len(data_loader)\n",
        "  else:\n",
        "    num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "  for batch_id, (input_batch, label_batch) in enumerate(data_loader):\n",
        "    if batch_id < num_batches:\n",
        "      input_batch, label_batch = input_batch.to(device), label_batch.to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        logits = model(input_batch)[:, -1, :] # logits of the last output token\n",
        "      predicted_labels = torch.argmax(logits, dim=-1)\n",
        "\n",
        "      num_examples += predicted_labels.shape[0]\n",
        "      correct_predictions += (predicted_labels == label_batch).sum().item()\n",
        "    else:\n",
        "      break\n",
        "  return correct_predictions / num_examples"
      ],
      "metadata": {
        "id": "qq1Pl5aFWzgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(211)\n",
        "\n",
        "\n",
        "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
        "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
        "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
        "\n",
        "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Q3hT_EhWzY_",
        "outputId": "7426e10b-fd4a-44a9-d87e-bced22d61eb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy: 51.25%\n",
            "Validation accuracy: 58.75%\n",
            "Test accuracy: 37.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch,\n",
        "                    label_batch,\n",
        "                    model,\n",
        "                    device):\n",
        "  input_batch, label_batch = input_batch.to(device), label_batch.to(device)\n",
        "  logits = model(input_batch)[:, -1, :] # logits of the last output token\n",
        "  loss = torch.nn.functional.cross_entropy(logits, label_batch)\n",
        "  return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader,\n",
        "                     model,\n",
        "                     device,\n",
        "                     num_batches=None):\n",
        "  total_loss = 0.0\n",
        "  if len(data_loader) == 0:\n",
        "    return float(\"nan\")\n",
        "  elif num_batches is None:\n",
        "    num_batches = len(data_loader)\n",
        "  else:\n",
        "    num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "  for batch_id, (input_batch, label_batch) in enumerate(data_loader):\n",
        "    if batch_id < num_batches:\n",
        "      loss = calc_loss_batch(input_batch, label_batch, model, device)\n",
        "      total_loss += loss.item()\n",
        "    else:\n",
        "      break\n",
        "  return total_loss / num_batches"
      ],
      "metadata": {
        "id": "n2u2EQ_nWU6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "\n",
        "torch.manual_seed(211)\n",
        "\n",
        "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
        "    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
        "\n",
        "print(f\"Training loss: {train_loss:.3f}\")\n",
        "print(f\"Validation loss: {val_loss:.3f}\")\n",
        "print(f\"Test loss: {test_loss:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b04828c-de43-4821-daf6-83029f1e30b6",
        "id": "ka5G-VLzbzJF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 1.533\n",
            "Validation loss: 1.299\n",
            "Test loss: 1.851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "# print(f\"Total trainable parameters before: {total_params:,}\")\n",
        "\n",
        "# for param in model.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "# print(f\"Total trainable parameters after: {total_params:,}\")"
      ],
      "metadata": {
        "id": "yJWhBfLRbzJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace_linear_with_lora(model, rank=16, alpha=16)\n",
        "\n",
        "# total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "# print(f\"Total trainable LoRA parameters: {total_params:,}\")"
      ],
      "metadata": {
        "id": "n1bz7_MJbzJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model.to(device)\n",
        "\n",
        "# print(model)"
      ],
      "metadata": {
        "id": "Ks1oBtGnbzJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import math\n",
        "# ORIG_BOOK_VERSION = False\n",
        "\n",
        "\n",
        "# def train_classifier(model,\n",
        "#                     train_loader,\n",
        "#                     val_loader,\n",
        "#                     optimizer,\n",
        "#                     device,\n",
        "#                     num_epochs,\n",
        "#                     eval_freq,\n",
        "#                     eval_iter, # number of batches to evaluate from\n",
        "#                     warmup_steps,\n",
        "#                     initial_lr=3e-05,\n",
        "#                     min_lr=1e-6):\n",
        "#   train_losses, val_losses = [], []\n",
        "#   train_accuracies, val_accuracies = [], []\n",
        "#   track_lrs = []\n",
        "#   examples_seen, global_step = 0, -1\n",
        "\n",
        "#   # retrieve the maximum/peak learning rate from the optimizer\n",
        "#   peak_lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "#   # calculate the total number of iterations in the training process\n",
        "#   total_training_steps = len(train_loader) * num_epochs\n",
        "\n",
        "#   # calculate the learning rate increment during the warmup phase\n",
        "#   lr_increment = (peak_lr - initial_lr) / warmup_steps\n",
        "\n",
        "#   for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "\n",
        "#     for input_batch, label_batch in train_loader:\n",
        "#       optimizer.zero_grad()\n",
        "#       global_step += 1\n",
        "\n",
        "\n",
        "#       # adjust the learning rate based on the current phase (warmup or cosine)\n",
        "#       if global_step < warmup_steps:\n",
        "#         lr = initial_lr + global_step * lr_increment\n",
        "#       else:\n",
        "#         # cosine annealing after warmup\n",
        "#         progress = ((global_step - warmup_steps) /\n",
        "#                     (total_training_steps - warmup_steps))\n",
        "#         lr = (min_lr +\n",
        "#          (peak_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress)))\n",
        "\n",
        "#       # apply the calculated learning rate to the optimizer\n",
        "#       for param_group in optimizer.param_groups:\n",
        "#         param_group[\"lr\"] = lr\n",
        "#       track_lrs.append(lr) # store the current learning rate\n",
        "\n",
        "#       # calculate and backpropagate the loss\n",
        "#       loss = calc_loss_batch(input_batch, label_batch, model, device)\n",
        "#       loss.backward()\n",
        "\n",
        "#       # apply gradient clipping after the warmup phase to avoid exploding gradients\n",
        "#       if ORIG_BOOK_VERSION:\n",
        "#         if global_step > warmup_steps:\n",
        "#           torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "#       else:\n",
        "#         # the book originally used global_step > warmup_steps, which led to a skipped clipping step after warmup\n",
        "#         if global_step >= warmup_steps:\n",
        "#           torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "#       optimizer.step()\n",
        "#       examples_seen += input_batch.shape[0] # New: track examples instead of tokens\n",
        "\n",
        "#       # optional evaluation step\n",
        "#       if global_step % eval_freq == 0:\n",
        "#         train_loss, val_loss = evaluate_model(model,\n",
        "#                                               train_loader,\n",
        "#                                               val_loader,\n",
        "#                                               device,\n",
        "#                                               eval_iter)\n",
        "#         train_losses.append(train_loss)\n",
        "#         val_losses.append(val_loss)\n",
        "#         print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "#                       f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "#     # calculate accuracy after each epoch\n",
        "#     train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "#     val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "#     print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
        "#     print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "#     train_accuracies.append(train_accuracy)\n",
        "#     val_accuracies.append(val_accuracy)\n",
        "\n",
        "#   return train_losses, val_losses, train_accuracies, val_accuracies, examples_seen, track_lrs"
      ],
      "metadata": {
        "id": "RRMpEMyPbY2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# num_epochs = 5\n",
        "# initial_lr = 1e-6\n",
        "# peak_lr = 1.5e-5\n",
        "\n",
        "\n",
        "# total_steps = len(train_loader) * num_epochs\n",
        "# warmup_steps = int(0.3 * total_steps) # 20% warmup\n",
        "# print(warmup_steps)\n",
        "\n",
        "# # pick a (smaller) constant LR\n",
        "# fixed_lr      = 5e-6"
      ],
      "metadata": {
        "id": "uF5vyiYybzJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier_simple(model,\n",
        "                            train_loader,\n",
        "                            val_loader,\n",
        "                            optimizer,\n",
        "                            device,\n",
        "                            num_epochs,\n",
        "                            eval_freq,\n",
        "                            eval_iter # number of batches to evaluate from\n",
        "                            ):\n",
        "  train_losses, val_losses = [], []\n",
        "  train_accuracies, val_accuracies = [], []\n",
        "  examples_seen, global_step = 0, -1\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    for input_batch, label_batch in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss = calc_loss_batch(input_batch, label_batch, model, device)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      examples_seen += input_batch.shape[0] # New: track examples instead of tokens\n",
        "      global_step += 1\n",
        "\n",
        "\n",
        "      # optional evaluation step\n",
        "      if global_step % eval_freq == 0:\n",
        "        train_loss, val_loss = evaluate_model(model,\n",
        "                                              train_loader,\n",
        "                                              val_loader,\n",
        "                                              device,\n",
        "                                              eval_iter)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "    # calculate accuracy after each epoch\n",
        "    train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "    val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
        "    print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "  return train_losses, val_losses, train_accuracies, val_accuracies, examples_seen"
      ],
      "metadata": {
        "id": "KoG5xTFZgTM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "training_start_time = time.time()\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "torch.manual_seed(211)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 10\n",
        "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
        ")\n",
        "\n",
        "\n",
        "training_end_time = time.time()\n",
        "runtime_in_seconds = training_end_time - training_start_time\n",
        "# format as minutes and seconds\n",
        "minutes, seconds = divmod(runtime_in_seconds, 60)\n",
        "print(f\"device: {device}\")\n",
        "print(f\"training runtime: {int(minutes)} min {seconds:.2f} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df18d237-dbda-4f53-eec6-e8aac3eaa44a",
        "id": "ET8O112rbzJG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 1.530, Val loss 1.177\n",
            "Ep 1 (Step 000050): Train loss 0.637, Val loss 0.639\n",
            "Ep 1 (Step 000100): Train loss 0.522, Val loss 0.533\n",
            "Training accuracy: 80.00% | Validation accuracy: 87.50%\n",
            "Ep 2 (Step 000150): Train loss 0.412, Val loss 0.336\n",
            "Ep 2 (Step 000200): Train loss 0.224, Val loss 0.106\n",
            "Ep 2 (Step 000250): Train loss 0.028, Val loss 0.070\n",
            "Training accuracy: 97.50% | Validation accuracy: 100.00%\n",
            "Ep 3 (Step 000300): Train loss 0.124, Val loss 0.076\n",
            "Ep 3 (Step 000350): Train loss 0.124, Val loss 0.048\n",
            "Training accuracy: 97.50% | Validation accuracy: 100.00%\n",
            "Ep 4 (Step 000400): Train loss 0.075, Val loss 0.037\n",
            "Ep 4 (Step 000450): Train loss 0.194, Val loss 0.043\n",
            "Ep 4 (Step 000500): Train loss 0.171, Val loss 0.041\n",
            "Training accuracy: 97.50% | Validation accuracy: 97.50%\n",
            "Ep 5 (Step 000550): Train loss 0.045, Val loss 0.037\n",
            "Ep 5 (Step 000600): Train loss 0.021, Val loss 0.035\n",
            "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
            "Ep 6 (Step 000650): Train loss 0.022, Val loss 0.036\n",
            "Ep 6 (Step 000700): Train loss 0.053, Val loss 0.031\n",
            "Ep 6 (Step 000750): Train loss 0.026, Val loss 0.037\n",
            "Training accuracy: 97.50% | Validation accuracy: 97.50%\n",
            "Ep 7 (Step 000800): Train loss 0.217, Val loss 0.029\n",
            "Ep 7 (Step 000850): Train loss 0.005, Val loss 0.027\n",
            "Ep 7 (Step 000900): Train loss 0.123, Val loss 0.037\n",
            "Training accuracy: 100.00% | Validation accuracy: 100.00%\n",
            "Ep 8 (Step 000950): Train loss 0.010, Val loss 0.027\n",
            "Ep 8 (Step 001000): Train loss 0.025, Val loss 0.024\n",
            "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
            "Ep 9 (Step 001050): Train loss 0.015, Val loss 0.026\n",
            "Ep 9 (Step 001100): Train loss 0.008, Val loss 0.050\n",
            "Ep 9 (Step 001150): Train loss 0.125, Val loss 0.036\n",
            "Training accuracy: 97.50% | Validation accuracy: 97.50%\n",
            "Ep 10 (Step 001200): Train loss 0.021, Val loss 0.023\n",
            "Ep 10 (Step 001250): Train loss 0.006, Val loss 0.056\n",
            "Training accuracy: 95.00% | Validation accuracy: 100.00%\n",
            "device: cuda\n",
            "training runtime: 0 min 35.33 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
        "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(label.capitalize())\n",
        "    ax1.legend()\n",
        "\n",
        "    # Create a second x-axis for examples seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(examples_seen, train_values, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Examples seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(f\"demos/gpt2/classification_finetune_{label}_gpt2.pdf\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "vTGiD185bzJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
        "\n",
        "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "ckpQI4gNRKY7",
        "outputId": "d55c66ca-8324-4ed7-c1ec-8a2c63196229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
        "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
        "\n",
        "plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label=\"accuracy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "aFYnwxrBRKVw",
        "outputId": "fa6f8545-9539-475b-83b3-db5856fa24b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAEiCAYAAAA21pHjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAZXRJREFUeJzt3Xdc1PUfwPHXHXuIICA4wQFORFzkXiiOKG1oaoqjTHObpZYzf0VZmSPTtNKWs9SGZSHuPXGBe6AoIC6WrLvv74+L0wtQUOAOeD8fj3t0973P9/t93ye8932/n6VSFEVBCCGEECZJbewAhBBCCJE7SdRCCCGECZNELYQQQpgwSdRCCCGECZNELYQQQpgwSdRCCCGECZNELYQQQpgwSdRCCCGECZNELYQQQpgwSdRCiCfWrl07xo4da+wwhCjRJFELYUQDBw5EpVJle3Tp0sXYoQkhTIS5sQMQorTr0qULy5YtM9hmZWVlpGiEEKZGrqiFMDIrKyvc3d0NHk5OTgBs27YNS0tLdu7cqS8/e/ZsypcvT2xsLACbNm2iVatWODo64uzszLPPPsuFCxf05S9fvoxKpWLNmjW0bt0aGxsbmjZtytmzZzl48CBNmjTB3t6erl27cvPmTf1+AwcOpEePHsycORNXV1ccHBwYNmwY6enpuX6WtLQ0JkyYQKVKlbCzs8Pf359t27bp379y5QpBQUE4OTlhZ2dHvXr1+PPPP3M93pdffomXlxfW1ta4ubnx0ksv6d/TarWEhIRQrVo1bGxs8PX15eeffzbY/+TJk3Tt2hV7e3vc3Nzo378/8fHx+vfbtWvH6NGjeeeddyhXrhzu7u7MmDEj13iEMAZJ1EKYsKw24P79+3Pv3j2OHj3K1KlT+frrr3FzcwMgOTmZ8ePHc+jQIcLCwlCr1fTs2ROtVmtwrOnTpzNlyhSOHDmCubk5ffv25Z133mHevHns3LmT8+fPM23aNIN9wsLCiIyMZNu2baxcuZJ169Yxc+bMXOMdOXIke/fuZdWqVRw/fpyXX36ZLl26cO7cOQBGjBhBWloaO3bs4MSJE3z88cfY29vneKxDhw4xevRo3n//fc6cOcOmTZto06aN/v2QkBC+//57Fi9ezKlTpxg3bhyvvvoq27dvB+Du3bt06NABPz8/Dh06xKZNm4iNjaVXr14G5/nuu++ws7Nj//79zJ49m/fff5/Q0NA8/h8SoggoQgijCQ4OVszMzBQ7OzuDxwcffKAvk5aWpjRs2FDp1auXUrduXeX1119/5DFv3rypAMqJEycURVGUS5cuKYDy9ddf68usXLlSAZSwsDD9tpCQEKVWrVoGsZUrV05JTk7Wb1u0aJFib2+vaDQaRVEUpW3btsqYMWMURVGUK1euKGZmZkp0dLRBPB07dlQmT56sKIqi+Pj4KDNmzMhT3fzyyy+Kg4ODkpCQkO291NRUxdbWVtmzZ4/B9iFDhih9+vRRFEVRZs2apXTu3Nng/atXryqAcubMGX38rVq1MijTtGlTZeLEiXmKUYiiIG3UQhhZ+/btWbRokcG2cuXK6Z9bWlry008/0aBBAzw8PPj8888Nyp47d45p06axf/9+4uPj9VfSUVFR1K9fX1+uQYMG+udZV+M+Pj4G2+Li4gyO7evri62trf518+bNSUpK4urVq3h4eBiUPXHiBBqNBm9vb4PtaWlpODs7AzB69GiGDx/OP//8Q0BAAC+++KJBXA/r1KkTHh4eVK9enS5dutClSxd69uyJra0t58+fJyUlhU6dOhnsk56ejp+fHwDHjh1j69atOV6xX7hwQR/nf89foUKFbPUghDFJohbCyOzs7KhZs+Yjy+zZsweA27dvc/v2bezs7PTvBQUF4eHhwdKlS6lYsSJarZb69etna0u2sLDQP1epVDlu++/t8vxISkrCzMyMw4cPY2ZmZvBeVrJ87bXXCAwMZOPGjfzzzz+EhITw2WefMWrUqGzHK1OmDEeOHGHbtm38888/TJs2jRkzZnDw4EGSkpIA2LhxI5UqVTLYL6sjXlJSEkFBQXz88cfZjl2hQgX984frAJ6+HoQoaJKohTBxFy5cYNy4cSxdupTVq1cTHBzM5s2bUavV3Lp1izNnzrB06VJat24NwK5duwrs3MeOHeP+/fvY2NgAsG/fPuzt7alSpUq2sn5+fmg0GuLi4vSx5KRKlSoMGzaMYcOGMXnyZJYuXZpjogYwNzcnICCAgIAApk+fjqOjI1u2bKFTp05YWVkRFRVF27Ztc9y3UaNG/PLLL3h6emJuLl91oviSv14hjCwtLY2YmBiDbebm5ri4uKDRaHj11VcJDAxk0KBBdOnSBR8fHz777DPefvttnJyccHZ2ZsmSJVSoUIGoqCgmTZpUYLGlp6czZMgQpkyZwuXLl5k+fTojR45Erc7eD9Xb25t+/foxYMAAPvvsM/z8/Lh58yZhYWE0aNCA7t27M3bsWLp27Yq3tzd37txh69at1KlTJ8dz//HHH1y8eJE2bdrg5OTEn3/+iVarpVatWpQpU4YJEyYwbtw4tFotrVq14t69e+zevRsHBweCg4MZMWIES5cupU+fPvpe3efPn2fVqlV8/fXX2a76hTBVkqiFMLJNmzYZ3IoFqFWrFqdPn+aDDz7gypUr/PHHH4Dulu2SJUvo06cPnTt3xtfXl1WrVjF69Gjq169PrVq1mD9/Pu3atSuQ2Dp27IiXlxdt2rQhLS2NPn36PHL40rJly/jf//7HW2+9RXR0NC4uLjzzzDM8++yzAGg0GkaMGMG1a9dwcHCgS5cu2drcszg6OrJu3TpmzJhBamoqXl5erFy5knr16gEwa9YsXF1dCQkJ4eLFizg6OtKoUSPeffddACpWrMju3buZOHEinTt3Ji0tDQ8PD7p06ZLjDw0hTJVKURTF2EEIIUzPwIEDuXv3Lhs2bDB2KEKUavKzUgghhDBhkqiFEEIIEya3voUQQggTJlfUQgghhAmTRC2EEEKYMEnUQgghhAmTRF2IFi5ciKenJ9bW1vj7+3PgwAFjh1TgQkJCaNq0KWXKlKF8+fL06NGDM2fOGJRJTU1lxIgRODs7Y29vz4svvqhfojFLVFQU3bt3x9bWlvLly/P222+TmZlpUGbbtm00atQIKysratasyfLlywv74xWojz76CJVKxdixY/XbSnvdREdH8+qrr+Ls7IyNjQ0+Pj4cOnRI/76iKEybNo0KFSpgY2NDQECAfiWuLLdv36Zfv344ODjg6OjIkCFD9FOMZjl+/DitW7fG2tqaKlWqMHv27CL5fE9Do9EwdepU/TKeNWrUYNasWTzcrag01c+OHTsICgqiYsWKqFSqbMMGi7Iu1q5dS+3atbG2tsbHx+eRS7UWCOOtB1KyrVq1SrG0tFS+/fZb5dSpU8rrr7+uODo6KrGxscYOrUAFBgYqy5YtU06ePKmEh4cr3bp1U6pWraokJSXpywwbNkypUqWKEhYWphw6dEh55plnlBYtWujfz8zMVOrXr68EBAQoR48eVf7880/FxcVFv+KSoijKxYsXFVtbW2X8+PFKRESEsmDBAsXMzEzZtGlTkX7eJ3XgwAHF09NTadCggX61KUUp3XVz+/ZtxcPDQxk4cKCyf/9+5eLFi8rff/+tnD9/Xl/mo48+UsqWLats2LBBOXbsmPLcc88p1apVU+7fv68v06VLF8XX11fZt2+fsnPnTqVmzZr6FbQURVHu3bunuLm5Kf369VNOnjyprFy5UrGxsVG++uqrIv28+fXBBx8ozs7Oyh9//KFcunRJWbt2rWJvb6/MmzdPX6Y01c+ff/6pvPfee8q6desUQFm/fr3B+0VVF7t371bMzMyU2bNnKxEREcqUKVMUCwsL/Wp1hUESdSFp1qyZMmLECP1rjUajVKxYUQkJCTFiVIUvLi5OAZTt27criqIod+/eVSwsLJS1a9fqy0RGRiqAsnfvXkVRdP8A1Wq1EhMToy+zaNEixcHBQUlLS1MURVHeeecdpV69egbn6t27txIYGFjYH+mpJSYmKl5eXkpoaKjBspClvW4mTpyYbYnJh2m1WsXd3V355JNP9Nvu3r2rWFlZKStXrlQURVEiIiIUQDl48KC+zF9//aWoVCr9cptffvml4uTkpK+vrHM/vKSnKerevbsyePBgg20vvPCC0q9fP0VRSnf9/DdRF2Vd9OrVS+nevbtBPP7+/sobb7xRoJ/xYXLruxCkp6dz+PBhAgIC9NvUajUBAQHs3bvXiJEVvnv37gEPlmk8fPgwGRkZBnVRu3Ztqlatqq+LvXv34uPjo196ESAwMJCEhAROnTqlL/PwMbLKFIf6HDFiBN27d88Wf2mvm99++40mTZrw8ssvU758efz8/Fi6dKn+/UuXLhETE2Pw2cqWLYu/v79B/Tg6OtKkSRN9mYCAANRqNfv379eXadOmDZaWlvoygYGBnDlzhjt37hT2x3xiLVq0ICwsjLNnzwK6BVJ27dpF165dAamfhxVlXRjj35sk6kIQHx+PRqMx+HIF3Xq//118oSTRarWMHTuWli1b6tdBjomJwdLSEkdHR4OyD9dFTExMjnWV9d6jyiQkJHD//v3C+DgFYtWqVRw5coSQkJBs75X2url48SKLFi3Cy8uLv//+m+HDhzN69Gi+++474MHne9S/o5iYGMqXL2/wvrm5OeXKlctXHZqiSZMm8corr1C7dm0sLCzw8/Nj7Nix9OvXD5D6eVhR1kVuZQqzrmRRDlFgRowYwcmTJwt0mcXi7OrVq4wZM4bQ0FCsra2NHY7J0Wq1NGnShA8//BDQLZN58uRJFi9eTHBwsJGjM741a9bw008/sWLFCurVq0d4eDhjx46lYsWKUj+ljFxRFwIXFxfMzMyy9d6NjY3F3d3dSFEVrpEjR/LHH3+wdetWKleurN/u7u5Oeno6d+/eNSj/cF24u7vnWFdZ7z2qjIODg36tZFNz+PBh4uLiaNSoEebm5pibm7N9+3bmz5+Pubk5bm5upbZuQLcSWN26dQ221alTh6ioKODB53vUvyN3d3fi4uIM3s/MzOT27dv5qkNT9Pbbb+uvqn18fOjfvz/jxo3T350p7fXzsKKsi9zKFGZdSaIuBJaWljRu3JiwsDD9Nq1WS1hYGM2bNzdiZAVPURRGjhzJ+vXr2bJlC9WqVTN4v3HjxlhYWBjUxZkzZ4iKitLXRfPmzTlx4oTBP6LQ0FAcHBz0X+TNmzc3OEZWGVOuz44dO3LixAnCw8P1jyZNmtCvXz/989JaNwAtW7bMNpTv7NmzeHh4AFCtWjXc3d0NPltCQgL79+83qJ+7d+9y+PBhfZktW7ag1Wrx9/fXl9mxYwcZGRn6MqGhodSqVQsnJ6dC+3xPKyUlJdtynGZmZmi1WkDq52FFWRdG+fdWaN3USrlVq1YpVlZWyvLly5WIiAhl6NChiqOjo0Hv3ZJg+PDhStmyZZVt27YpN27c0D9SUlL0ZYYNG6ZUrVpV2bJli3Lo0CGlefPmSvPmzfXvZw1B6ty5sxIeHq5s2rRJcXV1zXEI0ttvv61ERkYqCxcuLBZDkP7r4V7filK66+bAgQOKubm58sEHHyjnzp1TfvrpJ8XW1lb58ccf9WU++ugjxdHRUfn111+V48ePK88//3yOQ278/PyU/fv3K7t27VK8vLwMhtzcvXtXcXNzU/r376+cPHlSWbVqlWJra2tyw4/+Kzg4WKlUqZJ+eNa6desUFxcX5Z133tGXKU31k5iYqBw9elQ5evSoAihz5sxRjh49qly5ckVRlKKri927dyvm5ubKp59+qkRGRirTp0+X4VnF2YIFC5SqVasqlpaWSrNmzZR9+/YZO6QCB+T4WLZsmb7M/fv3lTfffFNxcnJSbG1tlZ49eyo3btwwOM7ly5eVrl27KjY2NoqLi4vy1ltvKRkZGQZltm7dqjRs2FCxtLRUqlevbnCO4uK/ibq0183vv/+u1K9fX7GyslJq166tLFmyxOB9rVarTJ06VXFzc1OsrKyUjh07KmfOnDEoc+vWLaVPnz6Kvb294uDgoAwaNEhJTEw0KHPs2DGlVatWipWVlVKpUiXlo48+KvTP9rQSEhKUMWPGKFWrVlWsra2V6tWrK++9957B0KHSVD9bt27N8bsmODhYUZSirYs1a9Yo3t7eiqWlpVKvXj1l48aNhfa5FUVRZPUsIYQQwoRJG7UQQghhwiRRCyGEECZMErUQQghhwiRRCyGEECZMErUQQghhwiRRCyGEECZMEnUhS0tLY8aMGaSlpRk7FJMjdfNoUj+PJvWTO6mbRytu9SPjqAtZQkICZcuW5d69ezg4OBg7HJMidfNoUj+PJvWTO6mbRytu9SNX1EIIIYQJk0QthBBCmDBZjzoHmZmZHD16FDc3t2yr1+RXYmIiANHR0SQkJBREeCWG1M2jSf08mtRP7qRuHs0U6ker1RIbG4ufnx/m5o9OxdJGnYODBw/SrFkzY4chhBCihDtw4ABNmzZ9ZBm5os6Bm5sboKvAChUqGDkaIYQQJc2NGzdo1qyZPt88iiTqHGTd7q5QoQKVK1c2cjRCCCFKqrw0r0pnMiGEEMKEGTVR79ixg6CgICpWrIhKpWLDhg2P3Wfbtm00atQIKysratasyfLly7OVWbhwIZ6enlhbW+Pv78+BAwcKPnghhBCiCBg1UScnJ+Pr68vChQvzVP7SpUt0796d9u3bEx4eztixY3nttdf4+++/9WVWr17N+PHjmT59OkeOHMHX15fAwEDi4uIK62MIIYQQhcZken2rVCrWr19Pjx49ci0zceJENm7cyMmTJ/XbXnnlFe7evcumTZsA8Pf3p2nTpnzxxReArgt8lSpVGDVqFJMmTcpTLNeuXaNKlSpcvXpV2qiFEEIUuPzkmWLVmWzv3r0EBAQYbAsMDGTs2LEApKenc/jwYSZPnqx/X61WExAQwN69e4sy1OIl4TrEnHx8OYDqbcHcSvc8NgLuXQPnGroHQFoiXHmCuvZoDlZldM9vXdA9ylYGt7q6bZlpELUPqj7z4PziidxLyeBifBK+lR1Rq1XGDqdYi7mXSuT1u6Ay7e4+FmZqmng6YW1hZuxQirf0FLC0LfLTFqtEHRMTk60ru5ubGwkJCdy/f587d+6g0WhyLHP69Olcj5uWlmYwOXvWYPhSITMNlnaAxBt5K//2hQeJ8tA3cPBraDsJ2v/74+jeNVjxcv7jeHMflK+je358DWz/CJq+Bt0/021LS4Tvn4OyVaDN29CwL5hZ5P88pVhCagbf7LzEt7sukZiWSb2KDowL8KZjnfKoVJKw8+PGvft8seU8aw5d5XU28Iw6gs8yX+aYUtPYoeXKzcGKEe1r0rtpFazMJWHnW+Qf8NtIeGUFeLQo0lMXq0RdWEJCQpg5c6axwzCO0xt1SdrSHly8Hl9e/dA/8LKVoaIflHF/sM3cSrctvx6+Si7jrjtG2YduB6nNwMYJ7l2F30fDrjm6HwgNehnGJLJJTstk+Z7LLNlxkXv3MwBQq+DU9QRe+/4QvlUcGd/JmzZeLpKwHyM+Joqz6z7grevtuJHpgAWZvGbzN+WUu7QxO8EBi2assHuVi+Y1jB2qgZh7qcQmpDHt11Ms3naBUR29eKlxZSzMTPtOgEkpVw3u34WjP0qifhR3d3diY2MNtsXGxuLg4ICNjQ1mZmaYmZnlWMbd3Z3cTJ48mfHjx+tfR0dHU7du3YIN3lQd+U7332fehA7v5W/fVuN0j4eVqw5Dtz1dTE0G6R4Ps3GC8ZFw6FvY9TncuQwbhsHOz6DdJKj3AjzldK8lzf10DT/su8zi7Re5nZwOQM3y9owL8OaZ6uVYuvMS3+25zLGrdwn+9gBNPZ0Y18mbFjVcjBy56bmVlMZXOy7Scf9gWqgiGEwCoZ6jGd/Zm3KO22D7bDi+imYZB2h29wDUCYJ27z5oujGy9Ewtqw9d5Yst57h+L5XJ606waNsFRnf0okfDiphLwjaUmQaHv4OkWOg4VbfNrR4M/AOqPFPk4RS7zmR//vknJ06c0G/r27cvt2/fNuhM1qxZMxYsWADoOpNVrVqVkSNHSmey/9JkwOpX4fxmGHUEnDyMHVHepCfDgSWwex7cv6PbVr4utJus+4Is5VeFqRkaVh6I4sttF7iZqGvS8XS2ZUyAF8/5VsLsoXbpm4lpLN5+gR/2XSE9UwtA8+rOvNXZmyae5YwSvym5dyuWZfujWbIvjpR0DW3Ux3jP9lfS275H/VZBhncg4s/B9o/hxM+AAqig/gu6v8u83K0qAqkZGlbs1/1txCfp/jaqu9gxJsCLZxtUNPjbKNWuHoBvOoHaHEYe0l1NF7D85BmjJuqkpCTOnz8PgJ+fH3PmzKF9+/aUK1eOqlWrMnnyZKKjo/n+++8B3fCs+vXrM2LECAYPHsyWLVsYPXo0GzduJDAwENANzwoODuarr76iWbNmzJ07lzVr1nD69Ok8TdUGpShRZ0m+BXbOxo4i/1ITYP9i2PMFpN3TbXNvAO3fA+/AUpew0zO1rDl0lYVbz3PjXioAlZ1sGN3BixcaVXrkVVNsQioLt55n5YEoMjS6r4Q23q681ckb3yqORRG+SUm4e4tT6z6i/pUfWJrZnfmaF6hfyYG3ArxpV8sV1aPu3sRFwrYQiPhV91qlhga9oe07ujtOJiAlPZMf9l5h8fYL3EnRNYd4u+nutgTWcy99nQw1mRBzHCo1erBtwwio5Ad+A8DcssBPWWwS9bZt22jfvn227cHBwSxfvpyBAwdy+fJltm3bZrDPuHHjiIiIoHLlykydOpWBAwca7P/FF1/wySefEBMTQ8OGDZk/fz7+/v55jqvUJeri7v4d2LsQ9i2C9CTdtkpNoNd3hu3cJVSmRsu6I9HM33KOa3fuA1ChrDUjO9Tk5cZVsDTP+23N6Lv3+WLLOdYeukamVvfVEFCnPOM6eVOvYtlCid+UJCfe5fi62dS9tJyyJANwwqwe13v+Qud67vlrw79xHLZ+CGf/0r1Wm+s6QbZ5GxyrFkL0+ZeUlsny3ZdYsuMiCamZANSt4MC4Tt4ElIZOhloNnFyn67x6LxrGHIMyebuge1rFJlGbqlKRqOPPgbk1OFYxdiQFJ/kW7JkHB5aCfXndLasS3DNco1X47Vg08zaf4/KtFABc7K0Y0b4GfZpVfaqhOFG3UpgXdo71R6/xb76ma313xnXyxtutTEGEb1LuJydybP1neJ//hnLolj28oq5MfJPx+AUORG32FB0Wrx2GbR/qmpgAAj+E5iMKIOqCc+9+Bt/s0o0ISErTJWzfymUZ18mbtt6uJS9ha7UQ+ZvuzsfNf0cE2ZSDl5dB9XZFEoIk6qdUKhL1qn66Ht/dP4OmQ4wdTcFKioO7V6FyY91rTQb8OgKaDNaNwy7mtFqFP0/eYO7mc5yP091BKGdnyfC2NXj1GQ9sLAuuF/yFm0nM23yO349fR1F0rQlBDSoyNsCL6q72BXYeY0lLTeHo+rnUPPMVLtwF4JqqAjF+Y/Dr9jpmj1knOF+i9sH+r6DHl2Bho9t24xiUqaD7YWkC7iSns2TnRZbvvsz9DA0ATTycGN/JmxY1S0AnQ0WBM3/p7nTE/tvXybostBgF/sMezOVQBCRRP6USn6i1GvjpJbiwxXD8ckl1aBn8MRbs3WDsiWI7YYqiKPwTEcvnoWc5HaMb61/WxoKhbaozsIUndlaFN4jjTEwiczef5a+TMYBueFdPv8qM6ehFVeeinwDiaaWnpXL0ty/wPPUlbtwC4AauXGswCr+g4ZhbFHybZDZaDSxqAXejdGNza2RvBjSW+KQ0Fm/TdTJM+7eT4TPVy/FW51o0LY6dDBUFzofB1g/g+hHdNssy0PxN3YgXG8ciD0kS9VMq8Yk6y71rpaINl7tRsOMTXUezZq/rtmm1EH+mWPxIURSFbWduMif0LCeidZ3myliZM6R1NQa3qoaDddHd3j8ZfY+5m8+yOVI3d765WsXLTSozsoMXlRxtiiyOJ5Wp0XJg4zI8j35ERUX3GeIox6W6b+L3/CgsrayLLpjEGFjVVzcL39jjuis7QH/rwgTEJqTy5dbzrDxwlXSNLmG39nLhrc61aFhcOhle3K5L0Ff3615b2IL/G9BiNNga70eHJOqnVGoSdWl28hf4eTDUfV433rV8bWNHlI2iKOw+f4vPQs9wNOouALaWZgxq6cnrravjaFsEV325CL96lzmhZ9lx9iYAlmZqXmlWhRHta+LmUITJLo80WoXfj11nXtg5Wt9Zx/sW3xGPI+drDaVhj7FY29gZJzBFgTuXHvQGVxTd3a4q/rpbsdYOxonrP3SdDM+z9tBVfSfDjrV1nQzrVzLRToZJcbp/45d36l6bW+tmO2w5FuxdjRoaSKJ+aiU6Ud+5ovuDLaKejSZr80zdxClZ4119XtLNdOZiGlNA7r94i89Cz3Lg0m0ArC3UDGjuyRttquNsbzq37g9dvs1n/5xl70Xd7WMrczWvPuPB8HY1cDGBOLUaDUf/+Z51x2/z0x3djzE3WxVzah6l0fOjsbEzsY5xF7bCDz10z22coOUYaDYULI30Q+I/om6lMH/LOdYdedDJsEs9XSfDWu4mVpeaTPjSX3dHrfEgaD3ecBZFI5NE/ZRKdKL+5XU4tQ66fqz7dVmaxUboeuNG/q57rVKDbx/deFcnT6OEdCTqDnP+Ocuu8/GA7kq1r39V3mxXg/ImeKWaZc+FeOb8c5ZDV3QT0NhYmBHcQvfDwsmu6K/8FUUhNCKW03/MY/T9L7modecl9VyGtPUiuIUn9oXYnv9UtFrdv89tH8Gtc7ptdq66GQCbDH7QCc3ILt5MYl7YOX479qCT4bP/djKsYaxOhtfDdRMhPfv5g34o1w7rLkpMsIlPEvVTKrGJ+v4d+LQWaNLg9S1QqbGxIzINN479O95VN7sdanPwe1U33rWI/oGfuHaPOaFn2HpGdyvZwkxFryZVGNmhJhXKmsaX8+MoisKOc/HM+ecMx67p2tLtrcwZ3NKTIa2rU9am8NvSFa2WXSfP8cmOmxy/dg877vOX1btEV32Oer2m4VDGNG4lP5YmE06s1Y3vvXNZt61MBWj9FjQaYDIdIs/G6joZ/nniQSfDHn6VGNPRCw/nIrwLoMmEeb6QcA26fgL+Q4vu3E9IEvVTKrGJev8S+OttcKsPw3aZTIcVk3HtkK7TyYUtutdmltB4oO7LsZBumUXeSODz0LP8E6Gbn95MreLFRpUY1cGLKuWKX29q0CXssMg45oSeJeKGbkyyg7U5r7euzqBW1QrlalbRajm1+3fMt4eQnp7Kc+n/w9bSnIEtPBnaygNH++LxYycbTQaEr9B1hrx3VbfNoTK0fRsa9jOZeQJOXb/H56Hn2Bz54O/45caVGdmhJpWdCunvOP68bmrPrEV5Di+Hy7tMqgnrUSRRP6USmagVBRa3gtiT0HW2rtejyNmVvbqE/XAnlGfehIDpBXaK83GJfL75HBuP65YXVamgR8NKjO7oRTUX02iPfFparcLfp2L4fPNZzsbqxns72VrwRtsaDGjuga1lwSTsiH2bULb8j3rpunGxqYoF3/ks58UunUyinbxAZKbBke91i9BkLUnr5AltJ4JPLzAzjVv5x/7tZLj97IM7Q680rcqI9jVxL1tATTe3L/67CMpq6LEIfF8pmOMWMUnUT6lEJuroI7C0PZhZwVunjTosodi4tAO2fABX9+k69HT75KkPeTk+mXlh5/g1PFrfGad7gwqMC/CiZnkT64xTQDRahT+OX2fe5nNcjNdNy+lib8nwdjXp5//kM6idORRGeuj/8EnTjYtNV8w5Wr4HNXpOw6ViMVlgJr8y7uvmBdg1B5JvgsoMRh4EZ9NaVvPwFV0nwz0XdJ0MLc3VvOqv62ToWuYJfzzdjdIl6PAVoOgmY6HZG9BtdgFFXbQkUT+lEpmofx8Lh5eBz8vw4tfGjqb4UBS4EKZrLsi6/X39KJz+UzcNZB4nSrh6O4UFW87xy5FoNP9m6M513RjXyZs6FYpJu+lTytRo2RB+nflh54i6rZvy1M3BipHta9KraRWszPOWsM+H7yT57/fxvX8AgAzFjCMuz+LRYxruVUz/lmeBSE/WTZWbfBMCP3iw/dph3VruJrLk694Lt5gTeoaDlx90MhzQwoM32tSgXF47GSZc191JOPwdaHULiFAzANq/W6z72UiifkolLlGnJ+s6kaUnQvDvUK2NsSMq3n58Cc6HQqNgeG7+I4veuKcbf7rm0FX9qlTta7kyvlMtfCqb6PjTQpah0fLz4WssCNOtjQxQydGGUR1q8mLjyljkssrXpVP7ubtxJn4puwHIVNQccepC5eenU7Ga6Y2DL3Jxp+HLZ6BCAxi0CSxNo4+DoijsPBfPZ6FnOXb1LgB2lmYMblWN11pVp6xtLu3sibGwey4c/EbXARagWlvd6nhV877IkqnKT54xjYYNUbhObdAlaadq4NHK2NEUf36v6maVajnmwbakm7ovxn/Hu8YlpvLl1gusOBClX+e5VU0XxnXyprGHkzGiNhkWZmr6NKvKC40qsebgVRZsOU/03ftMWneCL7ddYExHL3r4PVg3+/KF09zaMAm/hG1UUyloFRVHynbE7bkZNKvpY+RPY0JuRoKlvW5lLhNJ0gAqlYo23q609nJhy2ldJ8NT1xNYsOU8y/dc1nUybOlJmawZ9pJv6RL0gaWQqVsNjqotoMN74Fk6v7/kijoHJe6K+ptAXTtrx2m6Hsz/odEq+kXkRR79Z5pHh43DsLy6i3uNR/JToh9rD18jNUP3T8uvqiNvtKlOo6r/SdA2jg/GxaanQOo93bCbh/sPJMbozpUf1g4PJsjISNUNy1ObG87GlBSnm2s6P6zsHyxaoMmA5Hjd2POHJ89Jjte9lw+pKmt+DL/D4u0XuJ2Uigv38HC24eV2zdh38RaHw4+w2XICFioNR+zb4tx9Oh51iu8tz0KVchsyUh4MK7wbBRve1P27d83jXQczC7B7aAGOxFhQtLptWb3M0xIhLSl/sanNwL48iqLrZPjt3we5fDOBe9hhY2vHG21qMET5Gcu98w2Xq+3wHlRvX+JGqcit76dUohL1zTOwsJmu08n4iGzDjBRFoceXe/S3pET+WZPGJstJeKpj87dj75+gzrO658fXwLrXdV9IAzY8KBNSFdLu5e+4QfN0w8pAtxDBjy+Amw8M3/WgzHw/Xe/Z/OgwFdpM0D2/cRy+aq0b2/vW6Qdlvu4E1w7k77j+w6HrR6SkZ/LLtoP039OFdMUM77Qf9EVmVT5Ai7ZdqeFT/Fc/K1JZfVPyo4o/DPnnwetPa0FSjG5Ip/u/dzB2fAJb/pe/45arAaOP6F8qi1qgij3F2zYzWXvHC4BZNivpr/yO1t0XdYf3wKtziUvQWeTWt3jgyPe6/3p3yXEs8Lm4JH2SNleXzH8QhS0Ta7pkfsqL6u28YfYbFVW3UKtUqFTwyBo1+AJS6a561f/pUKU2023Pl4eOq8rtuOb5P67qP23HOR3jSeL997i2lub0f8YDZZ85KsxwLWNF/YoOjOvkTYPK3fN3TKHTdqLu/8exVQ9uIz+OKg9/Kyp1/v8//6e86t/jfvRCA55J8GZe2Dk+v92dXeqaHI1vychbXvSurs1zJ8OSTK6oc1Birqgz02FObUi5BX1WQ60u2Yp8vfMi/9sYSRtvV74f3MwIQQohhK6T4S+Hr+n7LABULGvNyA5evNwk906GxVV+8kzJ+uTCkNoMeizWzWBUMyDHIlkTE7T1Nv5qMkKI0svCTM0rzaqyZUJbZj1fDzcHK67fS+Xd9Sfo8Nk23cpd/y61WdpIoi7J1Gbg3Rl6fJnjzEX30zXs/3d1prbeLtneF0KIomZlbkb/5p5sf7s9U5+ti4u9JVdv3+ftn4/T+fMd/Br+YC6C0kISdSm279It0jO1VHK0Md6KN0IIkQNrCzOGtKrGjnfaM6lrbZxsLbgYn8yYVeF0nbeDP0/cQFtKErYk6pJq32LYPEO3/nQudvx727uNtwuqEtqzUghRvNlamjOsbQ12TuzAW528cbA252xsEm/+dITuC3YRGhFLSe9qJYm6JNJqYM8C2PU5XDuYazFpnxZCFBf2VuaM6ujFzokdGN2hJvZW5kTeSOD17w/RY+Futp2JK7EJWxJ1SdUlBOo+D7WfzfHtq7dTuHgzGTO1ihY1pX1aCFE8lLWxYHznWux8pz3D29XAxsKMY9fuMXDZQV5avJc95+ONHWKBk0RdEqnNoO5z0Ot7sMh5abkd53RX042qOuJgbRpr2gohRF452VkysUttdk5sz2utqmFlrubwlTv0/Xo/ryzZy8HLt40dYoGRRF1K6dunveS2txCi+HKxt2LKs3XZ8U57gpt7YGmmZt/F27y8eC/9v9lPeAmYdVESdUlz9Efdmq0J13MtkqHRsvu8bp3YtrUkUQshij83B2tmPl+frW+3o0+zqpirVew8F0+PhbsZsvwgJ6PzORWvCZFEXZIoiq4D2dYP4FxorsWORt0lKS2TcnaW1K9YOpdaFEKUTJUcbQh5wYctb7XjpcaVUasg7HQczy7YxbAfDnMmJtHYIeabJOqSJGov3DoPFnZQ/4Vci20/GwdAay8X1DK/txCiBKrqbMunL/uyeXxbnm9YEZUKNp2Kocu8HYxaeZQLN/O5+pcRSaIuSbIW4Kj/woPlCHOw46yuV6S0TwshSrrqrvbMe8WPv8e2oZuPO4oCvx+7Tqc52xm/Jpwrt5KNHeJjSaIuKe7fhVMbdM8bBedaLD4pjRP/ttW0lmlDhRClhLdbGb7s15iNo1sRUMcNrQLrjkTT4bPtTPrlONfupBg7xFxJoi4pTv6sW8bOtQ5UbpJrsV3ndFfTdSs4UL5MzkO3hBCipKpXsSxfBzfh1xEtaevtikarsOrgVdp/uo2pG04Scy/V2CFmI4m6pMi67d1owCMXWtfPRia9vYUQpZhvFUe+G9yMn4c1p0UNZzI0Cj/su0KbT7by/u8R3ExMM3aIekZP1AsXLsTT0xNra2v8/f05cOBArmUzMjJ4//33qVGjBtbW1vj6+rJp0yaDMjNmzEClUhk8ateuXdgfw7iuh8ONY2BmCb6v5FpMq1XYeU7GTwshRJYmnuVY8fozrHz9GZp6OpGeqeXb3ZdoM3srIX9Fcjs53dghGjdRr169mvHjxzN9+nSOHDmCr68vgYGBxMXF5Vh+ypQpfPXVVyxYsICIiAiGDRtGz549OXr0qEG5evXqcePGDf1j165dRfFxjCfrarpOENiWy7VYxI0E4pPSsbM0o7GHUxEFJ4QQpq95DWfWvNGc7wc3w7eKI/czNHy1/SKtP97Cp3+f4V5KhtFiM2qinjNnDq+//jqDBg2ibt26LF68GFtbW7799tscy//www+8++67dOvWjerVqzN8+HC6devGZ599ZlDO3Nwcd3d3/cPFpQR3mkpPgRNrdc8bDXhk0azb3s1ruGBpbvSbKUIIYVJUKhVtvF3Z8GYLvgluQr2KDiSna/hi63lazd7C/LBzJKYWfcI22rd1eno6hw8fJiAg4EEwajUBAQHs3bs3x33S0tKwtjbsAGVjY5PtivncuXNUrFiR6tWr069fP6Kioh4ZS1paGgkJCfpHYmIxGhAf8SukJYCjB3i2eWTRHfrVskrwDxchhHhKKpWKjnXc+GNUKxa/2ohabmVITM1kTuhZWs/eytpDV4s0HqMl6vj4eDQaDW5ubgbb3dzciImJyXGfwMBA5syZw7lz59BqtYSGhrJu3Tpu3LihL+Pv78/y5cvZtGkTixYt4tKlS7Ru3fqRyTckJISyZcvqH3Xr1i2YD1kU9J3I+oM69/+diakZHL5yB4C23uWLIjIhhCjWVCoVXepX4K8xrZnfx4/qrnbcTcnAysKsSOMoVvc/582bh5eXF7Vr18bS0pKRI0cyaNAg1A8lqK5du/Lyyy/ToEEDAgMD+fPPP7l79y5r1qzJ9biTJ0/m3r17+kdERERRfJynl5oACdGgUkPDfo8suvfCLTK1Cp7OtlR1ti2iAIUQovhTq1U851uRf8a2YVG/RjzrU6Foz1+kZ3uIi4sLZmZmxMbGGmyPjY3F3d09x31cXV3ZsGEDycnJXLlyhdOnT2Nvb0/16tVzPY+joyPe3t6cP38+1zJWVlY4ODjoH2XK5D6rl0mxdoDR4TB0GzhUfGRR/bAsb+ntLYQQT8LcTE1XnwpFPvWy0RK1paUljRs3JiwsTL9Nq9USFhZG8+bNH7mvtbU1lSpVIjMzk19++YXnn38+17JJSUlcuHCBChWK9hdQkVGroYLvI4soiqJP1G0kUQshRLFi1Fvf48ePZ+nSpXz33XdERkYyfPhwkpOTGTRoEAADBgxg8uTJ+vL79+9n3bp1XLx4kZ07d9KlSxe0Wi3vvPOOvsyECRPYvn07ly9fZs+ePfTs2RMzMzP69OlT5J+vUCVch8y8je+7FJ/MtTv3sTRT80x150IOTAghREEyN+bJe/fuzc2bN5k2bRoxMTE0bNiQTZs26TuYRUVFGbQ/p6amMmXKFC5evIi9vT3dunXjhx9+wNHRUV/m2rVr9OnTh1u3buHq6kqrVq3Yt28frq4l7Eryt9Fw/Qj0WAzenR9ZNKu3dxNPJ+ysjPq/XAghRD4Z/Vt75MiRjBw5Msf3tm3bZvC6bdu2j+3otWrVqoIKzXSlp0BcJKTcAucajy0u7dNCCFF8GT1RiydgaQtjjkH0occm6tQMDfsu3gakfVoIIYqjYjU8SzzEzByqPvPYYocu3+F+hobyZayo7V5MerMLIYTQk0Rd3CTfAk1mnovvOPegt7fqEatqCSGEME2SqIubjeNhbn04+3eeim8/I+3TQghRnEkbdXGSHA+nN4I2AxwqPbZ4zL1UzsQmolJBq5oyv7cQQhRHckVdnBxbpUvSFRuBe/3HFs8aluVb2REnO8vCjk4IIUQhyHei9vT05P3333/silSigCkKHPlO9/wxy1lm2X5OZiMTQojiLt+JeuzYsaxbt47q1avTqVMnVq1aRVpaWmHEJh52dT/EnwULW6j/4mOLa7QKu87FA9I+LYQQxdkTJerw8HAOHDhAnTp1GDVqFBUqVGDkyJEcOXKkMGIU8GA5y3ov6BbjeIxj1+5y734GDtbm+FYuW8jBCSGEKCxP3EbdqFEj5s+fz/Xr15k+fTpff/01TZs2pWHDhnz77bcoilKQcZZuqffg1Hrd87ze9v63t3drL1fMzaQrghBCFFdP3Os7IyOD9evXs2zZMkJDQ3nmmWcYMmQI165d491332Xz5s2sWLGiIGMtvU7+Ahkp4FILqjTL0y4Pxk9Lb28hhCjO8p2ojxw5wrJly1i5ciVqtZoBAwbw+eefU7t2bX2Znj170rRp0wINtFTLuu3daADkYdKSuynpHLt6F5COZEIIUdzlO1E3bdqUTp06sWjRInr06IGFhUW2MtWqVeOVV14pkABLvRvH4fpRUFuAb97qdNf5eLQKeLvZU6GsTSEHKIQQojDlO1FfvHgRDw+PR5axs7Nj2bJlTxyUeMjRH3T/rd0d7PJ2GzurfbqNl1xNCyFEcZfvXkZxcXHs378/2/b9+/dz6NChAglK/CvjPhxfrXuex05kiqLo26fb1pJELYQQxV2+E/WIESO4evVqtu3R0dGMGDGiQIIS/1K00HoCVG+ve+TBmdhEYhPSsLZQ09SzXCEHKIQQorDl+9Z3REQEjRo1yrbdz8+PiIiIAglK/MvSDlqO1j3yKGva0GeqO2NtYVZYkQkhhCgi+b6itrKyIjY2Ntv2GzduYG4ua3wY2/az0j4thBAlSb4TdefOnZk8eTL37t3Tb7t79y7vvvsunTp1KtDgSrUj38PxNbp26jxKSc/k4KU7gLRPCyFESZHvS+BPP/2UNm3a4OHhgZ+fHwDh4eG4ubnxww8/FHiApZImA8JmQXIc9P4R6gTlabf9F2+TrtFSydGG6i52hRykEEKIopDvRF2pUiWOHz/OTz/9xLFjx7CxsWHQoEH06dMnxzHV4glo0qHZ63B2E3h3yfNuWbe929ZyRZWHiVGEEEKYvidqVLazs2Po0KEFHYvIYmkHbd/RPfJhh7RPCyFEifPEvb8iIiKIiooiPT3dYPtzzz331EGJ/Lt6O4WL8cmYq1W0qOls7HCEEEIUkCeamaxnz56cOHEClUqlXyUr61arRqMp2AhLm5O/gNocvLuCuWWed8u67d2oqhMO1tIEIYQQJUW+e32PGTOGatWqERcXh62tLadOnWLHjh00adKEbdu2FUKIpYhWC5tnwJoBELEhX7s+3D4thBCi5Mj3FfXevXvZsmULLi4uqNVq1Go1rVq1IiQkhNGjR3P06NHCiLN0uLQN7kaBVVmo/Wyed0vP1LL3wi1A2qeFEKKkyfcVtUajoUyZMgC4uLhw/fp1ADw8PDhz5kzBRlfaZC1n2eBlsLTN+25Rd0hKy8TZzpJ6FR0KKTghhBDGkO8r6vr163Ps2DGqVauGv78/s2fPxtLSkiVLllC9evXCiLF0SL4FkX/onjcKzteuWb29W3u5oFbLsCwhhChJ8p2op0yZQnJyMgDvv/8+zz77LK1bt8bZ2ZnVq1cXeIClxvFVoM2ACg2hQoN87Srt00IIUXLlO1EHBgbqn9esWZPTp09z+/ZtnJycZJKNJ6UoD25753E5yyw3E9M4dT0BgNbSPi2EECVOvtqoMzIyMDc35+TJkwbby5UrJ0n6aVw7CDdPg7kN+LyUr113/rv2dP1KDrjYWxVGdEIIIYwoX4nawsKCqlWrFuhY6YULF+Lp6Ym1tTX+/v4cOHAg17IZGRm8//771KhRA2tra3x9fdm0adNTHdMkHPlO9996PcG6bL52ldnIhBCiZMt3r+/33nuPd999l9u3bz/1yVevXs348eOZPn06R44cwdfXl8DAQOLi4nIsP2XKFL766isWLFhAREQEw4YNo2fPngZDwvJ7TKNLTYCT63TP83nbW6tV2HEuHoC23pKohRCiJFIpWVOL5ZGfnx/nz58nIyMDDw8P7OwMV2k6cuRIno/l7+9P06ZN+eKLLwDQarVUqVKFUaNGMWnSpGzlK1asyHvvvceIESP021588UVsbGz48ccfn+iYObl27RpVqlTh6tWrVK5cOc+f54kcXg6/jwFnLxh5EPLRhHDi2j2CvtiFvZU5R6d1wsIs37+7hBBCGEF+8ky+O5P16NHjSeMykJ6ezuHDh5k8ebJ+m1qtJiAggL179+a4T1paGtbW1gbbbGxs2LVr1xMfM+u4aWlp+teJiYlP9JmeyMOdyPLZzr/9rO4uQfMazpKkhRCihMp3op4+fXqBnDg+Ph6NRoObm5vBdjc3N06fPp3jPoGBgcyZM4c2bdpQo0YNwsLCWLdunb7N/EmOCRASEsLMmTOf8hM9gYTrEHNSN7e3b598777jrNz2FkKIkq5YXYbNmzcPLy8vateujaWlJSNHjmTQoEGo1U/3MSZPnsy9e/f0j4iIiAKK+DEcKsJbp6H3T2Cfv2SbkJrB4ag7gCRqIYQoyfKd4dRqNWZmZrk+8srFxQUzMzNiY2MNtsfGxuLu7p7jPq6urmzYsIHk5GSuXLnC6dOnsbe318+I9iTHBLCyssLBwUH/yJoitUjYloNaXfK9257zt9BoFaq72FGlXN6nGxVCCFG85PvW9/r16w1eZ2RkcPToUb777rt83T62tLSkcePGhIWF6du9tVotYWFhjBw58pH7WltbU6lSJTIyMvjll1/o1avXUx+zyKWn5Gs+7//Kmo2sjVxNCyFEiZbvRP38889n2/bSSy9Rr149Vq9ezZAhQ/J8rPHjxxMcHEyTJk1o1qwZc+fOJTk5mUGDBgEwYMAAKlWqREhICAD79+8nOjqahg0bEh0dzYwZM9Bqtbzzzjt5PqbJWNkbMu5Dt0+hYsN87aooin78tNz2FkKIki3fiTo3zzzzDEOHDs3XPr179+bmzZtMmzaNmJgYGjZsyKZNm/SdwaKiogzan1NTU5kyZQoXL17E3t6ebt268cMPP+Do6JjnY5qEpJtwZS9oM8HWOd+7X7iZTPTd+1iaqfGvXq4QAhRCCGEq8j2OOif3799n8uTJ/PXXXyViqcsiGUedGAuXd+Z7ylCAb3dd4v0/ImhV04UfX/MvhOCEEEIUpkIdR/3fxTcURSExMRFbW1v9pCMiD8q4PVGShofbp10KMiIhhBAmKN+J+vPPPzdI1Gq1GldXV/z9/XFycirQ4EokTQaYWTzx7qkZGvZfugVAW+/yBRWVEEIIE5XvRD1w4MBCCKMUWTMA0pOg06x8dyIDOHDpNqkZWtwdrPF2sy/4+IQQQpiUfCfqZcuWYW9vz8svv2ywfe3ataSkpBAcHFxgwZU4Cdfh7CZQtGDxZEOzdjx021uWFhVCiJIv3xOehISE4OKSvW20fPnyfPjhhwUSVIkVvkKXpKs2B1fvJzqEjJ8WQojSJd+JOioqimrVqmXb7uHhQVRUVIEEVSJptXD0B93zfC5nmeX63fuci0tCrYJWNaUjmRBClAb5TtTly5fn+PHj2bYfO3YMZ+f8jwkuNS7vhDuXwcoB6mafNCYvdp7TXU37VnHE0dayAIMTQghhqvKdqPv06cPo0aPZunUrGo0GjUbDli1bGDNmDK+88kphxFgyZC1n6fMSWNo9umwutstsZEIIUerkuzPZrFmzuHz5Mh07dsTcXLe7VqtlwIAB0kadm5TbEPmb7vkT3vbO1GjZdU63rKW0TwshROmR70RtaWnJ6tWr+d///kd4eDg2Njb4+Pjg4eFRGPGVDMfXgCYd3H2gQsMnOsSxa3dJSM2krI0FvpUdCzQ8IYQQpuuJ5/r28vLCy8urIGMpmRQFjnyne94oGJ5wSNX2s7qr6VZeLpipZViWEEKUFvluo37xxRf5+OOPs22fPXt2trHVAog+AnERYG79xFOGgrRPCyFEaZXvRL1jxw66deuWbXvXrl3ZsWNHgQRVohxZrvtv3R5g82RTrN5JTuf4tbsAtPGSRC2EEKVJvhN1UlISlpbZhwZZWFiQkJBQIEGVGGmJcOIX3fMn7EQGsPN8PIoCtd3L4F7WuoCCE0IIURzkO1H7+PiwevXqbNtXrVpF3bp1CySoEiM9Ger10HUi82jxxIfZIbORCSFEqZXvzmRTp07lhRde4MKFC3To0AGAsLAwVqxYwc8//1zgARZrZdyhx5e6WcmesBOZoigPErXc9hZCiFIn34k6KCiIDRs28OGHH/Lzzz9jY2ODr68vW7ZsoVy5coURY/GnzveNC73TMYnEJaZhY2FGE09ZRlQIIUqbJxqe1b17d7p37w5AQkICK1euZMKECRw+fBiNRlOgAZZ2Wb29n6leDmsLMyNHI4QQoqg98aXejh07CA4OpmLFinz22Wd06NCBffv2FWRsggft0zIsSwghSqd8XVHHxMSwfPlyvvnmGxISEujVqxdpaWls2LBBOpIVguS0TA5evg1IRzIhhCit8nxFHRQURK1atTh+/Dhz587l+vXrLFiwoDBjK/X2XbxFhkahSjkbqrk82UIeQgghirc8X1H/9ddfjB49muHDh8vUoUVk+0O9vVVP2GtcCCFE8ZbnK+pdu3aRmJhI48aN8ff354svviA+Pr4wYyv1pH1aCCFEnhP1M888w9KlS7lx4wZvvPEGq1atomLFimi1WkJDQ0lMTCzMOEudK7eSuXwrBXO1iuY1nI0djhBCCCPJd69vOzs7Bg8ezK5duzhx4gRvvfUWH330EeXLl+e5554rjBhLpayr6cYeTpSxtjByNEIIIYzlyWfiAGrVqsXs2bO5du0aK1euLKiYBA+1T8ttbyGEKNWeKlFnMTMzo0ePHvz2228FcbhSLz1Ty54LtwBpnxZCiNKuQBK1KFiHrtwmJV2Di70ldSs4GDscIYQQRiSJ2gTtOKvrTd/GyxW1WoZlCSFEaSaJ2gRJ+7QQQogskqhNTFxCKpE3ElCpoLWXi7HDEUIIYWRGT9QLFy7E09MTa2tr/P39OXDgwCPLz507l1q1amFjY0OVKlUYN24cqamp+vdnzJiBSqUyeNSuXbuwP0aB2XFOd9u7fsWyONtbGTkaIYQQxvZEy1wWlNWrVzN+/HgWL16Mv78/c+fOJTAwkDNnzlC+fPls5VesWMGkSZP49ttvadGiBWfPnmXgwIGoVCrmzJmjL1evXj02b96sf21ubtSPmS8yG5kQQoiHGTWDzZkzh9dff51BgwYBsHjxYjZu3Mi3337LpEmTspXfs2cPLVu2pG/fvgB4enrSp08f9u/fb1DO3Nwcd3f3wv8ABUyjVdh5TtqnRemi0WjIyMgwdhhCFCgLCwvMzMwK5FhGS9Tp6ekcPnyYyZMn67ep1WoCAgLYu3dvjvu0aNGCH3/8kQMHDtCsWTMuXrzIn3/+Sf/+/Q3KnTt3jooVK2JtbU3z5s0JCQmhatWqhfp5CsLJ6HvcScmgjJU5flUdjR2OEIVKURRiYmK4e/eusUMRolA4Ojri7u7+1IsqGS1Rx8fHo9FocHNzM9ju5ubG6dOnc9ynb9++xMfH06pVKxRFITMzk2HDhvHuu+/qy/j7+7N8+XJq1arFjRs3mDlzJq1bt+bkyZOUKVMmx+OmpaWRlpamf22secuzbnu3qOmMhZnRuw8IUaiyknT58uWxtbWVFeJEiaEoCikpKcTFxQFQoUKFpzpe8Wm8BbZt28aHH37Il19+ib+/P+fPn2fMmDHMmjWLqVOnAtC1a1d9+QYNGuDv74+Hhwdr1qxhyJAhOR43JCSEmTNnFslneBQZliVKC41Go0/Szs6y6IwoeWxsbACIi4ujfPnyT3Ub3GiXbS4uLpiZmREbG2uwPTY2Ntf25alTp9K/f39ee+01fHx86NmzJx9++CEhISFotdoc93F0dMTb25vz58/nGsvkyZO5d++e/hEREfHkH+wJ3bufwdGrdwHdRCdClGRZbdK2trZGjkSIwpP19/20fTCMlqgtLS1p3LgxYWFh+m1arZawsDCaN2+e4z4pKSmo1YYhZ/1KURQlx32SkpK4cOHCI289WFlZ4eDgoH/kdou8MO05H49Gq1Dd1Y4q5eTLS5QOcrtblGQF9fdt1IbQ8ePHs3TpUr777jsiIyMZPnw4ycnJ+l7gAwYMMOhsFhQUxKJFi1i1ahWXLl0iNDSUqVOnEhQUpE/YEyZMYPv27Vy+fJk9e/bQs2dPzMzM6NOnj1E+Y17tOCfDsoQojTw9PZk7d26ey2/btg2VSiWd8EoRo7ZR9+7dm5s3bzJt2jRiYmJo2LAhmzZt0ncwi4qKMriCnjJlCiqViilTphAdHY2rqytBQUF88MEH+jLXrl2jT58+3Lp1C1dXV1q1asW+fftwdTXdBKgoCtvPSPu0EKbscVdH06dPZ8aMGfk+7sGDB7Gzs8tz+RYtWnDjxg3Kli2b73OJ4kml5HbPuBS7du0aVapU4erVq1SuXLnQz3c+LpGAOTuwNFdzbFpnbCwLZuydEKYqNTWVS5cuUa1aNaytrY0dTp7ExMTon69evZpp06Zx5swZ/TZ7e3vs7e0B3Y9vjUZTrCZbKg7S09OxtLQ0dhh59qi/8/zkGRkDZAK2/Xs17V+tnCRpIUyUu7u7/lG2bFlUKpX+9enTpylTpgx//fUXjRs3xsrKil27dnHhwgWef/553NzcsLe3p2nTpgazJkL2W98qlYqvv/6anj17Ymtri5eXF7/99pv+/f/e+l6+fDmOjo78/fff1KlTB3t7e7p06cKNGzf0+2RmZjJ69GgcHR1xdnZm4sSJBAcH06NHj1w/761bt+jTpw+VKlXC1tYWHx8fVq5caVBGq9Uye/ZsatasiZWVFVWrVs3xDme5cuWws7OjSZMm+gmqBg4cmO38Y8eOpV27dvrX7dq1Y+TIkYwdOxYXFxcCAwMB3WRZPj4+2NnZUaVKFd58802SkpIMjrV7927atWuHra0tTk5OBAYGcufOHb7//nucnZ0NhuQC9OjRI9ucHKZCErUJyJrfW9qnRWmlKAop6ZlGeRTkTcVJkybx0UcfERkZSYMGDUhKSqJbt26EhYVx9OhRunTpQlBQEFFRUY88zsyZM+nVqxfHjx+nW7du9OvXj9u3b+daPiUlhU8//ZQffviBHTt2EBUVxYQJE/Tvf/zxx/z0008sW7aM3bt3k5CQwIYNGx4ZQ2pqKo0bN2bjxo2cPHmSoUOH0r9/f4P1GCZPnsxHH33E1KlTiYiIYMWKFfqmy6SkJNq2bUt0dDS//fYbx44d45133sl1hE5uvvvuOywtLdm9ezeLFy8GdJNjzZ8/n1OnTvHdd9+xZcsW3nnnHf0+4eHhdOzYkbp167J371527dpFUFAQGo2Gl19+GY1GY/DjJy4ujo0bNzJ48OB8xVZU5L6MkaVmaNh/8RYg7dOi9LqfoaHutL+Ncu6I9wOxtSyYr8L333+fTp066V+XK1cOX19f/etZs2axfv16fvvtN0aOHJnrcQYOHKjvAPvhhx8yf/58Dhw4QJcuXXIsn5GRweLFi6lRowYAI0eO5P3339e/v2DBAiZPnkzPnj0B+OKLL/jzzz8f+VkqVapkkOxHjRrF33//zZo1a2jWrBmJiYnMmzePL774guDgYABq1KhBq1atAN3aDDdv3uTgwYOUK1cOgJo1az7ynDnx8vJi9uzZBtvGjh2rf+7p6cn//vc/hg0bxpdffgnA7NmzadKkif416NaAyNK3b1+WLVvGyy+/DMCPP/5I1apVDa7mTYkkaiPbf+k2aZlaKpS1xqu8vbHDEUI8hSZNmhi8TkpKYsaMGWzcuJEbN26QmZnJ/fv3H3tF3aBBA/1zOzs7HBwc9LNc5cTW1lafpEE3E1ZW+Xv37hEbG0uzZs3075uZmdG4ceNHXt1qNBo+/PBD1qxZQ3R0NOnp6aSlpenHBkdGRpKWlkbHjh1z3D88PBw/Pz99kn5SjRs3zrZt8+bNhISEcPr0aRISEsjMzCQ1NZWUlBRsbW0JDw/XJ+GcvP766zRt2pTo6GgqVarE8uXL9Qs8mSJJ1Eam7+3t5WqyfyRCFDYbCzMi3g802rkLyn97b0+YMIHQ0FA+/fRTatasiY2NDS+99BLp6emPPI6FhYXBa5VK9cikmlP5p72l/8knnzBv3jzmzp2rbw8eO3asPvasmbdy87j31Wp1thhzmhjkv3V6+fJlnn32WYYPH84HH3xAuXLl2LVrF0OGDCE9PR1bW9vHntvPzw9fX1++//57OnfuzKlTp9i4ceMj9zEmaaM2Mv346Vpy21uUXiqVCltLc6M8CvMH8u7duxk4cCA9e/bEx8cHd3d3Ll++XGjny0nZsmVxc3Pj4MGD+m0ajYYjR448cr/du3fz/PPP8+qrr+Lr60v16tU5e/as/n0vLy9sbGwMJq16WIMGDQgPD8+1bd3V1dWgwxvorsIf5/Dhw2i1Wj777DOeeeYZvL29uX79erZz5xZXltdee43ly5ezbNkyAgICqFKlymPPbSySqI0o+u59zscloVZByxouxg5HCFHAvLy8WLduHeHh4Rw7doy+ffvmuzNVQRg1ahQhISH8+uuvnDlzhjFjxnDnzp1H/kjx8vIiNDSUPXv2EBkZyRtvvGEw5bO1tTUTJ07knXfe4fvvv+fChQvs27ePb775BoA+ffrg7u5Ojx492L17NxcvXuSXX37Rr47YoUMHDh06xPfff8+5c+eYPn06J0+efOxnqVmzJhkZGSxYsICLFy/yww8/6DuZZZk8eTIHDx7kzTff5Pjx45w+fZpFixYRHx+vL9O3b1+uXbvG0qVLTbYTWRZJ1EaUtVqWX1UnytpaPKa0EKK4mTNnDk5OTrRo0YKgoCACAwNp1KhRkccxceJE+vTpw4ABA2jevDn29vYEBgY+cgz7lClTaNSoEYGBgbRr106fdB82depU3nrrLaZNm0adOnXo3bu3vm3c0tKSf/75h/Lly9OtWzd8fHz46KOP9LNIBgYGMnXqVN555x2aNm1KYmIiAwYMeOxn8fX1Zc6cOXz88cfUr1+fn376iZCQEIMy3t7e/PPPPxw7doxmzZrRvHlzfv31V4Nx7WXLluXFF1/E3t7+kcPUTIFMeJKDoprwZNgPh9l0KoZxAd6MCfAqtPMIYWqK44QnJYlWq6VOnTr06tWLWbNmGTsco+nYsSP16tVj/vz5hXL8gprwRDqTGUmGRsvu8/+On5b2aSFEIbpy5Qr//PMPbdu2JS0tjS+++IJLly7Rt29fY4dmFHfu3GHbtm1s27bNYAiXqZJEbSThV++SmJaJo60FPpVkzl4hROFRq9UsX76cCRMmoCgK9evXZ/PmzdSpU8fYoRmFn58fd+7c4eOPP6ZWrVrGDuexJFEbSVb7dGsvV8zUMixLCFF4qlSpwu7du40dhsko6p73T0s6kxnJ9rNZ46elt7cQQojcSaI2gltJaZyIvgfI/N5CCCEeTRK1Eew6H4+iQG33MpR3kB6vQgghcieJ2giybntLb28hhBCPI4m6iGm1CjvO/jssy0sStRBCiEeTRF3EImMSiE9Kw8bCjMaeTsYORwghhImTRF3Esm57t6jhjJV5wa3aI4QoHtq1a5dtPeW5c+c+ch+VSsWGDRue+twFdRxRtCRRF7Gs8dNtpLe3EMVKUFAQXbp0yfG9nTt3olKpOH78eL6Pe/DgQYYOHfq04RmYMWMGDRs2zLb9xo0bdO3atUDPJQqfJOoilJSWyaHLdwAZliVEcTNkyBBCQ0O5du1atveWLVtGkyZNaNCgQb6P6+rqiq2tbUGE+Fju7u5YWVkVyblMyePW/zZ1kqiL0N4Lt8jUKlQtZ4uni93jdxBCmIxnn30WV1dXli9fbrA9KSmJtWvXMmTIEG7dukWfPn2oVKkStra2+Pj4sHLlykce97+3vs+dO0ebNm2wtrambt26hIaGZttn4sSJeHt7Y2trS/Xq1Zk6dSoZGRkALF++nJkzZ3Ls2DFUKhUqlUof839vfZ84cYIOHTpgY2ODs7MzQ4cOJSkpSf/+wIED6dGjB59++ikVKlTA2dmZESNG6M+VkwsXLvD888/j5uaGvb09TZs2ZfPmzQZl0tLSmDhxIlWqVMHKyoqaNWvql8cEOHXqFM8++ywODg6UKVOG1q1bc+HCBSB70wFAjx49GDhwoEGdzpo1iwEDBuDg4KC/Y/Goesvy+++/07RpU6ytrXFxcaFnz54AvP/++9SvXz/b523YsCFTp07NtT4KgiTqIpR121uupoXIRXpy/h+azAf7azJ12zLu5+24+WBubs6AAQNYvnw5Dy86uHbtWjQaDX369CE1NZXGjRuzceNGTp48ydChQ+nfvz8HDhzI0zm0Wi0vvPAClpaW7N+/n8WLFzNx4sRs5cqUKcPy5cuJiIhg3rx5LF26lM8//xyA3r1789Zbb1GvXj1u3LjBjRs36N27d7ZjJCcnExgYiJOTEwcPHmTt2rVs3ryZkSNHGpTbunUrFy5cYOvWrXz33XcsX74824+VhyUlJdGtWzfCwsI4evQoXbp0ISgoiKioKH2ZAQMGsHLlSubPn09kZCRfffUV9vb2AERHR9OmTRusrKzYsmULhw8fZvDgwWRmZuZ2yhx9+umn+Pr6cvToUX0ifVS9AWzcuJGePXvSrVs3jh49SlhYGM2aNQNg8ODBREZGcvDgQX35o0ePcvz4cQYNGpSv2PJNEdlcvXpVAZSrV68W6HFbf7xF8Zj4h/LPqZgCPa4Qxc39+/eViIgI5f79+4ZvTHfI/+Pkugf7n1yn2/ZtN8Pjflwt533zKTIyUgGUrVu36re1bt1aefXVV3Pdp3v37spbb72lf922bVtlzJgx+tceHh7K559/riiKovz999+Kubm5Eh0drX//r7/+UgBl/fr1uZ7jk08+URo3bqx/PX36dMXX1zdbuYePs2TJEsXJyUlJSkrSv79x40ZFrVYrMTG676jg4GDFw8NDyczM1Jd5+eWXld69e+caS07q1aunLFiwQFEURTlz5owCKKGhoTmWnTx5slKtWjUlPT09x/f/W3+KoijPP/+8EhwcrH/t4eGh9OjR47Fx/bfemjdvrvTr1y/X8l27dlWGDx+ufz1q1CilXbt2uZbP9e9cyV+ekSvqInI5Ppmo2ylYmKloXsPZ2OEIIZ5A7dq1adGiBd9++y0A58+fZ+fOnQwZMgQAjUbDrFmz8PHxoVy5ctjb2/P3338bXE0+SmRkJFWqVKFixYr6bc2bN89WbvXq1bRs2RJ3d3fs7e2ZMmVKns/x8Ll8fX2xs3vQDNeyZUu0Wi1nzpzRb6tXrx5mZg9GqFSoUIG4uLhcj5uUlMSECROoU6cOjo6O2NvbExkZqY8vPDwcMzMz2rZtm+P+4eHhtG7dGgsLi3x9nv9q0qRJtm2Pq7fw8HA6duyY6zFff/11Vq5cSWpqKunp6axYsYLBgwc/VZx5IatnFZGsYVmNPZywt5JqFyJH717P/z5mD3WOqh2kO4bqP9cgY088XVwPGTJkCKNGjWLhwoUsW7aMGjVq6JPOJ598wrx585g7dy4+Pj7Y2dkxduzYAu3MtHfvXvr168fMmTMJDAykbNmyrFq1is8++6zAzvGw/yZMlUqFVqvNtfyECRMIDQ3l008/pWbNmtjY2PDSSy/p68DGxuaR53vc+2q12qDpAcixzfzhHyCQt3p73LmDgoKwsrJi/fr1WFpakpGRwUsvvfTIfQqCXFEXkQft0+WNHIkQJszSLv8Ps4d++JqZ67ZZ2OTtuE+gV69eqNVqVqxYwffff8/gwYNRqXRL1e7evZvnn3+eV199FV9fX6pXr87Zs2fzfOw6depw9epVbty4od+2b98+gzJ79uzBw8OD9957jyZNmuDl5cWVK1cMP66lJRqN5rHnOnbsGMnJD9rqd+/ejVqtfqo1mnfv3s3AgQPp2bMnPj4+uLu7Gywr6ePjg1arZfv27Tnu36BBA3bu3JlrhzVXV1eD+tFoNJw8efKxceWl3ho0aEBYWFiuxzA3Nyc4OJhly5axbNkyXnnllccm94IgiboIpGVq2HPhFgBtvGVZSyGKM3t7e3r37s3kyZO5ceOGQW9jLy8vQkND2bNnD5GRkbzxxhvExsbm+dgBAQF4e3sTHBzMsWPH2LlzJ++9955BGS8vL6Kioli1ahUXLlxg/vz5rF+/3qCMp6cnly5dIjw8nPj4eNLS0rKdq1+/flhbWxMcHMzJkyfZunUro0aNon///ri5ueWvUv4T37p16wgPD+fYsWP07dvX4Arc09OT4OBgBg8ezIYNG7h06RLbtm1jzZo1AIwcOZKEhAReeeUVDh06xLlz5/jhhx/0t+M7dOjAxo0b2bhxI6dPn2b48OHcvXs3T3E9rt6mT5/OypUrmT59OpGRkZw4cYKPP/7YoMxrr73Gli1b2LRpU5Hc9gZJ1EXi8OU73M/Q4FrGiroVHIwdjhDiKQ0ZMoQ7d+4QGBho0J48ZcoUGjVqRGBgIO3atcPd3Z0ePXrk+bhqtZr169dz//59mjVrxmuvvcYHH3xgUOa5555j3LhxjBw5koYNG7Jnz55sw4NefPFFunTpQvv27XF1dc1xiJitrS1///03t2/fpmnTprz00kt07NiRL774In+V8R9z5szBycmJFi1aEBQURGBgII0aNTIos2jRIl566SXefPNNateuzeuvv66/snd2dmbLli0kJSXRtm1bGjduzNKlS/W34AcPHkxwcDADBgygbdu2VK9enfbt2z82rrzUW7t27Vi7di2//fYbDRs2pEOHDtl67Ht5edGiRQtq166Nv7//01RVnqmU/97sF1y7do0qVapw9epVKleu/NTHC/kzkq92XOSFRpWY06vh0wcoRDGXmprKpUuXqFatGtbWstSrKD4URcHLy4s333yT8ePHP7Lso/7O85NnpFdTEdgu46eFEKLYu3nzJqtWrSImJqbwx04/RBJ1IYtNSOV0TCIqFbSWZS2FEKLYKl++PC4uLixZsgQnp6Jb/dDobdQLFy7E09MTa2tr/P39HzuDz9y5c6lVqxY2NjZUqVKFcePGkZqa+lTHLExZvb0bVCpLOTtLo8UhhBDi6SiKws2bN+nbt2+RnteoiXr16tWMHz+e6dOnc+TIEXx9fQkMDMx1MP2KFSuYNGmSvkfeN998w+rVq3n33Xef+JiFbbusliWEEOIpGDVRz5kzh9dff51BgwZRt25dFi9ejK2trX7Wn//as2cPLVu2pG/fvnh6etK5c2f69OljcMWc32MWJo1WYdf5eEDap4UQQjwZoyXq9PR0Dh8+TEBAwINg1GoCAgLYu3dvjvu0aNGCw4cP6xPzxYsX+fPPP+nWrdsTHxN0K7kkJCToH4mJiQXxETl+7S53UzIoY21OwyqOBXJMIUoSGXQiSrKC+vs2Wmey+Ph4NBpNtoH1bm5unD59Osd9+vbtS3x8PK1atUJRFDIzMxk2bJj+1veTHBMgJCSEmTNnPuUnym7HWd3VdKuaLpibGb07gBAmI2tMbEpKSpHM7CSEMaSkpADZp2HNr2LV63vbtm18+OGHfPnll/j7+3P+/HnGjBnDrFmznmo90MmTJxuMh4uOjqZu3bpPHe/QNtXxrVKWMtbFqpqFKHRmZmY4Ojrq+47Y2trqp+EUorhTFIWUlBTi4uJwdHQ0WNTkSRgtg7i4uGBmZpZter3Y2Fjc3d1z3Gfq1Kn079+f1157DdDNGZucnMzQoUN57733nuiYAFZWVlhZPZjYPyEh4Uk/lgEbSzPa1ZK5vYXISda/SWN19BSisDk6Oj4y9+SV0RK1paUljRs3JiwsTD/FnlarJSwsLNvC5VlSUlJQqw1vIWf9UlEU5YmOKYQwDpVKRYUKFShfvnyuCzAIUVxZWFg89ZV0FqPekx0/fjzBwcE0adKEZs2aMXfuXJKTk/UzvgwYMIBKlSoREhIC6JYYmzNnDn5+fvpb31OnTiUoKEhfIY87phDCtJiZmRXYF5oQJZFRE3Xv3r25efMm06ZNIyYmhoYNG7Jp0yZ9Z7CoqCiDK+gpU6agUqmYMmUK0dHRuLq6EhQUZDBp/eOOKYQQQhQnsihHDgp6UQ4hhBDiYfnJMzJmSAghhDBhMm4oB1mLnN+4ccPIkQghhCiJsvJLVr55FEnUOcga3tWsWTMjRyKEEKIki42NpWrVqo8sI23UOcjMzOTo0aO4ubllGw6WX4mJidStW5eIiAjKlClTQBGWPFJPeSd1lTdST3kndZU3BVlPWq2W2NhY/Pz8MDd/9DWzJOpClpCQQNmyZbl37x4ODg7GDsdkST3lndRV3kg95Z3UVd4Yq56kM5kQQghhwiRRCyGEECZMEnUhs7KyYvr06QZziYvspJ7yTuoqb6Se8k7qKm+MVU/SRi2EEEKYMLmiFkIIIUyYJGohhBDChEmiFkIIIUyYJOpCtHDhQjw9PbG2tsbf358DBw4YOySTExISQtOmTSlTpgzly5enR48enDlzxthhmbyPPvoIlUrF2LFjjR2KSYqOjubVV1/F2dkZGxsbfHx8OHTokLHDMikajYapU6dSrVo1bGxsqFGjBrNmzUK6LcGOHTsICgqiYsWKqFQqNmzYYPC+oihMmzaNChUqYGNjQ0BAAOfOnSu0eCRRF5LVq1czfvx4pk+fzpEjR/D19SUwMJC4uDhjh2ZStm/fzogRI9i3bx+hoaFkZGTQuXNnkpOTjR2ayTp48CBfffUVDRo0MHYoJunOnTu0bNkSCwsL/vrrLyIiIvjss89wcnIydmgm5eOPP2bRokV88cUXREZG8vHHHzN79mwWLFhg7NCMLjk5GV9fXxYuXJjj+7Nnz2b+/PksXryY/fv3Y2dnR2BgIKmpqYUTkCIKRbNmzZQRI0boX2s0GqVixYpKSEiIEaMyfXFxcQqgbN++3dihmKTExETFy8tLCQ0NVdq2bauMGTPG2CGZnIkTJyqtWrUydhgmr3v37srgwYMNtr3wwgtKv379jBSRaQKU9evX619rtVrF3d1d+eSTT/Tb7t69q1hZWSkrV64slBjkiroQpKenc/jwYQICAvTb1Go1AQEB7N2714iRmb579+4BUK5cOSNHYppGjBhB9+7dDf62hKHffvuNJk2a8PLLL1O+fHn8/PxYunSpscMyOS1atCAsLIyzZ88CcOzYMXbt2kXXrl2NHJlpu3TpEjExMQb/BsuWLYu/v3+hfb/L6lmFID4+Ho1Gg5ubm8F2Nzc3Tp8+baSoTJ9Wq2Xs2LG0bNmS+vXrGzsck7Nq1SqOHDnCwYMHjR2KSbt48SKLFi1i/PjxvPvuuxw8eJDRo0djaWlJcHCwscMzGZMmTSIhIYHatWtjZmaGRqPhgw8+oF+/fsYOzaTFxMQA5Pj9nvVeQZNELUzGiBEjOHnyJLt27TJ2KCbn6tWrjBkzhtDQUKytrY0djknTarU0adKEDz/8EAA/Pz9OnjzJ4sWLJVE/ZM2aNfz000+sWLGCevXqER4eztixY6lYsaLUk4mRW9+FwMXFBTMzM/261lliY2Nxd3c3UlSmbeTIkfzxxx9s3bqVypUrGzsck3P48GHi4uJo1KgR5ubmmJubs337dubPn4+5uTkajcbYIZqMChUqULduXYNtderUISoqykgRmaa3336bSZMm8corr+Dj40P//v0ZN24cISEhxg7NpGV9hxfl97sk6kJgaWlJ48aNCQsL02/TarWEhYXRvHlzI0ZmehRFYeTIkaxfv54tW7ZQrVo1Y4dkkjp27MiJEycIDw/XP5o0aUK/fv0IDw/HzMzM2CGajJYtW2Yb4nf27Fk8PDyMFJFpSklJQa02TAFmZmZotVojRVQ8VKtWDXd3d4Pv94SEBPbv319o3+9y67uQjB8/nuDgYJo0aUKzZs2YO3cuycnJDBo0yNihmZQRI0awYsUKfv31V8qUKaNv4ylbtiw2NjZGjs50lClTJlu7vZ2dHc7OztKe/x/jxo2jRYsWfPjhh/Tq1YsDBw6wZMkSlixZYuzQTEpQUBAffPABVatWpV69ehw9epQ5c+YwePBgY4dmdElJSZw/f17/+tKlS4SHh1OuXDmqVq3K2LFj+d///oeXlxfVqlVj6tSpVKxYkR49ehROQIXSl1woiqIoCxYsUKpWrapYWloqzZo1U/bt22fskEwOkONj2bJlxg7N5MnwrNz9/vvvSv369RUrKyuldu3aypIlS4wdkslJSEhQxowZo1StWlWxtrZWqlevrrz33ntKWlqasUMzuq1bt+b4vRQcHKwoim6I1tSpUxU3NzfFyspK6dixo3LmzJlCi0dWzxJCCCFMmLRRCyGEECZMErUQQghhwiRRCyGEECZMErUQQghhwiRRCyGEECZMErUQQghhwiRRCyGEECZMErUQQghhwiRRCyGKlEqlYsOGDcYOQ4hiQxK1EKXIwIEDUalU2R5dunQxdmhCiFzIohxClDJdunRh2bJlBtusrKyMFI0Q4nHkilqIUsbKygp3d3eDh5OTE6C7Lb1o0SK6du2KjY0N1atX5+effzbY/8SJE3To0AEbGxucnZ0ZOnQoSUlJBmW+/fZb6tWrh5WVFRUqVGDkyJEG78fHx9OzZ09sbW3x8vLit99+0793584d+vXrh6urKzY2Nnh5eWX7YSFEaSKJWghhYOrUqbz44oscO3aMfv368corrxAZGQlAcnIygYGBODk5cfDgQdauXcvmzZsNEvGiRYsYMWIEQ4cO5cSJE/z222/UrFnT4BwzZ86kV69eHD9+nG7dutGvXz9u376tP39ERAR//fUXkZGRLFq0CBcXl6KrACFMTaGtyyWEMDnBwcGKmZmZYmdnZ/D44IMPFEXRLTs6bNgwg338/f2V4cOHK4qiKEuWLFGcnJyUpKQk/fsbN25U1Gq1EhMToyiKolSsWFF57733co0BUKZMmaJ/nZSUpADKX3/9pSiKogQFBSmDBg0qmA8sRAkgbdRClDLt27dn0aJFBtvKlSunf968eXOD95o3b054eDgAkZGR+Pr6Ymdnp3+/ZcuWaLVazpw5g0ql4vr163Ts2PGRMTRo0ED/3M7ODgcHB+Li4gAYPnw4L774IkeOHKFz58706NGDFi1aPNFnFaIkkEQtRCljZ2eX7VZ0QbGxsclTOQsLC4PXKpUKrVYLQNeuXbly5Qp//vknoaGhdOzYkREjRvDpp58WeLxCFAfSRi2EMLBv375sr+vUqQNAnTp1OHbsGMnJyfr3d+/ejVqtplatWpQpUwZPT0/CwsKeKgZXV1eCg4P58ccfmTt3LkuWLHmq4wlRnMkVtRClTFpaGjExMQbbzM3N9R221q5dS5MmTWjVqhU//fQTBw4c4JtvvgGgX79+TJ8+neDgYGbMmMHNmzcZNWoU/fv3x83NDYAZM2YwbNgwypcvT9euXUlMTGT37t2MGjUqT/FNmzaNxo0bU69ePdLS0vjjjz/0PxSEKI0kUQtRymzatIkKFSoYbKtVqxanT58GdD2yV61axZtvvkmFChVYuXIldevWBcDW1pa///6bMWPG0LRpU2xtbXnxxReZM2eO/ljBwcGkpqby+eefM2HCBFxcXHjppZfyHJ+lpSWTJ0/m8uXL2NjY0Lp1a1atWlUAn1yI4kmlKIpi7CCEEKZBpVKxfv16evToYexQhBD/kjZqIYQQwoRJohZCCCFMmLRRCyH0pCVMCNMjV9RCCCGECZNELYQQQpgwSdRCCCGECZNELYQQQpgwSdRCCCGECZNELYQQQpgwSdRCCCGECZNELYQQQpgwSdRCCCGECfs/8kLZJ7PJe6kAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
        "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
        "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
        "\n",
        "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heC7w3u7RKOT",
        "outputId": "c169ef5b-cb8b-4760-b879-f94446d02230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy: 98.46%\n",
            "Validation accuracy: 95.97%\n",
            "Test accuracy: 96.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_review(text,\n",
        "                    model,\n",
        "                    tokenizer,\n",
        "                    device,\n",
        "                    max_length=None,\n",
        "                    pad_token_id=50256):\n",
        "  model.eval()\n",
        "\n",
        "  # input preprocessing\n",
        "  input_ids = tokenizer.encode(text)\n",
        "  supported_context_length = model.position_emb.weight.shape[0]\n",
        "  input_ids = input_ids[:min(max_length, supported_context_length)]\n",
        "  input_ids += [pad_token_id] * (max_length  - len(input_ids))\n",
        "  input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)\n",
        "\n",
        "  # model inference\n",
        "  with torch.no_grad():\n",
        "    logits = model(input_tensor)[:, -1, :] # logits of the last output token\n",
        "  predicted_label = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "  # return the classifier result\n",
        "  return \"spam\" if predicted_label == 1 else \"not spam\""
      ],
      "metadata": {
        "id": "YUKitU9YkJ2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_1 = (\n",
        "    \"You are a winner you have been specially\"\n",
        "    \" selected to receive $1000 cash or a $2000 award.\"\n",
        ")\n",
        "\n",
        "print(classify_review(\n",
        "    text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU-_q1rtkJMR",
        "outputId": "3087d143-4795-4a04-cae3-b457b9447fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_2 = (\n",
        "    \"Hey, just wanted to check if we're still on\"\n",
        "    \" for dinner tonight? Let me know!\"\n",
        ")\n",
        "\n",
        "print(classify_review(\n",
        "    text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IC2F8_FkJIv",
        "outputId": "0be97214-f5b8-4a75-ef70-67a3905da35f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model state_dict\n",
        "torch.save(model.state_dict(), \"demos/gpt2/classification_finetune_gpt2.pth\")"
      ],
      "metadata": {
        "id": "ujc30E3fcUhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model state_dict\n",
        "model = GPT2Model(BASE_CONFIG)\n",
        "num_classes = 2\n",
        "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.load_state_dict(torch.load(\"demos/gpt2/classification_finetune_gpt2.pth\", map_location=device, weights_only=True))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AncvUNvccUhv",
        "outputId": "5deb430b-7b55-4abf-f53b-bc4e2f95a2f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Model(\n",
              "  (token_emb): Embedding(50257, 768)\n",
              "  (position_emb): Embedding(1024, 768)\n",
              "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
              "  (transformer_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (output_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm1): LayerNorm()\n",
              "      (layer_norm2): LayerNorm()\n",
              "      (drop_skip): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.7 Download weights"
      ],
      "metadata": {
        "id": "H3qP3h7aAHAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change into and then zip the gpt2 folder but exclude certain files\n",
        "!cd demos/gpt2 && zip -r ../gpt2.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n",
        "\n",
        "# Download the zipped gpt2 app (if running in Google Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(\"demos/gpt2.zip\")\n",
        "except:\n",
        "    print(\"Not running in Google Colab, can't use google.colab.files.download(), please manually download.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "SWY50AL2AEQT",
        "outputId": "5ed0ad3a-734d-4a9a-85ab-97e48c191620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: baseline_gpt2.pth (deflated 14%)\n",
            "  adding: baseline_loss_plot.pdf (deflated 24%)\n",
            "  adding: classification_finetune_accuracy_gpt2.pdf (deflated 32%)\n",
            "  adding: classification_finetune_gpt2.pth (deflated 16%)\n",
            "  adding: classification_finetune_loss_gpt2.pdf (deflated 31%)\n",
            "  adding: instruction_finetune_gpt2_plot.pdf (deflated 23%)\n",
            "  adding: instruction_finetune_gpt2.pth (deflated 11%)\n",
            "  adding: openai_weight_gpt2.pth (deflated 14%)\n",
            "  adding: stable_training_gpt2.pth (deflated 14%)\n",
            "  adding: stable_training_loss_plot.pdf (deflated 24%)\n",
            "  adding: stable_training_with_lora_gpt2.pth (deflated 14%)\n",
            "  adding: stable_training_with_lora_loss_plot.pdf (deflated 25%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_21df07ce-fb79-4f0f-b2d2-e83be6e45368\", \"gpt2.zip\", 6050071585)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2"
      ],
      "metadata": {
        "id": "2s8S-ff1lvpT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.1 Creating `generate_answer` function"
      ],
      "metadata": {
        "id": "G2jNq4Sq57FH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c97739c-32c1-4753-813a-c021eb88e2d3",
        "id": "w_G-4H_6goRz"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradio version: 5.31.0\n"
          ]
        }
      ],
      "source": [
        "# Import/install Gradio\n",
        "try:\n",
        "    import gradio as gr\n",
        "except:\n",
        "    !pip -q install gradio\n",
        "    import gradio as gr\n",
        "\n",
        "print(f\"Gradio version: {gr.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Create title, description and article strings\n",
        "title = \"GPT2 From Scratch\"\n",
        "description = \"Finetune GPT2 for classification task and instruction task.\"\n",
        "article = \"Learning LLM from scratch\"\n",
        "\n",
        "\n",
        "def greet(name, intensity):\n",
        "    return \"Hello, \" + name + \"!\" * int(intensity)\n",
        "\n",
        "# Create the Gradio demo\n",
        "demo = gr.Interface(fn=greet, # mapping function from input to output\n",
        "                    inputs=[\"text\", \"slider\"],\n",
        "                    outputs=[\"text\"],\n",
        "                    #examples=example_list,\n",
        "                    title=title,\n",
        "                    description=description,\n",
        "                    article=article,\n",
        "                    )\n",
        "\n",
        "# Launch the demo!\n",
        "demo.launch(debug=False, # print errors locally?\n",
        "            share=True) # generate a publically shareable URL?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "outputId": "12505519-3b50-4a62-d9df-4c556062d706",
        "id": "I9U1vse_goR0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://bedca9cdff265bdc25.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://bedca9cdff265bdc25.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ukkd1esmgghc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "def generate_answer(input_prompt,\n",
        "                    model,\n",
        "                    max_new_tokens,\n",
        "                    top_k,\n",
        "                    temperature,\n",
        "                    ):\n",
        "\n",
        "  # start the timer\n",
        "  start_time = time.time()\n",
        "\n",
        "  torch.manual_seed(211)\n",
        "\n",
        "  model = gpt\n",
        "  model.eval()\n",
        "\n",
        "  token_ids = generate_text(\n",
        "      model=model,\n",
        "      input_batch=text_to_token_ids(input_prompt, bpe_tokenizer).to(device),\n",
        "      max_new_tokens=max_new_tokens,\n",
        "      context_size=NEW_CONFIG[\"context_length\"],\n",
        "      top_k=top_k,\n",
        "      temperature=temperature\n",
        "  )\n",
        "\n",
        "  # Calculate the prediction time\n",
        "  pred_time = round(time.time() - start_time)\n",
        "\n",
        "  # print(\"Output text:\\n\", token_ids_to_text(token_ids, bpe_tokenizer))\n",
        "  return token_ids_to_text(token_ids, bpe_tokenizer), pred_time\n",
        "\n"
      ],
      "metadata": {
        "id": "tAOA1t5oRKJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Create title, description and article strings\n",
        "title = \"GPT2 From Scratch\"\n",
        "description = \"Finetune GPT2 for classification task and instruction task.\"\n",
        "article = \"Learning LLM from scratch\"\n",
        "\n",
        "\n",
        "def greet(name, intensity):\n",
        "    return \"Hello, \" + name + \"!\" * int(intensity)\n",
        "\n",
        "# Create the Gradio demo\n",
        "demo = gr.Interface(fn=generate_answer, # mapping function from input to output\n",
        "                    inputs=[\n",
        "                        gr.Textbox(label=\"Input Text\"),\n",
        "                        gr.Dropdown(choices=[\"baseline_gpt2\",\n",
        "                                             \"stable_training_gpt2\",\n",
        "                                             \"stable_training_with_lora_gpt2\",\n",
        "                                             \"openai_weights\",\n",
        "                                             \"instruction_finetune_gpt2\",\n",
        "                                             \"classification_finetune_gpt2\",\n",
        "                                             ],\n",
        "                                    label=\"Model\",\n",
        "                                    multiselect=False),\n",
        "                        gr.Slider(minimum=0, maximum=500, step=1, value=50, label=\"Choose number of generating tokens\"),\n",
        "                        gr.Slider(minimum=0, maximum=50, step=1, value=10, label=\"Choose top k\"),\n",
        "                        gr.Slider(minimum=0.0, maximum=5.0, step=0.1, value=1.5, label=\"Choose temperature\"),\n",
        "                        ],\n",
        "                    outputs=[\n",
        "                        gr.Textbox(label=\"Result\"),\n",
        "                        gr.Number(label=\"Time Taken (s)\"),\n",
        "                        ],\n",
        "                    #examples=example_list,\n",
        "                    title=title,\n",
        "                    description=description,\n",
        "                    article=article,\n",
        "                    )\n",
        "\n",
        "# Launch the demo!\n",
        "demo.launch(debug=True, # print errors locally?\n",
        "            share=True) # generate a publically shareable URL?"
      ],
      "metadata": {
        "id": "op0VuuxPRKHA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "a82b4c65-d74a-42c8-a904-6ddd817fc117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://60d6975fbd644f650c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://60d6975fbd644f650c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://60d6975fbd644f650c.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oo16TPaMRJ_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cKZ5pUvrRJ9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c-_OklY6RJ61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SfoDZNLuRJ4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ubvFm3zPRJ2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "636lU7YDRJz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pd6elrIaRJxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1GFX4oDPRJvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Es9lKjMfWZnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T0UYR5fzWZkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_end_time = time.time()\n",
        "runtime_in_seconds = notebook_end_time - notebook_start_time\n",
        "\n",
        "# format as minutes and seconds\n",
        "minutes, seconds = divmod(runtime_in_seconds, 60)\n",
        "print(f\"Notebook runtime: {int(minutes)} min {seconds:.2f} sec\")"
      ],
      "metadata": {
        "id": "HO6vc8XNWY6r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c75a48a-8cad-4796-8c66-5b4ac26f5759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Notebook runtime: 92 min 43.62 sec\n"
          ]
        }
      ]
    }
  ]
}