{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:09.589159Z",
     "start_time": "2025-06-19T09:31:09.584350Z"
    },
    "id": "sWaTJoLNha-4"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "notebook_start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uelz6sV5vwLU"
   },
   "source": [
    "# Download and Inspect on the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:09.841320Z",
     "start_time": "2025-06-19T09:31:09.624236Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0Os5anWkJxb",
    "outputId": "ca0b7913-1b59-4757-b996-7249b02f88c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../datasets/the-verdict.txt', <http.client.HTTPMessage at 0x24a89d9c7a0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/\"\n",
    "        \"refs/heads/main/ch02/01_main-chapter-code/\"\n",
    "        \"the-verdict.txt\")\n",
    "file_path = \"../datasets/the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:09.883726Z",
     "start_time": "2025-06-19T09:31:09.877090Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5l8VG1r2ywoh",
    "outputId": "5e7e664b-c6ae-4fd9-8b5a-f5ef008a0eaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
      "\n",
      "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it'\n"
     ]
    }
   ],
   "source": [
    "with open(\"../datasets/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  raw_text = f.read()\n",
    "\n",
    "print(\"total number of characters:\", len(raw_text))\n",
    "print(raw_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBD3g65tzfCB"
   },
   "source": [
    "## Split texts into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:09.983798Z",
     "start_time": "2025-06-19T09:31:09.979033Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aqw4Z-d3yfee",
    "outputId": "fa5c4091-a826-4050-fe3d-339f0c949413"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'World', '!', '', ' ', 'Let', \"'\", 's', ' ', 'test', ' ', 'this', ',', '', ' ', '', '--', 'sentence', '.', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "experiment_text = \"Hello, World! Let's test this, --sentence.\"\n",
    "\n",
    "# the splitting delimiters included in the output\n",
    "splitted_word_list = re.split(r'([,.:;?_!\"()\\'\\s]|--)', experiment_text)\n",
    "print(splitted_word_list)\n",
    "\n",
    "\n",
    "# not included\n",
    "# splitted_word_list2 = re.split(r'[,.]|\\s', experiment_text)\n",
    "# print(splitted_word_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:10.123782Z",
     "start_time": "2025-06-19T09:31:10.118943Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUsIDKpd2Oq4",
    "outputId": "32c78513-5f24-48c2-fccb-4b6ae75850df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'World', '!', 'Let', \"'\", 's', 'test', 'this', ',', '--', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "# get rid of white-space\n",
    "splitted_word_list = [word for word in splitted_word_list if word.strip()]\n",
    "print(splitted_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBuj3-h38-XT"
   },
   "source": [
    "Let's apply it to the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:10.305845Z",
     "start_time": "2025-06-19T09:31:10.297929Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XIeq57OX6902",
    "outputId": "e132d7f5-f285-488c-c123-8228ce8e9740"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\'\\s]|--)', raw_text)\n",
    "preprocessed = [word for word in preprocessed if word.strip()]\n",
    "print(preprocessed[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EfSJvDD9WlA"
   },
   "source": [
    "## Convert tokens into token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:10.489058Z",
     "start_time": "2025-06-19T09:31:10.484131Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oQxXvDgO7K_o",
    "outputId": "ed658106-2faa-40c8-d0c5-34b28767e320"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', \"'\", '(', ')', ',', '--', '.', ':', ';', '?', 'A', 'Ah', 'Among', 'And', 'Are', 'Arrt', 'As', 'At', 'Be', 'Begin', 'Burlington', 'But', 'By', 'Carlo', 'Chicago', 'Claude', 'Come', 'Croft', 'Destroyed', 'Devonshire', 'Don', 'Dubarry', 'Emperors', 'Florence', 'For', 'Gallery', 'Gideon', 'Gisburn', 'Gisburns', 'Grafton', 'Greek', 'Grindle', 'Grindles', 'HAD', 'Had', 'Hang', 'Has', 'He', 'Her', 'Hermia', 'His', 'How', 'I', 'If', 'In', 'It', 'Jack', 'Jove', 'Just', 'Lord', 'Made', 'Miss', 'Money', 'Monte', 'Moon-dancers', 'Mr', 'Mrs', 'My', 'Never', 'No', 'Now', 'Nutley', 'Of', 'Oh', 'On', 'Once', 'Only', 'Or', 'Perhaps', 'Poor', 'Professional', 'Renaissance', 'Rickham', 'Riviera', 'Rome', 'Russian', 'Sevres', 'She', 'Stroud', 'Strouds', 'Suddenly', 'That', 'The', 'Then', 'There', 'They', 'This', 'Those', 'Though', 'Thwing', 'Thwings', 'To', 'Usually', 'Venetian', 'Victor', 'Was', 'We', 'Well', 'What', 'When', 'Why', 'Yes', 'You', '_', 'a', 'abdication', 'able', 'about', 'above', 'abruptly', 'absolute', 'absorbed', 'absurdity', 'academic', 'accuse', 'accustomed', 'across', 'activity', 'add', 'added', 'admirers', 'adopted', 'adulation', 'advance', 'aesthetic', 'affect', 'afraid', 'after', 'afterward', 'again', 'ago', 'ah', 'air', 'alive', 'all', 'almost', 'alone', 'along', 'always', 'am', 'amazement', 'amid', 'among', 'amplest', 'amusing', 'an', 'and', 'another', 'answer', 'answered', 'any', 'anything', 'anywhere', 'apparent', 'apparently', 'appearance', 'appeared', 'appointed', 'are', 'arm', 'arm-chair', 'arm-chairs', 'arms', 'art', 'articles', 'artist', 'as', 'aside', 'asked', 'at', 'atmosphere', 'atom', 'attack', 'attention', 'attitude', 'audacities', 'away', 'awful', 'axioms', 'azaleas', 'back', 'background', 'balance', 'balancing', 'balustraded', 'basking', 'bath-rooms', 'be', 'beaming', 'bean-stalk', 'bear', 'beard', 'beauty', 'became', 'because', 'becoming', 'bed', 'been', 'before', 'began', 'begun', 'behind', 'being', 'believed', 'beneath', 'bespoke', 'better', 'between', 'big', 'bits', 'bitterness', 'blocked', 'born', 'borne', 'boudoir', 'bravura', 'break', 'breaking', 'breathing', 'bric-a-brac', 'briefly', 'brings', 'bronzes', 'brought', 'brown', 'brush', 'bull', 'business', 'but', 'buying', 'by', 'called', 'came', 'can', 'canvas', 'canvases', 'cards', 'care', 'career', 'caught', 'central', 'chair', 'chap', 'characteristic', 'charming', 'cheap', 'check', 'cheeks', 'chest', 'chimney-piece', 'chucked', 'cigar', 'cigarette', 'cigars', 'circulation', 'circumstance', 'circus-clown', 'claimed', 'clasping', 'clear', 'cleverer', 'close', 'clue', 'coat', 'collapsed', 'colour', 'come', 'comfortable', 'coming', 'companion', 'compared', 'complex', 'confident', 'congesting', 'conjugal', 'constraint', 'consummate', 'contended', 'continued', 'corner', 'corrected', 'could', 'couldn', 'count', 'countenance', 'couple', 'course', 'covered', 'craft', 'cried', 'crossed', 'crowned', 'crumbled', 'cry', 'cured', 'curiosity', 'curious', 'current', 'curtains', 'd', 'dabble', 'damask', 'dark', 'dashed', 'day', 'days', 'dead', 'deadening', 'dear', 'deep', 'deerhound', 'degree', 'delicate', 'demand', 'denied', 'deploring', 'deprecating', 'deprecatingly', 'desire', 'destroyed', 'destruction', 'desultory', 'detail', 'diagnosis', 'did', 'didn', 'died', 'dim', 'dimmest', 'dingy', 'dining-room', 'disarming', 'discovery', 'discrimination', 'discussion', 'disdain', 'disdained', 'disease', 'disguised', 'display', 'dissatisfied', 'distinguished', 'distract', 'divert', 'do', 'doesn', 'doing', 'domestic', 'don', 'done', 'donkey', 'down', 'dozen', 'dragged', 'drawing-room', 'drawing-rooms', 'drawn', 'dress-closets', 'drew', 'dropped', 'each', 'earth', 'ease', 'easel', 'easy', 'echoed', 'economy', 'effect', 'effects', 'efforts', 'egregious', 'eighteenth-century', 'elbow', 'elegant', 'else', 'embarrassed', 'enabled', 'end', 'endless', 'enjoy', 'enlightenment', 'enough', 'ensuing', 'equally', 'equanimity', 'escape', 'established', 'etching', 'even', 'event', 'ever', 'everlasting', 'every', 'exasperated', 'except', 'excuse', 'excusing', 'existed', 'expected', 'exquisite', 'exquisitely', 'extenuation', 'exterminating', 'extracting', 'eye', 'eyebrows', 'eyes', 'face', 'faces', 'fact', 'faded', 'failed', 'failure', 'fair', 'faith', 'false', 'familiar', 'famille-verte', 'fancy', 'fashionable', 'fate', 'feather', 'feet', 'fell', 'fellow', 'felt', 'few', 'fewer', 'finality', 'find', 'fingers', 'first', 'fit', 'fitting', 'five', 'flash', 'flashed', 'florid', 'flowers', 'fluently', 'flung', 'follow', 'followed', 'fond', 'footstep', 'for', 'forced', 'forcing', 'forehead', 'foreign', 'foreseen', 'forgive', 'forgotten', 'form', 'formed', 'forming', 'forward', 'fostered', 'found', 'foundations', 'fragment', 'fragments', 'frame', 'frames', 'frequently', 'friend', 'from', 'full', 'fullest', 'furiously', 'furrowed', 'garlanded', 'garlands', 'gave', 'genial', 'genius', 'gesture', 'get', 'getting', 'give', 'given', 'glad', 'glanced', 'glimpse', 'gloried', 'glory', 'go', 'going', 'gone', 'good', 'good-breeding', 'good-humoured', 'got', 'grace', 'gradually', 'gray', 'grayish', 'great', 'greatest', 'greatness', 'grew', 'groping', 'growing', 'had', 'hadn', 'hair', 'half', 'half-light', 'half-mechanically', 'hall', 'hand', 'hands', 'handsome', 'hanging', 'happen', 'happened', 'hard', 'hardly', 'has', 'have', 'haven', 'having', 'he', 'head', 'hear', 'heard', 'heart', 'height', 'her', 'here', 'hermit', 'herself', 'hesitations', 'hide', 'high', 'him', 'himself', 'hint', 'his', 'history', 'holding', 'home', 'honour', 'hooded', 'hostess', 'hot-house', 'hour', 'hours', 'house', 'how', 'hung', 'husband', 'idea', 'idle', 'idling', 'if', 'immediately', 'in', 'incense', 'indifferent', 'inevitable', 'inevitably', 'inflexible', 'insensible', 'insignificant', 'instinctively', 'instructive', 'interesting', 'into', 'ironic', 'irony', 'irrelevance', 'irrevocable', 'is', 'it', 'its', 'itself', 'jardiniere', 'jealousy', 'just', 'keep', 'kept', 'kind', 'knees', 'knew', 'know', 'known', 'laid', 'lair', 'landing', 'language', 'last', 'late', 'later', 'latter', 'laugh', 'laughed', 'lay', 'leading', 'lean', 'learned', 'least', 'leathery', 'leave', 'led', 'left', 'leisure', 'lends', 'lent', 'let', 'lies', 'life', 'life-likeness', 'lift', 'lifted', 'light', 'lightly', 'like', 'liked', 'line', 'lines', 'lingered', 'lips', 'lit', 'little', 'live', 'll', 'loathing', 'long', 'longed', 'longer', 'look', 'looked', 'looking', 'lose', 'loss', 'lounging', 'lovely', 'lucky', 'lump', 'luncheon-table', 'luxury', 'lying', 'made', 'make', 'man', 'manage', 'managed', 'mantel-piece', 'marble', 'married', 'may', 'me', 'meant', 'mediocrity', 'medium', 'mentioned', 'mere', 'merely', 'met', 'might', 'mighty', 'millionaire', 'mine', 'minute', 'minutes', 'mirrors', 'modest', 'modesty', 'moment', 'money', 'monumental', 'mood', 'morbidly', 'more', 'most', 'mourn', 'mourned', 'moustache', 'moved', 'much', 'muddling', 'multiplied', 'murmur', 'muscles', 'must', 'my', 'myself', 'mysterious', 'naive', 'near', 'nearly', 'negatived', 'nervous', 'nervousness', 'neutral', 'never', 'next', 'no', 'none', 'not', 'note', 'nothing', 'now', 'nymphs', 'oak', 'obituary', 'object', 'objects', 'occurred', 'oddly', 'of', 'off', 'often', 'oh', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'open', 'or', 'other', 'our', 'ourselves', 'out', 'outline', 'oval', 'over', 'own', 'packed', 'paid', 'paint', 'painted', 'painter', 'painting', 'pale', 'paled', 'palm-trees', 'panel', 'panelling', 'pardonable', 'pardoned', 'part', 'passages', 'passing', 'past', 'pastels', 'pathos', 'patient', 'people', 'perceptible', 'perfect', 'persistence', 'persuasively', 'phrase', 'picture', 'pictures', 'pines', 'pink', 'place', 'placed', 'plain', 'platitudes', 'pleased', 'pockets', 'point', 'poised', 'poor', 'portrait', 'posing', 'possessed', 'poverty', 'predicted', 'preliminary', 'presenting', 'prestidigitation', 'pretty', 'previous', 'price', 'pride', 'princely', 'prism', 'problem', 'proclaiming', 'prodigious', 'profusion', 'protest', 'prove', 'public', 'purblind', 'purely', 'pushed', 'put', 'qualities', 'quality', 'queerly', 'question', 'quickly', 'quietly', 'quite', 'quote', 'rain', 'raised', 'random', 'rather', 're', 'real', 'really', 'reared', 'reason', 'reassurance', 'recovering', 'recreated', 'reflected', 'reflection', 'regrets', 'relatively', 'remained', 'remember', 'reminded', 'repeating', 'represented', 'reproduction', 'resented', 'resolve', 'resources', 'rest', 'rich', 'ridiculous', 'robbed', 'romantic', 'room', 'rose', 'rs', 'rule', 'run', 's', 'said', 'same', 'satisfaction', 'savour', 'saw', 'say', 'saying', 'says', 'scorn', 'scornful', 'secret', 'see', 'seemed', 'seen', 'self-confident', 'send', 'sensation', 'sensitive', 'sent', 'serious', 'set', 'sex', 'shade', 'shaking', 'shall', 'she', 'shirked', 'short', 'should', 'shoulder', 'shoulders', 'show', 'showed', 'showy', 'shrug', 'shrugged', 'sight', 'sign', 'silent', 'silver', 'similar', 'simpleton', 'simplifications', 'simply', 'since', 'single', 'sitter', 'sitters', 'sketch', 'skill', 'slight', 'slightly', 'slowly', 'small', 'smile', 'smiling', 'sneer', 'so', 'solace', 'some', 'somebody', 'something', 'spacious', 'spaniel', 'speaking-tubes', 'speculations', 'spite', 'splash', 'square', 'stairs', 'stammer', 'stand', 'standing', 'started', 'stay', 'still', 'stocked', 'stood', 'stopped', 'stopping', 'straddling', 'straight', 'strain', 'straining', 'strange', 'straw', 'stream', 'stroke', 'strokes', 'strolled', 'strongest', 'strongly', 'struck', 'studio', 'stuff', 'subject', 'substantial', 'suburban', 'such', 'suddenly', 'suffered', 'sugar', 'suggested', 'sunburn', 'sunburnt', 'sunlit', 'superb', 'sure', 'surest', 'surface', 'surprise', 'surprised', 'surrounded', 'suspected', 'sweetly', 'sweetness', 'swelling', 'swept', 'swum', 't', 'table', 'take', 'taken', 'talking', 'tea', 'tears', 'technicalities', 'technique', 'tell', 'tells', 'tempting', 'terra-cotta', 'terrace', 'terraces', 'terribly', 'than', 'that', 'the', 'their', 'them', 'then', 'there', 'therefore', 'they', 'thin', 'thing', 'things', 'think', 'this', 'thither', 'those', 'though', 'thought', 'three', 'threshold', 'threw', 'through', 'throwing', 'tie', 'till', 'time', 'timorously', 'tinge', 'tips', 'tired', 'to', 'told', 'tone', 'tones', 'too', 'took', 'tottering', 'touched', 'toward', 'trace', 'trade', 'transmute', 'traps', 'travelled', 'tribute', 'tributes', 'tricks', 'tried', 'trouser-presses', 'true', 'truth', 'turned', 'twenty', 'twenty-four', 'twice', 'twirling', 'unaccountable', 'uncertain', 'under', 'underlay', 'underneath', 'understand', 'unexpected', 'untouched', 'unusual', 'up', 'up-stream', 'upon', 'upset', 'upstairs', 'us', 'used', 'usual', 'value', 'varnishing', 'vases', 've', 'veins', 'velveteen', 'very', 'villa', 'vindicated', 'virtuosity', 'vista', 'vocation', 'voice', 'wall', 'wander', 'want', 'wanted', 'wants', 'was', 'wasn', 'watched', 'watching', 'water-colour', 'waves', 'way', 'weekly', 'weeks', 'welcome', 'went', 'were', 'what', 'when', 'whenever', 'where', 'which', 'while', 'white', 'white-panelled', 'who', 'whole', 'whom', 'why', 'wide', 'widow', 'wife', 'wild', 'wincing', 'window-curtains', 'wish', 'with', 'without', 'wits', 'woman', 'women', 'won', 'wonder', 'wondered', 'word', 'work', 'working', 'worth', 'would', 'wouldn', 'year', 'years', 'yellow', 'yet', 'you', 'younger', 'your', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "set_of_all_words = sorted(set(preprocessed))\n",
    "print(set_of_all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:10.598632Z",
     "start_time": "2025-06-19T09:31:10.594338Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-qhp7tYY9lny",
    "outputId": "c333f566-bf42-489c-e3e6-2ee47c2837f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(set_of_all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:10.780827Z",
     "start_time": "2025-06-19T09:31:10.773811Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uYQcGAmZ98Dq",
    "outputId": "a841932e-1370-47fc-d673-cc39311f76fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "# print some vocab\n",
    "vocab = {token:integer for integer, token in enumerate(set_of_all_words)}\n",
    "\n",
    "for i, token in enumerate(vocab.items()):\n",
    "  print(token)\n",
    "  if i == 50:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SuF-ZVbJ_E3X"
   },
   "source": [
    "## Implement a Complete Simple Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:10.919749Z",
     "start_time": "2025-06-19T09:31:10.911555Z"
    },
    "id": "46O-l6y1-2cj"
   },
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "  def __init__(self, vocab):\n",
    "    self.word_to_id = vocab\n",
    "    self.id_to_word = {id:word for word, id in vocab.items()}\n",
    "\n",
    "  # word --> id (tokenize)\n",
    "  def encode(self, text):\n",
    "    preprocessed = re.split(r'([,.:;?_!\"()\\'\\s]|--)', text)\n",
    "    preprocessed = [word for word in preprocessed if word.strip()]\n",
    "    id_list = [self.word_to_id[word] for word in preprocessed]\n",
    "    return id_list\n",
    "\n",
    "  # id --> word (detokenize)\n",
    "  def decode(self, id_list):\n",
    "    word_list = [self.id_to_word[id] for id in id_list]\n",
    "    text_with_white_space = \" \".join(word_list)\n",
    "    # remove space before punctuations\n",
    "    text = re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text_with_white_space)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:11.000486Z",
     "start_time": "2025-06-19T09:31:10.994734Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W7oXLHf1BLLS",
    "outputId": "b890c51c-6dc7-4b9a-bb83-2ff11b9de4e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "simple_tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "id_list = simple_tokenizer.encode(text)\n",
    "print(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:11.135201Z",
     "start_time": "2025-06-19T09:31:11.129049Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wcueE41RBtdV",
    "outputId": "b1428fbd-1afd-4715-a4fb-a5f5245f84f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "print(simple_tokenizer.decode(id_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qj2zzmxkEsDR"
   },
   "source": [
    "Our simple tokenizer works, but it cannot work on the data out of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2W7IC1NqE5O1"
   },
   "source": [
    "## Adding Special Context Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:11.290969Z",
     "start_time": "2025-06-19T09:31:11.278034Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxBfNLw4E75U",
    "outputId": "296daff1-fc66-4d48-bee8-2b3558224b99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n",
      "('year', 1122)\n",
      "('years', 1123)\n",
      "('yellow', 1124)\n",
      "('yet', 1125)\n",
      "('you', 1126)\n",
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|unk|>', 1130)\n",
      "('<|endoftext|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "list_of_all_words = sorted(list(set(preprocessed)))\n",
    "list_of_all_words.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
    "vocab = {word:id for id, word in enumerate(list_of_all_words)}\n",
    "\n",
    "print(len(vocab.items()))\n",
    "\n",
    "for i, word in enumerate(list(vocab.items())[-10:]):\n",
    "  print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ftebhi8mNPG"
   },
   "source": [
    "## Implement a Simple Tokenizer That Handles Unknown Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:11.470042Z",
     "start_time": "2025-06-19T09:31:11.451765Z"
    },
    "id": "sdU2aFVKk2bg"
   },
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "  def __init__(self, vocab):\n",
    "    self.word_to_id = vocab\n",
    "    self.id_to_word = {id:word for word, id in vocab.items()}\n",
    "\n",
    "  # word --> id (tokenizer)\n",
    "  def encode(self, text):\n",
    "    preprocessed = re.split(r'([,.:;?_!\"()\\'\\s]|--)', text)\n",
    "    preprocessed = [word for word in preprocessed if word.strip()]\n",
    "    # handle unknown words\n",
    "    word_list = [word if word in self.word_to_id\n",
    "              else \"<|unk|>\" for word in preprocessed]\n",
    "    id_list = [self.word_to_id[word] for word in word_list]\n",
    "    return id_list\n",
    "\n",
    "  # id --> word (detokenizer)\n",
    "  def decode(self, id_list):\n",
    "    word_list = [self.id_to_word[id] for id in id_list]\n",
    "    text_with_white_space = \" \".join(word_list)\n",
    "    # remove space before punctuations\n",
    "    text = re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text_with_white_space)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeXnWFITpYhb"
   },
   "source": [
    "Let's test our new tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:11.522503Z",
     "start_time": "2025-06-19T09:31:11.515711Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5oCtkcOmdMD",
    "outputId": "69a0f293-4314-4bfd-fa46-cde7ed5f0870"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"In the midst of winter, I found, within me,\n",
      " an invincible summer\" said Albert Camus <|endoftext|> Happy Hunger Games, and may the odds be ever in your favor.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"\"\"\"In the midst of winter, I found, within me,\n",
    " an invincible summer\" said Albert Camus\"\"\"\n",
    "text2 = \"\"\"Happy Hunger Games, and may the odds be ever in your favor.\"\"\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:11.644227Z",
     "start_time": "2025-06-19T09:31:11.639370Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l3AXg9dvqJmt",
    "outputId": "67e66894-b0a4-4c9f-d87b-e6c9dd6dd255"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 55, 988, 1130, 722, 1130, 5, 53, 469, 5, 1130, 663, 5, 156, 1130, 1130, 1, 851, 1130, 1130, 1131, 1130, 1130, 1130, 5, 157, 662, 988, 1130, 198, 401, 568, 1128, 1130, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizerv2 = SimpleTokenizerV2(vocab)\n",
    "print(tokenizerv2.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:11.786749Z",
     "start_time": "2025-06-19T09:31:11.779541Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_KdFxc0Eqfnc",
    "outputId": "66e18501-0063-46db-e70e-9a4684b535b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" In the <|unk|> of <|unk|>, I found, <|unk|> me, an <|unk|> <|unk|>\" said <|unk|> <|unk|> <|endoftext|> <|unk|> <|unk|> <|unk|>, and may the <|unk|> be ever in your <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizerv2.decode(tokenizerv2.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUnxGBrBsP70"
   },
   "source": [
    "## Byte Pair Encoding (PBE)\n",
    "\n",
    "To handle unknown words at ease, we tokenize subword instead:\n",
    "\n",
    "[tiktoken doc](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:11.903634Z",
     "start_time": "2025-06-19T09:31:11.894694Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Osl5tnXAqqDl",
    "outputId": "871c5b2a-f7d8-4d64-c92a-61af6c8ec87c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version:  0.9.0\n",
      "<Encoding 'cl100k_base'>\n",
      "100277\n",
      "['gpt2', 'r50k_base', 'p50k_base', 'p50k_edit', 'cl100k_base', 'o200k_base']\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version: \", version(\"tiktoken\"))\n",
    "print(tiktoken.encoding_for_model(\"gpt-3.5-turbo\"))\n",
    "print(tiktoken.encoding_for_model(\"gpt-3.5-turbo\").n_vocab)\n",
    "print(tiktoken.list_encoding_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:13.296138Z",
     "start_time": "2025-06-19T09:31:13.288908Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jB-xqiXZskys",
    "outputId": "0d007787-d321-425f-81a1-abc2019cbe4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n",
      "\"In the midst of winter, I found, within me,\n",
      " an invincible summer\" said Albert Camus <|endoftext|> Happy Hunger Games, and may the odds be ever in your favor.\n"
     ]
    }
   ],
   "source": [
    "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(bpe_tokenizer.n_vocab)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:13.454371Z",
     "start_time": "2025-06-19T09:31:13.449908Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3fvAn23tvoD_",
    "outputId": "ce4638d0-ed1d-40d3-a050-44792eaca182"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 818, 262, 15925, 286, 7374, 11, 314, 1043, 11, 1626, 502, 11, 198, 281, 46038, 3931, 1, 531, 9966, 7298, 385, 220, 50256, 14628, 32367, 5776, 11, 290, 743, 262, 10402, 307, 1683, 287, 534, 2661, 13]\n"
     ]
    }
   ],
   "source": [
    "id_list = bpe_tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:13.630520Z",
     "start_time": "2025-06-19T09:31:13.623004Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sT_eWsEgv5So",
    "outputId": "860d4992-0ba7-4ba1-a89f-a007f3ef5958"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"In the midst of winter, I found, within me,\n",
      " an invincible summer\" said Albert Camus <|endoftext|> Happy Hunger Games, and may the odds be ever in your favor.\n"
     ]
    }
   ],
   "source": [
    "decoded_text = bpe_tokenizer.decode(id_list)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:13.755539Z",
     "start_time": "2025-06-19T09:31:13.750708Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qK0weqTPwAS1",
    "outputId": "9127ee42-12af-407b-a6ff-486bf5b21da9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\"'\n",
      "b'In'\n",
      "b' the'\n",
      "b' midst'\n",
      "b' of'\n",
      "b' winter'\n",
      "b','\n",
      "b' I'\n",
      "b' found'\n",
      "b','\n",
      "b' within'\n",
      "b' me'\n",
      "b','\n",
      "b'\\n'\n",
      "b' an'\n",
      "b' invincible'\n",
      "b' summer'\n",
      "b'\"'\n",
      "b' said'\n",
      "b' Albert'\n",
      "b' Cam'\n",
      "b'us'\n",
      "b' '\n",
      "b'<|endoftext|>'\n",
      "b' Happy'\n",
      "b' Hunger'\n",
      "b' Games'\n",
      "b','\n",
      "b' and'\n",
      "b' may'\n",
      "b' the'\n",
      "b' odds'\n",
      "b' be'\n",
      "b' ever'\n",
      "b' in'\n",
      "b' your'\n",
      "b' favor'\n",
      "b'.'\n"
     ]
    }
   ],
   "source": [
    "# let's decode into byte\n",
    "decoded_text_byte = [bpe_tokenizer.decode_single_token_bytes(token)\n",
    "                    for token in id_list]\n",
    "for token in decoded_text_byte:\n",
    "  #print(token.decode(\"utf-8\"))\n",
    "  print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BaUI0al00fu"
   },
   "source": [
    "## Comparing Trained LLM Tokenizers From HuggingFace\n",
    "\n",
    "[color combination](https://coolors.co/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:13.880185Z",
     "start_time": "2025-06-19T09:31:13.869967Z"
    },
    "id": "XureqCiL5pMx"
   },
   "outputs": [],
   "source": [
    "# recommended\n",
    "# colors_list = [ #'R;B;G'\n",
    "#     '102;194;165', '252;141;98', '141;160;203',\n",
    "#     '231;138;195', '166;216;84', '255;217;47'\n",
    "# ]\n",
    "\n",
    "\n",
    "# colors_list = [ #'R;B;G'\n",
    "#     '244;241;222', '224;122;95', '61;64;91',\n",
    "#     '129;178;154', '242;204;143',\n",
    "# ]\n",
    "\n",
    "\n",
    "# colors_list = [ #'R;B;G'\n",
    "#     '255;190;11', '251;86;7', '255;0;110',\n",
    "#     '131;56;236', '58;134;255',\n",
    "# ]\n",
    "\n",
    "\n",
    "# # recommended\n",
    "# colors_list = [ #'R;B;G'\n",
    "#     '251;248;204', '253;228;207', '255;207;210',\n",
    "#     '241;192;232', '207;186;240', '163;196;243',\n",
    "#     '144;219;244', '142;236;245', '152;245;225',\n",
    "#     '185;251;192',\n",
    "# ]\n",
    "\n",
    "# recommended\n",
    "colors_list = [ #'R;B;G'\n",
    "    '255;173;173', '255;214;165', '253;255;182',\n",
    "    '253;255;182', '202;255;191', '155;246;255',\n",
    "    '160;196;255', '189;178;255', '255;198;225',\n",
    "    '255;255;252',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:13.985413Z",
     "start_time": "2025-06-19T09:31:13.979464Z"
    },
    "id": "_Wkzg3P10yet"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def show_tokens(sentence, tokenizer_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    token_ids = tokenizer(sentence).input_ids\n",
    "    for idx, t in enumerate(token_ids):\n",
    "        print(\n",
    "            f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +\n",
    "            tokenizer.decode(t) +\n",
    "            '\\x1b[0m',\n",
    "            end=' '\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:14.081235Z",
     "start_time": "2025-06-19T09:31:14.077067Z"
    },
    "id": "_XJ_65dq1i2D"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "English and CAPITALIZATION\n",
    "ðŸŽµ å¤§æ¨¡åž‹èªžè¨€\n",
    "show_tokens False None elif == >= else: two tabs:\"    \" Three tabs: \"       \"\n",
    "12.0*50=600\n",
    "\n",
    "def greet():\n",
    "  while True:\n",
    "    print(\"Hello World!\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:15.022204Z",
     "start_time": "2025-06-19T09:31:14.152277Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8sDygGGS2BfG",
    "outputId": "69cf3ce4-a19f-46f7-bbb3-76e374e58927"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;48;2;255;173;173m[CLS]\u001b[0m \u001b[0;30;48;2;255;214;165menglish\u001b[0m \u001b[0;30;48;2;253;255;182mand\u001b[0m \u001b[0;30;48;2;253;255;182mcapital\u001b[0m \u001b[0;30;48;2;202;255;191m##ization\u001b[0m \u001b[0;30;48;2;155;246;255m[UNK]\u001b[0m \u001b[0;30;48;2;160;196;255må¤§\u001b[0m \u001b[0;30;48;2;189;178;255m[UNK]\u001b[0m \u001b[0;30;48;2;255;198;225m[UNK]\u001b[0m \u001b[0;30;48;2;255;255;252mèªž\u001b[0m \u001b[0;30;48;2;255;173;173m[UNK]\u001b[0m \u001b[0;30;48;2;255;214;165mshow\u001b[0m \u001b[0;30;48;2;253;255;182m_\u001b[0m \u001b[0;30;48;2;253;255;182mtoken\u001b[0m \u001b[0;30;48;2;202;255;191m##s\u001b[0m \u001b[0;30;48;2;155;246;255mfalse\u001b[0m \u001b[0;30;48;2;160;196;255mnone\u001b[0m \u001b[0;30;48;2;189;178;255meli\u001b[0m \u001b[0;30;48;2;255;198;225m##f\u001b[0m \u001b[0;30;48;2;255;255;252m=\u001b[0m \u001b[0;30;48;2;255;173;173m=\u001b[0m \u001b[0;30;48;2;255;214;165m>\u001b[0m \u001b[0;30;48;2;253;255;182m=\u001b[0m \u001b[0;30;48;2;253;255;182melse\u001b[0m \u001b[0;30;48;2;202;255;191m:\u001b[0m \u001b[0;30;48;2;155;246;255mtwo\u001b[0m \u001b[0;30;48;2;160;196;255mtab\u001b[0m \u001b[0;30;48;2;189;178;255m##s\u001b[0m \u001b[0;30;48;2;255;198;225m:\u001b[0m \u001b[0;30;48;2;255;255;252m\"\u001b[0m \u001b[0;30;48;2;255;173;173m\"\u001b[0m \u001b[0;30;48;2;255;214;165mthree\u001b[0m \u001b[0;30;48;2;253;255;182mtab\u001b[0m \u001b[0;30;48;2;253;255;182m##s\u001b[0m \u001b[0;30;48;2;202;255;191m:\u001b[0m \u001b[0;30;48;2;155;246;255m\"\u001b[0m \u001b[0;30;48;2;160;196;255m\"\u001b[0m \u001b[0;30;48;2;189;178;255m12\u001b[0m \u001b[0;30;48;2;255;198;225m.\u001b[0m \u001b[0;30;48;2;255;255;252m0\u001b[0m \u001b[0;30;48;2;255;173;173m*\u001b[0m \u001b[0;30;48;2;255;214;165m50\u001b[0m \u001b[0;30;48;2;253;255;182m=\u001b[0m \u001b[0;30;48;2;253;255;182m600\u001b[0m \u001b[0;30;48;2;202;255;191mdef\u001b[0m \u001b[0;30;48;2;155;246;255mgreet\u001b[0m \u001b[0;30;48;2;160;196;255m(\u001b[0m \u001b[0;30;48;2;189;178;255m)\u001b[0m \u001b[0;30;48;2;255;198;225m:\u001b[0m \u001b[0;30;48;2;255;255;252mwhile\u001b[0m \u001b[0;30;48;2;255;173;173mtrue\u001b[0m \u001b[0;30;48;2;255;214;165m:\u001b[0m \u001b[0;30;48;2;253;255;182mprint\u001b[0m \u001b[0;30;48;2;253;255;182m(\u001b[0m \u001b[0;30;48;2;202;255;191m\"\u001b[0m \u001b[0;30;48;2;155;246;255mhello\u001b[0m \u001b[0;30;48;2;160;196;255mworld\u001b[0m \u001b[0;30;48;2;189;178;255m!\u001b[0m \u001b[0;30;48;2;255;198;225m\"\u001b[0m \u001b[0;30;48;2;255;255;252m)\u001b[0m \u001b[0;30;48;2;255;173;173m[SEP]\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:15.905320Z",
     "start_time": "2025-06-19T09:31:15.078574Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8pUHv3CP2hkk",
    "outputId": "ce937e10-6eda-4a60-d9f7-7729e6860088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;48;2;255;173;173m[CLS]\u001b[0m \u001b[0;30;48;2;255;214;165mEnglish\u001b[0m \u001b[0;30;48;2;253;255;182mand\u001b[0m \u001b[0;30;48;2;253;255;182mCA\u001b[0m \u001b[0;30;48;2;202;255;191m##PI\u001b[0m \u001b[0;30;48;2;155;246;255m##TA\u001b[0m \u001b[0;30;48;2;160;196;255m##L\u001b[0m \u001b[0;30;48;2;189;178;255m##I\u001b[0m \u001b[0;30;48;2;255;198;225m##Z\u001b[0m \u001b[0;30;48;2;255;255;252m##AT\u001b[0m \u001b[0;30;48;2;255;173;173m##ION\u001b[0m \u001b[0;30;48;2;255;214;165m[UNK]\u001b[0m \u001b[0;30;48;2;253;255;182må¤§\u001b[0m \u001b[0;30;48;2;253;255;182m[UNK]\u001b[0m \u001b[0;30;48;2;202;255;191m[UNK]\u001b[0m \u001b[0;30;48;2;155;246;255m[UNK]\u001b[0m \u001b[0;30;48;2;160;196;255m[UNK]\u001b[0m \u001b[0;30;48;2;189;178;255mshow\u001b[0m \u001b[0;30;48;2;255;198;225m_\u001b[0m \u001b[0;30;48;2;255;255;252mtoken\u001b[0m \u001b[0;30;48;2;255;173;173m##s\u001b[0m \u001b[0;30;48;2;255;214;165mF\u001b[0m \u001b[0;30;48;2;253;255;182m##als\u001b[0m \u001b[0;30;48;2;253;255;182m##e\u001b[0m \u001b[0;30;48;2;202;255;191mNone\u001b[0m \u001b[0;30;48;2;155;246;255mel\u001b[0m \u001b[0;30;48;2;160;196;255m##if\u001b[0m \u001b[0;30;48;2;189;178;255m=\u001b[0m \u001b[0;30;48;2;255;198;225m=\u001b[0m \u001b[0;30;48;2;255;255;252m>\u001b[0m \u001b[0;30;48;2;255;173;173m=\u001b[0m \u001b[0;30;48;2;255;214;165melse\u001b[0m \u001b[0;30;48;2;253;255;182m:\u001b[0m \u001b[0;30;48;2;253;255;182mtwo\u001b[0m \u001b[0;30;48;2;202;255;191mta\u001b[0m \u001b[0;30;48;2;155;246;255m##bs\u001b[0m \u001b[0;30;48;2;160;196;255m:\u001b[0m \u001b[0;30;48;2;189;178;255m\"\u001b[0m \u001b[0;30;48;2;255;198;225m\"\u001b[0m \u001b[0;30;48;2;255;255;252mThree\u001b[0m \u001b[0;30;48;2;255;173;173mta\u001b[0m \u001b[0;30;48;2;255;214;165m##bs\u001b[0m \u001b[0;30;48;2;253;255;182m:\u001b[0m \u001b[0;30;48;2;253;255;182m\"\u001b[0m \u001b[0;30;48;2;202;255;191m\"\u001b[0m \u001b[0;30;48;2;155;246;255m12\u001b[0m \u001b[0;30;48;2;160;196;255m.\u001b[0m \u001b[0;30;48;2;189;178;255m0\u001b[0m \u001b[0;30;48;2;255;198;225m*\u001b[0m \u001b[0;30;48;2;255;255;252m50\u001b[0m \u001b[0;30;48;2;255;173;173m=\u001b[0m \u001b[0;30;48;2;255;214;165m600\u001b[0m \u001b[0;30;48;2;253;255;182mdef\u001b[0m \u001b[0;30;48;2;253;255;182mgreet\u001b[0m \u001b[0;30;48;2;202;255;191m(\u001b[0m \u001b[0;30;48;2;155;246;255m)\u001b[0m \u001b[0;30;48;2;160;196;255m:\u001b[0m \u001b[0;30;48;2;189;178;255mwhile\u001b[0m \u001b[0;30;48;2;255;198;225mTrue\u001b[0m \u001b[0;30;48;2;255;255;252m:\u001b[0m \u001b[0;30;48;2;255;173;173mprint\u001b[0m \u001b[0;30;48;2;255;214;165m(\u001b[0m \u001b[0;30;48;2;253;255;182m\"\u001b[0m \u001b[0;30;48;2;253;255;182mHello\u001b[0m \u001b[0;30;48;2;202;255;191mWorld\u001b[0m \u001b[0;30;48;2;155;246;255m!\u001b[0m \u001b[0;30;48;2;160;196;255m\"\u001b[0m \u001b[0;30;48;2;189;178;255m)\u001b[0m \u001b[0;30;48;2;255;198;225m[SEP]\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:16.849853Z",
     "start_time": "2025-06-19T09:31:15.970285Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CWRLWBC2nj8",
    "outputId": "64326ff6-30cc-4674-d83f-50f6b7e23555"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;48;2;255;173;173m\n",
      "\u001b[0m \u001b[0;30;48;2;255;214;165mEnglish\u001b[0m \u001b[0;30;48;2;253;255;182m and\u001b[0m \u001b[0;30;48;2;253;255;182m CAP\u001b[0m \u001b[0;30;48;2;202;255;191mITAL\u001b[0m \u001b[0;30;48;2;155;246;255mIZ\u001b[0m \u001b[0;30;48;2;160;196;255mATION\u001b[0m \u001b[0;30;48;2;189;178;255m\n",
      "\u001b[0m \u001b[0;30;48;2;255;198;225mï¿½\u001b[0m \u001b[0;30;48;2;255;255;252mï¿½\u001b[0m \u001b[0;30;48;2;255;173;173mï¿½\u001b[0m \u001b[0;30;48;2;255;214;165m ï¿½\u001b[0m \u001b[0;30;48;2;253;255;182mï¿½\u001b[0m \u001b[0;30;48;2;253;255;182mï¿½\u001b[0m \u001b[0;30;48;2;202;255;191mï¿½\u001b[0m \u001b[0;30;48;2;155;246;255mï¿½\u001b[0m \u001b[0;30;48;2;160;196;255mï¿½\u001b[0m \u001b[0;30;48;2;189;178;255mï¿½\u001b[0m \u001b[0;30;48;2;255;198;225mï¿½\u001b[0m \u001b[0;30;48;2;255;255;252mï¿½\u001b[0m \u001b[0;30;48;2;255;173;173mï¿½\u001b[0m \u001b[0;30;48;2;255;214;165mï¿½\u001b[0m \u001b[0;30;48;2;253;255;182mï¿½\u001b[0m \u001b[0;30;48;2;253;255;182mï¿½\u001b[0m \u001b[0;30;48;2;202;255;191m\n",
      "\u001b[0m \u001b[0;30;48;2;155;246;255mshow\u001b[0m \u001b[0;30;48;2;160;196;255m_\u001b[0m \u001b[0;30;48;2;189;178;255mt\u001b[0m \u001b[0;30;48;2;255;198;225mok\u001b[0m \u001b[0;30;48;2;255;255;252mens\u001b[0m \u001b[0;30;48;2;255;173;173m False\u001b[0m \u001b[0;30;48;2;255;214;165m None\u001b[0m \u001b[0;30;48;2;253;255;182m el\u001b[0m \u001b[0;30;48;2;253;255;182mif\u001b[0m \u001b[0;30;48;2;202;255;191m ==\u001b[0m \u001b[0;30;48;2;155;246;255m >=\u001b[0m \u001b[0;30;48;2;160;196;255m else\u001b[0m \u001b[0;30;48;2;189;178;255m:\u001b[0m \u001b[0;30;48;2;255;198;225m two\u001b[0m \u001b[0;30;48;2;255;255;252m tabs\u001b[0m \u001b[0;30;48;2;255;173;173m:\"\u001b[0m \u001b[0;30;48;2;255;214;165m \u001b[0m \u001b[0;30;48;2;253;255;182m \u001b[0m \u001b[0;30;48;2;253;255;182m \u001b[0m \u001b[0;30;48;2;202;255;191m \"\u001b[0m \u001b[0;30;48;2;155;246;255m Three\u001b[0m \u001b[0;30;48;2;160;196;255m tabs\u001b[0m \u001b[0;30;48;2;189;178;255m:\u001b[0m \u001b[0;30;48;2;255;198;225m \"\u001b[0m \u001b[0;30;48;2;255;255;252m \u001b[0m \u001b[0;30;48;2;255;173;173m \u001b[0m \u001b[0;30;48;2;255;214;165m \u001b[0m \u001b[0;30;48;2;253;255;182m \u001b[0m \u001b[0;30;48;2;253;255;182m \u001b[0m \u001b[0;30;48;2;202;255;191m \u001b[0m \u001b[0;30;48;2;155;246;255m \"\u001b[0m \u001b[0;30;48;2;160;196;255m\n",
      "\u001b[0m \u001b[0;30;48;2;189;178;255m12\u001b[0m \u001b[0;30;48;2;255;198;225m.\u001b[0m \u001b[0;30;48;2;255;255;252m0\u001b[0m \u001b[0;30;48;2;255;173;173m*\u001b[0m \u001b[0;30;48;2;255;214;165m50\u001b[0m \u001b[0;30;48;2;253;255;182m=\u001b[0m \u001b[0;30;48;2;253;255;182m600\u001b[0m \u001b[0;30;48;2;202;255;191m\n",
      "\u001b[0m \u001b[0;30;48;2;155;246;255m\n",
      "\u001b[0m \u001b[0;30;48;2;160;196;255mdef\u001b[0m \u001b[0;30;48;2;189;178;255m greet\u001b[0m \u001b[0;30;48;2;255;198;225m():\u001b[0m \u001b[0;30;48;2;255;255;252m\n",
      "\u001b[0m \u001b[0;30;48;2;255;173;173m \u001b[0m \u001b[0;30;48;2;255;214;165m while\u001b[0m \u001b[0;30;48;2;253;255;182m True\u001b[0m \u001b[0;30;48;2;253;255;182m:\u001b[0m \u001b[0;30;48;2;202;255;191m\n",
      "\u001b[0m \u001b[0;30;48;2;155;246;255m \u001b[0m \u001b[0;30;48;2;160;196;255m \u001b[0m \u001b[0;30;48;2;189;178;255m \u001b[0m \u001b[0;30;48;2;255;198;225m print\u001b[0m \u001b[0;30;48;2;255;255;252m(\"\u001b[0m \u001b[0;30;48;2;255;173;173mHello\u001b[0m \u001b[0;30;48;2;255;214;165m World\u001b[0m \u001b[0;30;48;2;253;255;182m!\"\u001b[0m \u001b[0;30;48;2;253;255;182m)\u001b[0m \u001b[0;30;48;2;202;255;191m\n",
      "\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:17.531989Z",
     "start_time": "2025-06-19T09:31:16.908847Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x3doV4Ap2nZQ",
    "outputId": "d80c4022-0e91-4a68-8198-8dbeca7ae315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;48;2;255;173;173mEnglish\u001b[0m \u001b[0;30;48;2;255;214;165mand\u001b[0m \u001b[0;30;48;2;253;255;182mCA\u001b[0m \u001b[0;30;48;2;253;255;182mPI\u001b[0m \u001b[0;30;48;2;202;255;191mTAL\u001b[0m \u001b[0;30;48;2;155;246;255mIZ\u001b[0m \u001b[0;30;48;2;160;196;255mATION\u001b[0m \u001b[0;30;48;2;189;178;255m\u001b[0m \u001b[0;30;48;2;255;198;225m<unk>\u001b[0m \u001b[0;30;48;2;255;255;252m\u001b[0m \u001b[0;30;48;2;255;173;173m<unk>\u001b[0m \u001b[0;30;48;2;255;214;165mshow\u001b[0m \u001b[0;30;48;2;253;255;182m_\u001b[0m \u001b[0;30;48;2;253;255;182mto\u001b[0m \u001b[0;30;48;2;202;255;191mken\u001b[0m \u001b[0;30;48;2;155;246;255ms\u001b[0m \u001b[0;30;48;2;160;196;255mFal\u001b[0m \u001b[0;30;48;2;189;178;255ms\u001b[0m \u001b[0;30;48;2;255;198;225me\u001b[0m \u001b[0;30;48;2;255;255;252mNone\u001b[0m \u001b[0;30;48;2;255;173;173m\u001b[0m \u001b[0;30;48;2;255;214;165me\u001b[0m \u001b[0;30;48;2;253;255;182ml\u001b[0m \u001b[0;30;48;2;253;255;182mif\u001b[0m \u001b[0;30;48;2;202;255;191m=\u001b[0m \u001b[0;30;48;2;155;246;255m=\u001b[0m \u001b[0;30;48;2;160;196;255m>\u001b[0m \u001b[0;30;48;2;189;178;255m=\u001b[0m \u001b[0;30;48;2;255;198;225melse\u001b[0m \u001b[0;30;48;2;255;255;252m:\u001b[0m \u001b[0;30;48;2;255;173;173mtwo\u001b[0m \u001b[0;30;48;2;255;214;165mtab\u001b[0m \u001b[0;30;48;2;253;255;182ms\u001b[0m \u001b[0;30;48;2;253;255;182m:\u001b[0m \u001b[0;30;48;2;202;255;191m\"\u001b[0m \u001b[0;30;48;2;155;246;255m\"\u001b[0m \u001b[0;30;48;2;160;196;255mThree\u001b[0m \u001b[0;30;48;2;189;178;255mtab\u001b[0m \u001b[0;30;48;2;255;198;225ms\u001b[0m \u001b[0;30;48;2;255;255;252m:\u001b[0m \u001b[0;30;48;2;255;173;173m\"\u001b[0m \u001b[0;30;48;2;255;214;165m\"\u001b[0m \u001b[0;30;48;2;253;255;182m12.\u001b[0m \u001b[0;30;48;2;253;255;182m0\u001b[0m \u001b[0;30;48;2;202;255;191m*\u001b[0m \u001b[0;30;48;2;155;246;255m50\u001b[0m \u001b[0;30;48;2;160;196;255m=\u001b[0m \u001b[0;30;48;2;189;178;255m600\u001b[0m \u001b[0;30;48;2;255;198;225mde\u001b[0m \u001b[0;30;48;2;255;255;252mf\u001b[0m \u001b[0;30;48;2;255;173;173mgreet\u001b[0m \u001b[0;30;48;2;255;214;165m()\u001b[0m \u001b[0;30;48;2;253;255;182m:\u001b[0m \u001b[0;30;48;2;253;255;182mwhile\u001b[0m \u001b[0;30;48;2;202;255;191mTrue\u001b[0m \u001b[0;30;48;2;155;246;255m:\u001b[0m \u001b[0;30;48;2;160;196;255mprint\u001b[0m \u001b[0;30;48;2;189;178;255m(\u001b[0m \u001b[0;30;48;2;255;198;225m\"\u001b[0m \u001b[0;30;48;2;255;255;252mH\u001b[0m \u001b[0;30;48;2;255;173;173mello\u001b[0m \u001b[0;30;48;2;255;214;165mWorld\u001b[0m \u001b[0;30;48;2;253;255;182m!\"\u001b[0m \u001b[0;30;48;2;253;255;182m)\u001b[0m \u001b[0;30;48;2;202;255;191m\u001b[0m \u001b[0;30;48;2;155;246;255m</s>\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:18.477698Z",
     "start_time": "2025-06-19T09:31:17.559861Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XFF0eWlc2nWZ",
    "outputId": "e04b520e-9215-456b-8004-578ebbd8e68e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;48;2;255;173;173m\n",
      "\u001b[0m \u001b[0;30;48;2;255;214;165mEnglish\u001b[0m \u001b[0;30;48;2;253;255;182m and\u001b[0m \u001b[0;30;48;2;253;255;182m CAPITAL\u001b[0m \u001b[0;30;48;2;202;255;191mIZATION\u001b[0m \u001b[0;30;48;2;155;246;255m\n",
      "\u001b[0m \u001b[0;30;48;2;160;196;255mï¿½\u001b[0m \u001b[0;30;48;2;189;178;255mï¿½\u001b[0m \u001b[0;30;48;2;255;198;225mï¿½\u001b[0m \u001b[0;30;48;2;255;255;252m ï¿½\u001b[0m \u001b[0;30;48;2;255;173;173mï¿½\u001b[0m \u001b[0;30;48;2;255;214;165mæ¨¡\u001b[0m \u001b[0;30;48;2;253;255;182måž‹\u001b[0m \u001b[0;30;48;2;253;255;182mï¿½\u001b[0m \u001b[0;30;48;2;202;255;191mï¿½\u001b[0m \u001b[0;30;48;2;155;246;255mè¨€\u001b[0m \u001b[0;30;48;2;160;196;255m\n",
      "\u001b[0m \u001b[0;30;48;2;189;178;255mshow\u001b[0m \u001b[0;30;48;2;255;198;225m_tokens\u001b[0m \u001b[0;30;48;2;255;255;252m False\u001b[0m \u001b[0;30;48;2;255;173;173m None\u001b[0m \u001b[0;30;48;2;255;214;165m elif\u001b[0m \u001b[0;30;48;2;253;255;182m ==\u001b[0m \u001b[0;30;48;2;253;255;182m >=\u001b[0m \u001b[0;30;48;2;202;255;191m else\u001b[0m \u001b[0;30;48;2;155;246;255m:\u001b[0m \u001b[0;30;48;2;160;196;255m two\u001b[0m \u001b[0;30;48;2;189;178;255m tabs\u001b[0m \u001b[0;30;48;2;255;198;225m:\"\u001b[0m \u001b[0;30;48;2;255;255;252m   \u001b[0m \u001b[0;30;48;2;255;173;173m \"\u001b[0m \u001b[0;30;48;2;255;214;165m Three\u001b[0m \u001b[0;30;48;2;253;255;182m tabs\u001b[0m \u001b[0;30;48;2;253;255;182m:\u001b[0m \u001b[0;30;48;2;202;255;191m \"\u001b[0m \u001b[0;30;48;2;155;246;255m      \u001b[0m \u001b[0;30;48;2;160;196;255m \"\n",
      "\u001b[0m \u001b[0;30;48;2;189;178;255m12\u001b[0m \u001b[0;30;48;2;255;198;225m.\u001b[0m \u001b[0;30;48;2;255;255;252m0\u001b[0m \u001b[0;30;48;2;255;173;173m*\u001b[0m \u001b[0;30;48;2;255;214;165m50\u001b[0m \u001b[0;30;48;2;253;255;182m=\u001b[0m \u001b[0;30;48;2;253;255;182m600\u001b[0m \u001b[0;30;48;2;202;255;191m\n",
      "\n",
      "\u001b[0m \u001b[0;30;48;2;155;246;255mdef\u001b[0m \u001b[0;30;48;2;160;196;255m greet\u001b[0m \u001b[0;30;48;2;189;178;255m():\n",
      "\u001b[0m \u001b[0;30;48;2;255;198;225m \u001b[0m \u001b[0;30;48;2;255;255;252m while\u001b[0m \u001b[0;30;48;2;255;173;173m True\u001b[0m \u001b[0;30;48;2;255;214;165m:\n",
      "\u001b[0m \u001b[0;30;48;2;253;255;182m   \u001b[0m \u001b[0;30;48;2;253;255;182m print\u001b[0m \u001b[0;30;48;2;202;255;191m(\"\u001b[0m \u001b[0;30;48;2;155;246;255mHello\u001b[0m \u001b[0;30;48;2;160;196;255m World\u001b[0m \u001b[0;30;48;2;189;178;255m!\")\n",
      "\u001b[0m "
     ]
    }
   ],
   "source": [
    "# The official is `tiktoken` but this the same tokenizer on the HF platform\n",
    "show_tokens(text, \"Xenova/gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:19.138918Z",
     "start_time": "2025-06-19T09:31:18.523105Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YmzN5U2D3Qa6",
    "outputId": "de9ea9b9-f438-46de-e8d7-724ce864def1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;48;2;255;173;173m\n",
      "\u001b[0m \u001b[0;30;48;2;255;214;165mEnglish\u001b[0m \u001b[0;30;48;2;253;255;182m and\u001b[0m \u001b[0;30;48;2;253;255;182m CAPITAL\u001b[0m \u001b[0;30;48;2;202;255;191mIZATION\u001b[0m \u001b[0;30;48;2;155;246;255m\n",
      "\u001b[0m \u001b[0;30;48;2;160;196;255mï¿½\u001b[0m \u001b[0;30;48;2;189;178;255mï¿½\u001b[0m \u001b[0;30;48;2;255;198;225mï¿½\u001b[0m \u001b[0;30;48;2;255;255;252m å¤§\u001b[0m \u001b[0;30;48;2;255;173;173mæ¨¡åž‹\u001b[0m \u001b[0;30;48;2;255;214;165mèªž\u001b[0m \u001b[0;30;48;2;253;255;182mè¨€\u001b[0m \u001b[0;30;48;2;253;255;182m\n",
      "\u001b[0m \u001b[0;30;48;2;202;255;191mshow\u001b[0m \u001b[0;30;48;2;155;246;255m_\u001b[0m \u001b[0;30;48;2;160;196;255mtokens\u001b[0m \u001b[0;30;48;2;189;178;255m False\u001b[0m \u001b[0;30;48;2;255;198;225m None\u001b[0m \u001b[0;30;48;2;255;255;252m elif\u001b[0m \u001b[0;30;48;2;255;173;173m ==\u001b[0m \u001b[0;30;48;2;255;214;165m >=\u001b[0m \u001b[0;30;48;2;253;255;182m else\u001b[0m \u001b[0;30;48;2;253;255;182m:\u001b[0m \u001b[0;30;48;2;202;255;191m two\u001b[0m \u001b[0;30;48;2;155;246;255m tabs\u001b[0m \u001b[0;30;48;2;160;196;255m:\"\u001b[0m \u001b[0;30;48;2;189;178;255m   \u001b[0m \u001b[0;30;48;2;255;198;225m \"\u001b[0m \u001b[0;30;48;2;255;255;252m Three\u001b[0m \u001b[0;30;48;2;255;173;173m tabs\u001b[0m \u001b[0;30;48;2;255;214;165m:\u001b[0m \u001b[0;30;48;2;253;255;182m \"\u001b[0m \u001b[0;30;48;2;253;255;182m      \u001b[0m \u001b[0;30;48;2;202;255;191m \"\u001b[0m \u001b[0;30;48;2;155;246;255m\n",
      "\u001b[0m \u001b[0;30;48;2;160;196;255m1\u001b[0m \u001b[0;30;48;2;189;178;255m2\u001b[0m \u001b[0;30;48;2;255;198;225m.\u001b[0m \u001b[0;30;48;2;255;255;252m0\u001b[0m \u001b[0;30;48;2;255;173;173m*\u001b[0m \u001b[0;30;48;2;255;214;165m5\u001b[0m \u001b[0;30;48;2;253;255;182m0\u001b[0m \u001b[0;30;48;2;253;255;182m=\u001b[0m \u001b[0;30;48;2;202;255;191m6\u001b[0m \u001b[0;30;48;2;155;246;255m0\u001b[0m \u001b[0;30;48;2;160;196;255m0\u001b[0m \u001b[0;30;48;2;189;178;255m\n",
      "\u001b[0m \u001b[0;30;48;2;255;198;225m\n",
      "\u001b[0m \u001b[0;30;48;2;255;255;252mdef\u001b[0m \u001b[0;30;48;2;255;173;173m g\u001b[0m \u001b[0;30;48;2;255;214;165mreet\u001b[0m \u001b[0;30;48;2;253;255;182m():\u001b[0m \u001b[0;30;48;2;253;255;182m\n",
      " \u001b[0m \u001b[0;30;48;2;202;255;191m while\u001b[0m \u001b[0;30;48;2;155;246;255m True\u001b[0m \u001b[0;30;48;2;160;196;255m:\u001b[0m \u001b[0;30;48;2;189;178;255m\n",
      "   \u001b[0m \u001b[0;30;48;2;255;198;225m print\u001b[0m \u001b[0;30;48;2;255;255;252m(\"\u001b[0m \u001b[0;30;48;2;255;173;173mHello\u001b[0m \u001b[0;30;48;2;255;214;165m World\u001b[0m \u001b[0;30;48;2;253;255;182m!\")\u001b[0m \u001b[0;30;48;2;253;255;182m\n",
      "\u001b[0m "
     ]
    }
   ],
   "source": [
    "# You need to request access before being able to use this tokenizer\n",
    "show_tokens(text, \"bigcode/starcoder2-15b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:19.842006Z",
     "start_time": "2025-06-19T09:31:19.181701Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PGBA7BRN3VV5",
    "outputId": "4a0e8d4d-3693-4a50-beb1-49e4f0be6bcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;48;2;255;173;173m\n",
      "\u001b[0m \u001b[0;30;48;2;255;214;165mEnglish\u001b[0m \u001b[0;30;48;2;253;255;182m and\u001b[0m \u001b[0;30;48;2;253;255;182m CAP\u001b[0m \u001b[0;30;48;2;202;255;191mITAL\u001b[0m \u001b[0;30;48;2;155;246;255mIZATION\u001b[0m \u001b[0;30;48;2;160;196;255m\n",
      "\u001b[0m \u001b[0;30;48;2;189;178;255mï¿½\u001b[0m \u001b[0;30;48;2;255;198;225mï¿½\u001b[0m \u001b[0;30;48;2;255;255;252mï¿½\u001b[0m \u001b[0;30;48;2;255;173;173mï¿½\u001b[0m \u001b[0;30;48;2;255;214;165m ï¿½\u001b[0m \u001b[0;30;48;2;253;255;182mï¿½\u001b[0m \u001b[0;30;48;2;253;255;182mï¿½\u001b[0m \u001b[0;30;48;2;202;255;191mï¿½\u001b[0m \u001b[0;30;48;2;155;246;255mï¿½\u001b[0m \u001b[0;30;48;2;160;196;255mï¿½\u001b[0m \u001b[0;30;48;2;189;178;255mï¿½\u001b[0m \u001b[0;30;48;2;255;198;225mï¿½\u001b[0m \u001b[0;30;48;2;255;255;252mï¿½\u001b[0m \u001b[0;30;48;2;255;173;173mï¿½\u001b[0m \u001b[0;30;48;2;255;214;165mï¿½\u001b[0m \u001b[0;30;48;2;253;255;182mï¿½\u001b[0m \u001b[0;30;48;2;253;255;182mï¿½\u001b[0m \u001b[0;30;48;2;202;255;191mï¿½\u001b[0m \u001b[0;30;48;2;155;246;255mï¿½\u001b[0m \u001b[0;30;48;2;160;196;255m\n",
      "\u001b[0m \u001b[0;30;48;2;189;178;255mshow\u001b[0m \u001b[0;30;48;2;255;198;225m_\u001b[0m \u001b[0;30;48;2;255;255;252mtokens\u001b[0m \u001b[0;30;48;2;255;173;173m False\u001b[0m \u001b[0;30;48;2;255;214;165m None\u001b[0m \u001b[0;30;48;2;253;255;182m elif\u001b[0m \u001b[0;30;48;2;253;255;182m \u001b[0m \u001b[0;30;48;2;202;255;191m==\u001b[0m \u001b[0;30;48;2;155;246;255m \u001b[0m \u001b[0;30;48;2;160;196;255m>\u001b[0m \u001b[0;30;48;2;189;178;255m=\u001b[0m \u001b[0;30;48;2;255;198;225m else\u001b[0m \u001b[0;30;48;2;255;255;252m:\u001b[0m \u001b[0;30;48;2;255;173;173m two\u001b[0m \u001b[0;30;48;2;255;214;165m t\u001b[0m \u001b[0;30;48;2;253;255;182mabs\u001b[0m \u001b[0;30;48;2;253;255;182m:\u001b[0m \u001b[0;30;48;2;202;255;191m\"\u001b[0m \u001b[0;30;48;2;155;246;255m    \u001b[0m \u001b[0;30;48;2;160;196;255m\"\u001b[0m \u001b[0;30;48;2;189;178;255m Three\u001b[0m \u001b[0;30;48;2;255;198;225m t\u001b[0m \u001b[0;30;48;2;255;255;252mabs\u001b[0m \u001b[0;30;48;2;255;173;173m:\u001b[0m \u001b[0;30;48;2;255;214;165m \u001b[0m \u001b[0;30;48;2;253;255;182m\"\u001b[0m \u001b[0;30;48;2;253;255;182m       \u001b[0m \u001b[0;30;48;2;202;255;191m\"\u001b[0m \u001b[0;30;48;2;155;246;255m\n",
      "\u001b[0m \u001b[0;30;48;2;160;196;255m1\u001b[0m \u001b[0;30;48;2;189;178;255m2\u001b[0m \u001b[0;30;48;2;255;198;225m.\u001b[0m \u001b[0;30;48;2;255;255;252m0\u001b[0m \u001b[0;30;48;2;255;173;173m*\u001b[0m \u001b[0;30;48;2;255;214;165m5\u001b[0m \u001b[0;30;48;2;253;255;182m0\u001b[0m \u001b[0;30;48;2;253;255;182m=\u001b[0m \u001b[0;30;48;2;202;255;191m6\u001b[0m \u001b[0;30;48;2;155;246;255m0\u001b[0m \u001b[0;30;48;2;160;196;255m0\u001b[0m \u001b[0;30;48;2;189;178;255m\n",
      "\u001b[0m \u001b[0;30;48;2;255;198;225m\n",
      "\u001b[0m \u001b[0;30;48;2;255;255;252mdef\u001b[0m \u001b[0;30;48;2;255;173;173m gre\u001b[0m \u001b[0;30;48;2;255;214;165met\u001b[0m \u001b[0;30;48;2;253;255;182m(\u001b[0m \u001b[0;30;48;2;253;255;182m)\u001b[0m \u001b[0;30;48;2;202;255;191m:\u001b[0m \u001b[0;30;48;2;155;246;255m\n",
      "\u001b[0m \u001b[0;30;48;2;160;196;255m \u001b[0m \u001b[0;30;48;2;189;178;255m while\u001b[0m \u001b[0;30;48;2;255;198;225m True\u001b[0m \u001b[0;30;48;2;255;255;252m:\u001b[0m \u001b[0;30;48;2;255;173;173m\n",
      "\u001b[0m \u001b[0;30;48;2;255;214;165m   \u001b[0m \u001b[0;30;48;2;253;255;182m print\u001b[0m \u001b[0;30;48;2;253;255;182m(\u001b[0m \u001b[0;30;48;2;202;255;191m\"\u001b[0m \u001b[0;30;48;2;155;246;255mH\u001b[0m \u001b[0;30;48;2;160;196;255mello\u001b[0m \u001b[0;30;48;2;189;178;255m World\u001b[0m \u001b[0;30;48;2;255;198;225m!\u001b[0m \u001b[0;30;48;2;255;255;252m\"\u001b[0m \u001b[0;30;48;2;255;173;173m)\u001b[0m \u001b[0;30;48;2;255;214;165m\n",
      "\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"facebook/galactica-1.3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:20.468903Z",
     "start_time": "2025-06-19T09:31:19.897670Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aSjf5Ccx3VGx",
    "outputId": "fd11224e-348f-4c21-cfb3-d782053f1add"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;48;2;255;173;173m\u001b[0m \u001b[0;30;48;2;255;214;165m\n",
      "\u001b[0m \u001b[0;30;48;2;253;255;182mEnglish\u001b[0m \u001b[0;30;48;2;253;255;182mand\u001b[0m \u001b[0;30;48;2;202;255;191mC\u001b[0m \u001b[0;30;48;2;155;246;255mAP\u001b[0m \u001b[0;30;48;2;160;196;255mIT\u001b[0m \u001b[0;30;48;2;189;178;255mAL\u001b[0m \u001b[0;30;48;2;255;198;225mIZ\u001b[0m \u001b[0;30;48;2;255;255;252mATION\u001b[0m \u001b[0;30;48;2;255;173;173m\n",
      "\u001b[0m \u001b[0;30;48;2;255;214;165mï¿½\u001b[0m \u001b[0;30;48;2;253;255;182mï¿½\u001b[0m \u001b[0;30;48;2;253;255;182mï¿½\u001b[0m \u001b[0;30;48;2;202;255;191mï¿½\u001b[0m \u001b[0;30;48;2;155;246;255m\u001b[0m \u001b[0;30;48;2;160;196;255må¤§\u001b[0m \u001b[0;30;48;2;189;178;255mæ¨¡\u001b[0m \u001b[0;30;48;2;255;198;225måž‹\u001b[0m \u001b[0;30;48;2;255;255;252mèªž\u001b[0m \u001b[0;30;48;2;255;173;173mè¨€\u001b[0m \u001b[0;30;48;2;255;214;165m\n",
      "\u001b[0m \u001b[0;30;48;2;253;255;182mshow\u001b[0m \u001b[0;30;48;2;253;255;182m_\u001b[0m \u001b[0;30;48;2;202;255;191mto\u001b[0m \u001b[0;30;48;2;155;246;255mkens\u001b[0m \u001b[0;30;48;2;160;196;255mFalse\u001b[0m \u001b[0;30;48;2;189;178;255mNone\u001b[0m \u001b[0;30;48;2;255;198;225melif\u001b[0m \u001b[0;30;48;2;255;255;252m==\u001b[0m \u001b[0;30;48;2;255;173;173m>=\u001b[0m \u001b[0;30;48;2;255;214;165melse\u001b[0m \u001b[0;30;48;2;253;255;182m:\u001b[0m \u001b[0;30;48;2;253;255;182mtwo\u001b[0m \u001b[0;30;48;2;202;255;191mtabs\u001b[0m \u001b[0;30;48;2;155;246;255m:\"\u001b[0m \u001b[0;30;48;2;160;196;255m  \u001b[0m \u001b[0;30;48;2;189;178;255m\"\u001b[0m \u001b[0;30;48;2;255;198;225mThree\u001b[0m \u001b[0;30;48;2;255;255;252mtabs\u001b[0m \u001b[0;30;48;2;255;173;173m:\u001b[0m \u001b[0;30;48;2;255;214;165m\"\u001b[0m \u001b[0;30;48;2;253;255;182m     \u001b[0m \u001b[0;30;48;2;253;255;182m\"\u001b[0m \u001b[0;30;48;2;202;255;191m\n",
      "\u001b[0m \u001b[0;30;48;2;155;246;255m1\u001b[0m \u001b[0;30;48;2;160;196;255m2\u001b[0m \u001b[0;30;48;2;189;178;255m.\u001b[0m \u001b[0;30;48;2;255;198;225m0\u001b[0m \u001b[0;30;48;2;255;255;252m*\u001b[0m \u001b[0;30;48;2;255;173;173m5\u001b[0m \u001b[0;30;48;2;255;214;165m0\u001b[0m \u001b[0;30;48;2;253;255;182m=\u001b[0m \u001b[0;30;48;2;253;255;182m6\u001b[0m \u001b[0;30;48;2;202;255;191m0\u001b[0m \u001b[0;30;48;2;155;246;255m0\u001b[0m \u001b[0;30;48;2;160;196;255m\n",
      "\u001b[0m \u001b[0;30;48;2;189;178;255m\n",
      "\u001b[0m \u001b[0;30;48;2;255;198;225mdef\u001b[0m \u001b[0;30;48;2;255;255;252mgre\u001b[0m \u001b[0;30;48;2;255;173;173met\u001b[0m \u001b[0;30;48;2;255;214;165m():\u001b[0m \u001b[0;30;48;2;253;255;182m\n",
      "\u001b[0m \u001b[0;30;48;2;253;255;182m\u001b[0m \u001b[0;30;48;2;202;255;191mwhile\u001b[0m \u001b[0;30;48;2;155;246;255mTrue\u001b[0m \u001b[0;30;48;2;160;196;255m:\u001b[0m \u001b[0;30;48;2;189;178;255m\n",
      "\u001b[0m \u001b[0;30;48;2;255;198;225m  \u001b[0m \u001b[0;30;48;2;255;255;252mprint\u001b[0m \u001b[0;30;48;2;255;173;173m(\"\u001b[0m \u001b[0;30;48;2;255;214;165mHello\u001b[0m \u001b[0;30;48;2;253;255;182mWorld\u001b[0m \u001b[0;30;48;2;253;255;182m!\u001b[0m \u001b[0;30;48;2;202;255;191m\")\u001b[0m \u001b[0;30;48;2;155;246;255m\n",
      "\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCs4Fbrwx02L"
   },
   "source": [
    "# Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:20.507949Z",
     "start_time": "2025-06-19T09:31:20.496931Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uyElkZL1wXg7",
    "outputId": "7b539ee3-2693-461b-f944-b5bc4afb4ecc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n",
      "20479\n"
     ]
    }
   ],
   "source": [
    "encoded_text = bpe_tokenizer.encode(raw_text)\n",
    "print(len(encoded_text))\n",
    "print(len(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:20.612299Z",
     "start_time": "2025-06-19T09:31:20.607307Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LUYCUf_B8hnz",
    "outputId": "b6dec39a-52b4-4dfc-b0af-bb6c37c4ac39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438, 568, 340, 373, 645, 1049, 5975, 284, 502, 284, 3285, 326, 11, 287, 262, 6001, 286, 465, 13476, 11, 339, 550, 5710, 465, 12036, 11, 6405, 257, 5527, 27075, 11, 290, 4920, 2241, 287, 257, 4489, 64, 319, 262, 34686, 41976, 13, 357, 10915, 314, 2138, 1807, 340, 561, 423, 587, 10598, 393, 28537, 2014, 198, 198, 1, 464, 6001, 286, 465, 13476, 1, 438, 5562, 373, 644, 262, 1466, 1444, 340, 13, 314, 460, 3285, 9074, 13, 46606, 536]\n"
     ]
    }
   ],
   "source": [
    "encoded_sample = encoded_text[:100]\n",
    "print(encoded_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zoZp7V_9j91"
   },
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:20.744092Z",
     "start_time": "2025-06-19T09:31:20.737510Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2XU7FjvO9Ob3",
    "outputId": "abb9006f-e60c-4445-915c-33b1f2c83b15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input vector: [40, 367, 2885, 1464, 1807, 3619, 402, 271]\n",
      "target vector:    [367, 2885, 1464, 1807, 3619, 402, 271, 10899]\n"
     ]
    }
   ],
   "source": [
    "context_length = 8\n",
    "input = encoded_sample[:context_length]\n",
    "target = encoded_sample[1:context_length+1]\n",
    "print(f\"input vector: {input}\")\n",
    "print(f\"target vector:    {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T09:31:20.920476Z",
     "start_time": "2025-06-19T09:31:20.887318Z"
    },
    "id": "-SFDC8O791zv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# create a subclass of Dataset\n",
    "class GPT2DatasetV1(Dataset):\n",
    "  def __init__(self,\n",
    "               text,\n",
    "               tokenizer,\n",
    "               context_length, # length of each input vector\n",
    "               stride # chunk the text into overlapping sequence of context_length\n",
    "               ):\n",
    "    self.input_id_vectors = []\n",
    "    self.target_id_vectors = []\n",
    "\n",
    "    # tokenize the entire text\n",
    "    token_list = tokenizer.encode(text)\n",
    "\n",
    "    # append input and target vectors\n",
    "    for i in range(0, len(token_list) - context_length, stride):\n",
    "      input_vector = token_list[i:i+context_length]\n",
    "      target_vector = token_list[i+1:i+context_length+1]\n",
    "      self.input_id_vectors.append(torch.tensor(input_vector))\n",
    "      self.target_id_vectors.append(torch.tensor(target_vector))\n",
    "\n",
    "  # get the number of input vectors\n",
    "  def __len__(self):\n",
    "    return len(self.input_id_vectors)\n",
    "\n",
    "  # return the (input vector, target vector) pair\n",
    "  def __getitem__(self, id):\n",
    "    return self.input_id_vectors[id], self.target_id_vectors[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8sINjEacBM8",
    "outputId": "1b88a7ec-e9d0-4e94-8219-dd0232ab6bd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements:  5141\n",
      "some random pair:  (tensor([ 3619,   402,   271, 10899]), tensor([  402,   271, 10899,  2138]))\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "***EXAMPLES WITH TOKEN IDS:***\n",
      "Example 1\n",
      "Input IDs : [40, 367, 2885, 1464]\n",
      "Target IDs: [367, 2885, 1464, 1807]\n",
      "------------------------------------------------------------\n",
      "Example 2\n",
      "Input IDs : [367, 2885, 1464, 1807]\n",
      "Target IDs: [2885, 1464, 1807, 3619]\n",
      "------------------------------------------------------------\n",
      "Example 3\n",
      "Input IDs : [2885, 1464, 1807, 3619]\n",
      "Target IDs: [1464, 1807, 3619, 402]\n",
      "------------------------------------------------------------\n",
      "Example 4\n",
      "Input IDs : [1464, 1807, 3619, 402]\n",
      "Target IDs: [1807, 3619, 402, 271]\n",
      "------------------------------------------------------------\n",
      "Example 5\n",
      "Input IDs : [1807, 3619, 402, 271]\n",
      "Target IDs: [3619, 402, 271, 10899]\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "***EXAMPLES WITH TEXTS:***\n",
      "Example 1\n",
      "Input Text : I HAD always\n",
      "Target Text:  HAD always thought\n",
      "------------------------------------------------------------\n",
      "Example 2\n",
      "Input Text :  HAD always thought\n",
      "Target Text: AD always thought Jack\n",
      "------------------------------------------------------------\n",
      "Example 3\n",
      "Input Text : AD always thought Jack\n",
      "Target Text:  always thought Jack G\n",
      "------------------------------------------------------------\n",
      "Example 4\n",
      "Input Text :  always thought Jack G\n",
      "Target Text:  thought Jack Gis\n",
      "------------------------------------------------------------\n",
      "Example 5\n",
      "Input Text :  thought Jack Gis\n",
      "Target Text:  Jack Gisburn\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_LENGTH = 4\n",
    "STRIDE = 1\n",
    "\n",
    "\n",
    "\n",
    "dataset = GPT2DatasetV1(text=raw_text,\n",
    "                        tokenizer=bpe_tokenizer,\n",
    "                        context_length=CONTEXT_LENGTH,\n",
    "                        stride=STRIDE)\n",
    "print(\"number of elements: \", dataset.__len__())\n",
    "print(\"some random pair: \", dataset.__getitem__(5))\n",
    "\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"-\" * 60)\n",
    "print(\"-\" * 60)\n",
    "print(\"***EXAMPLES WITH TOKEN IDS:***\")\n",
    "for i in range(5):\n",
    "    input_ids, target_ids = dataset[i]\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(\"Input IDs :\", input_ids.tolist())\n",
    "    print(\"Target IDs:\", target_ids.tolist())\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"-\" * 60)\n",
    "print(\"-\" * 60)\n",
    "print(\"***EXAMPLES WITH TEXTS:***\")\n",
    "for i in range(5):\n",
    "    input_ids, target_ids = dataset[i]\n",
    "    input_text = bpe_tokenizer.decode(input_ids.tolist())\n",
    "    target_text = bpe_tokenizer.decode(target_ids.tolist())\n",
    "\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(\"Input Text :\", input_text)\n",
    "    print(\"Target Text:\", target_text)\n",
    "    print(\"-\" * 60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEuDIM2-bgX1"
   },
   "source": [
    "## Create a DataLoader Function That Loads the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "wM4VTyXe-LpQ"
   },
   "outputs": [],
   "source": [
    "def create_dataloader_V1(text,\n",
    "                 batch_size=4,\n",
    "                 context_length=256,\n",
    "                 stride=128,\n",
    "                 shuffle=True, # shuffle dataset\n",
    "                 drop_last=True, # drop last batch if it not equal required size\n",
    "                 num_workers=0 # number of CPU processes for preprocessing\n",
    "                 ):\n",
    "\n",
    "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "  dataset = GPT2DatasetV1(text=text,\n",
    "                          tokenizer=tokenizer,\n",
    "                          context_length=context_length,\n",
    "                          stride=stride)\n",
    "\n",
    "  dataloader = DataLoader(dataset=dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=shuffle,\n",
    "                          drop_last=drop_last,\n",
    "                          num_workers=num_workers)\n",
    "\n",
    "  return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b34rq43EiS_p"
   },
   "source": [
    "Now let's create a dataloader instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0j7u6VO-iLnM",
    "outputId": "bc5ac0d2-19d5-470f-d9a1-d5f74f477af5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n"
     ]
    }
   ],
   "source": [
    "context_length = 4\n",
    "\n",
    "dataloader = create_dataloader_V1(text=raw_text,\n",
    "                                  batch_size=8,\n",
    "                                  context_length=4,\n",
    "                                  stride=4,\n",
    "                                  shuffle=False)\n",
    "\n",
    "print(len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ITBab9iSirn0",
    "outputId": "03b4d737-5f21-4b1e-eb41-528d8e1d8c60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Input IDs : tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Target IDs: tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n",
      "------------------------------------------------------------\n",
      "Batch 2\n",
      "Input IDs : tensor([[  287,   262,  6001,   286],\n",
      "        [  465, 13476,    11,   339],\n",
      "        [  550,  5710,   465, 12036],\n",
      "        [   11,  6405,   257,  5527],\n",
      "        [27075,    11,   290,  4920],\n",
      "        [ 2241,   287,   257,  4489],\n",
      "        [   64,   319,   262, 34686],\n",
      "        [41976,    13,   357, 10915]])\n",
      "Target IDs: tensor([[  262,  6001,   286,   465],\n",
      "        [13476,    11,   339,   550],\n",
      "        [ 5710,   465, 12036,    11],\n",
      "        [ 6405,   257,  5527, 27075],\n",
      "        [   11,   290,  4920,  2241],\n",
      "        [  287,   257,  4489,    64],\n",
      "        [  319,   262, 34686, 41976],\n",
      "        [   13,   357, 10915,   314]])\n",
      "------------------------------------------------------------\n",
      "Batch 3\n",
      "Input IDs : tensor([[  314,  2138,  1807,   340],\n",
      "        [  561,   423,   587, 10598],\n",
      "        [  393, 28537,  2014,   198],\n",
      "        [  198,     1,   464,  6001],\n",
      "        [  286,   465, 13476,     1],\n",
      "        [  438,  5562,   373,   644],\n",
      "        [  262,  1466,  1444,   340],\n",
      "        [   13,   314,   460,  3285]])\n",
      "Target IDs: tensor([[ 2138,  1807,   340,   561],\n",
      "        [  423,   587, 10598,   393],\n",
      "        [28537,  2014,   198,   198],\n",
      "        [    1,   464,  6001,   286],\n",
      "        [  465, 13476,     1,   438],\n",
      "        [ 5562,   373,   644,   262],\n",
      "        [ 1466,  1444,   340,    13],\n",
      "        [  314,   460,  3285,  9074]])\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, (input_ids, target_ids) in enumerate(dataloader):\n",
    "    print(f\"Batch {i+1}\")\n",
    "    print(\"Input IDs :\", input_ids)\n",
    "    print(\"Target IDs:\", target_ids)\n",
    "    print(\"-\" * 60)\n",
    "    if i == 2:  # stop after 3 batches (0, 1, 2)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ykza5gikpvx"
   },
   "source": [
    "# Token Embeddings and (Absolute) Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xYpKaI-k8RpP",
    "outputId": "a9394e14-2b79-4f16-ad99-24f9e8b3711d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(50257, 256)\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = bpe_tokenizer.n_vocab\n",
    "EMBEDDING_DIM = 256\n",
    "\n",
    "# token embeddings\n",
    "token_embedding_layer = torch.nn.Embedding(num_embeddings=VOCAB_SIZE,\n",
    "                                           embedding_dim=EMBEDDING_DIM)\n",
    "\n",
    "print(token_embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y-aaOlDWi8Jl",
    "outputId": "70944a57-4eec-4ee7-edae-05793766563d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  314,  2138,  1807,   340],\n",
       "        [  561,   423,   587, 10598],\n",
       "        [  393, 28537,  2014,   198],\n",
       "        [  198,     1,   464,  6001],\n",
       "        [  286,   465, 13476,     1],\n",
       "        [  438,  5562,   373,   644],\n",
       "        [  262,  1466,  1444,   340],\n",
       "        [   13,   314,   460,  3285]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0YkN4-Me-6li",
    "outputId": "3a631ee9-6111-4746-8c3d-651a7f8678c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(input_ids)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZHGWcolM_Ci3",
    "outputId": "ec28da1e-209c-44f5-901c-6d589a59976f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "# absolute positional embedding\n",
    "pos_embedding_layer = torch.nn.Embedding(num_embeddings=context_length,\n",
    "                                         embedding_dim=EMBEDDING_DIM)\n",
    "pos_embedding = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q412FrNhBNHk",
    "outputId": "028b2a7e-445c-4eca-835d-3857178a0ed6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# input embedding\n",
    "input_embeddings = token_embeddings + pos_embedding\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E90pnaI-CKgD"
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4UiUcujCE6m",
    "outputId": "51a02aee-a1ca-4cfc-da1b-6ac2ad1ea9c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap g\n"
     ]
    }
   ],
   "source": [
    "# import and print out text\n",
    "import urllib.request\n",
    "\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/\"\n",
    "        \"refs/heads/main/ch02/01_main-chapter-code/\"\n",
    "        \"the-verdict.txt\")\n",
    "file_path = \"../datasets/the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "  raw_text = f.read()\n",
    "\n",
    "print(\"total number of characters:\", len(raw_text))\n",
    "print(raw_text[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0j9w0Mk1Eay3",
    "outputId": "1f4e3423-d1ce-426e-8f45-e74792e6da8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n",
      "('year', 1122)\n",
      "('years', 1123)\n",
      "('yellow', 1124)\n",
      "('yet', 1125)\n",
      "('you', 1126)\n",
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|unk|>', 1130)\n",
      "('<|endoftext|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "# create vocab\n",
    "preprocessed_text = re.split(r'([,.:;\"()?_!\\'\\s]|--)', raw_text)\n",
    "preprocessed_text = [word for word in preprocessed_text if word.strip()]\n",
    "\n",
    "# adding special tokens\n",
    "list_of_all_words = sorted(list(set(preprocessed)))\n",
    "list_of_all_words.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(list_of_all_words)}\n",
    "\n",
    "print(len(vocab.items()))\n",
    "\n",
    "for i, word in enumerate(list(vocab.items())[-10:]):\n",
    "  print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "RpTM6ZP5CW20"
   },
   "outputs": [],
   "source": [
    "# create the tokenizer class\n",
    "class SimpleTokenizerV2:\n",
    "  def __init__(self, vocab):\n",
    "    self.word_to_id = vocab # a dict\n",
    "    self.id_to_word = {id:word for word, id in vocab.items()}\n",
    "\n",
    "  # word --> id (tokenizer)\n",
    "  def encode(self, text):\n",
    "    splitted_text = re.split(r'([,.:;\"()?_!\\'\\s]|--)', text)\n",
    "    word_list = [word for word in splitted_text if word.strip()]\n",
    "    # handle unknown words\n",
    "    word_list = [word if word in self.word_to_id\n",
    "                 else \"<|unk|>\" for word in word_list]\n",
    "    id_list = [self.word_to_id[word] for word in word_list]\n",
    "    return id_list\n",
    "\n",
    "  # id --> word (detokenizer)\n",
    "  def decode(self, id_list):\n",
    "    word_list = [self.id_to_word[id] for id in id_list]\n",
    "    text_with_white_space = \" \".join(word_list)\n",
    "    text = re.sub(r'\\s+([,.:;\"()?_!\\'\\s])', r'\\1', text_with_white_space)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PyynsJ5uaCge",
    "outputId": "3416e7c3-4f1d-489d-e88e-dd20cddcaebc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "\"In the midst of winter, I found, within me,\n",
      " an invincible summer\" said Albert Camus <|endoftext|> Happy Hunger Games, and may the odds be ever in your favor.\n",
      "------------------------------------------------------------\n",
      "[1, 55, 988, 1130, 722, 1130, 5, 53, 469, 5, 1130, 663, 5, 156, 1130, 1130, 1, 851, 1130, 1130, 1131, 1130, 1130, 1130, 5, 157, 662, 988, 1130, 198, 401, 568, 1128, 1130, 7]\n",
      "------------------------------------------------------------\n",
      "\" In the <|unk|> of <|unk|>, I found, <|unk|> me, an <|unk|> <|unk|>\" said <|unk|> <|unk|> <|endoftext|> <|unk|> <|unk|> <|unk|>, and may the <|unk|> be ever in your <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"\"\"\"In the midst of winter, I found, within me,\n",
    " an invincible summer\" said Albert Camus\"\"\"\n",
    "text2 = \"\"\"Happy Hunger Games, and may the odds be ever in your favor.\"\"\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(text)\n",
    "tokenizerv2 = SimpleTokenizerV2(vocab)\n",
    "print(\"-\" * 60)\n",
    "print(tokenizerv2.encode(text))\n",
    "print(\"-\" * 60)\n",
    "print(tokenizerv2.decode(tokenizerv2.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "kfvuv8q8bYld"
   },
   "outputs": [],
   "source": [
    "# use BPE\n",
    "import tiktoken\n",
    "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded_text = bpe_tokenizer.encode(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "gJcrTZsZCaFv"
   },
   "outputs": [],
   "source": [
    "# create the Dataset class\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPT2DatasetV1(Dataset):\n",
    "  def __init__(self,\n",
    "               text,\n",
    "               tokenizer,\n",
    "               context_length,\n",
    "               stride):\n",
    "    self.input_id_vectors_list = []\n",
    "    self.target_id_vectors_list = []\n",
    "\n",
    "    # tokenize the text\n",
    "    token_list = tokenizer.encode(text)\n",
    "\n",
    "    # append input and target vectors\n",
    "    for i in range(0, len(token_list) - context_length, stride):\n",
    "      input_id_vector = token_list[i:i+context_length]\n",
    "      target_id_vector = token_list[i+1:i+context_length+1]\n",
    "      self.input_id_vectors_list.append(torch.tensor(input_id_vector))\n",
    "      self.target_id_vectors_list.append(torch.tensor(target_id_vector))\n",
    "\n",
    "  # get the number of input vectors\n",
    "  def __len__(self):\n",
    "    return len(self.input_id_vectors_list)\n",
    "\n",
    "  # return the (input vector, target vector) pair\n",
    "  def __getitem__(self, id):\n",
    "    return self.input_id_vectors_list[id], self.target_id_vectors_list[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "kUL0bnXbClYv"
   },
   "outputs": [],
   "source": [
    "# create the DataLoader function\n",
    "def create_dataLoader_V1(text,\n",
    "                         batch_size=4,\n",
    "                         context_length=256,\n",
    "                         stride=128,\n",
    "                         shuffle=True,\n",
    "                         drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "  dataset = GPT2DatasetV1(text=text,\n",
    "                          tokenizer=tokenizer,\n",
    "                          context_length=context_length,\n",
    "                          stride=stride)\n",
    "\n",
    "  dataloader = DataLoader(dataset=dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=shuffle,\n",
    "                          drop_last=drop_last,\n",
    "                          num_workers=num_workers)\n",
    "\n",
    "  return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "l98SHn5tf8Hi"
   },
   "outputs": [],
   "source": [
    "dataloader = create_dataloader_V1(text=raw_text,\n",
    "                                  batch_size=8,\n",
    "                                  context_length=4,\n",
    "                                  stride=4,\n",
    "                                  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "Gbr3y_SCgNLH"
   },
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([[  314,  2138,  1807,   340],\n",
    "        [  561,   423,   587, 10598],\n",
    "        [  393, 28537,  2014,   198],\n",
    "        [  198,     1,   464,  6001],\n",
    "        [  286,   465, 13476,     1],\n",
    "        [  438,  5562,   373,   644],\n",
    "        [  262,  1466,  1444,   340],\n",
    "        [   13,   314,   460,  3285]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "twEMAUD7CpRy",
    "outputId": "de0060d0-11a2-4ed1-8f88-852d848dc4a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# token embedding\n",
    "VOCAB_SIZE = bpe_tokenizer.n_vocab\n",
    "EMBEDDING_DIM =  256\n",
    "\n",
    "# token embedding\n",
    "token_embedding_layer = torch.nn.Embedding(num_embeddings=VOCAB_SIZE,\n",
    "                                           embedding_dim=EMBEDDING_DIM)\n",
    "token_embeddings = token_embedding_layer(input_ids)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XE__eEUFflhP",
    "outputId": "b39edde7-fe40-4c68-c122-4a5029246dd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "# absolute positional embedding\n",
    "pos_embedding_layer = torch.nn.Embedding(num_embeddings=context_length,\n",
    "                                         embedding_dim=EMBEDDING_DIM)\n",
    "pos_embedding = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2LcfnDL6hEJZ",
    "outputId": "dc85a1bc-cca8-42cc-ce13-00a1cced8333"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# input embedding\n",
    "input_embeddings = token_embeddings + pos_embedding\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kkVOrx59hdGL",
    "outputId": "d60e3a4a-b421-4ad2-e7dd-92b2e766e845"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook runtime: 0 min 22.37 sec\n"
     ]
    }
   ],
   "source": [
    "notebook_end_time = time.time()\n",
    "runtime_in_seconds = notebook_end_time - notebook_start_time\n",
    "\n",
    "# format as minutes and seconds\n",
    "minutes, seconds = divmod(runtime_in_seconds, 60)\n",
    "print(f\"Notebook runtime: {int(minutes)} min {seconds:.2f} sec\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
