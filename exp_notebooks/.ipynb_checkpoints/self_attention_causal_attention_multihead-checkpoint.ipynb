{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UZvwi-zhVTIx"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "notebook_start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BW3wN4mJi7o5"
   },
   "source": [
    "# Coding Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEY69Bqy4CnO"
   },
   "source": [
    "## Simple Self-Attention Without Trainable Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CxYGxrh3dzC"
   },
   "source": [
    "Let's create a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ug_J_HJidIZ",
    "outputId": "d22cd192-9a0e-4a21-8fce-caaa58e2896e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "inputs.shape # [token, embedding_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PLnYvh-v3kU8",
    "outputId": "257cc930-6f3e-43d2-a4f9-8d84b3f71951"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2 = inputs[1] # 2nd input token is the query\n",
    "attention_scores_2 = torch.empty(inputs.shape[0])\n",
    "\n",
    "# compute attention scores\n",
    "for i, input_embedding in enumerate(inputs):\n",
    "  attention_scores_2[i] = torch.dot(query_2, input_embedding)\n",
    "\n",
    "attention_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oD3Se2hJ4SBy",
    "outputId": "185db050-1f64-46e0-c5ac-200cb46c1d40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# compute attention weights\n",
    "attention_weights_2 = torch.softmax(attention_scores_2, dim=0)\n",
    "\n",
    "print(\"attention weights: \", attention_weights_2)\n",
    "print(\"sum:\", attention_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wNIHqOdF5-Iv",
    "outputId": "5d76c5aa-a86a-4db3-8378-4dd9a18a81eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4419, 0.6515, 0.5683])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute context vector\n",
    "context_vector_2 = torch.zeros(inputs.shape[1])\n",
    "\n",
    "for i, input_embedding in enumerate(inputs):\n",
    "  context_vector_2 += attention_weights_2[i] * input_embedding\n",
    "\n",
    "context_vector_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7p-k7peP7tbK"
   },
   "source": [
    "Let's compute for all queries simultaneously using matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JukK-r-f7qo7",
    "outputId": "f922c40e-d62f-44d3-e754-b04bba199484"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute attention scores\n",
    "attention_scores = inputs @ inputs.T\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KcgW42Fh6in7",
    "outputId": "f800a33a-303e-49e4-86cc-5f58abf32b58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute attention weights\n",
    "attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "print(\"row sums:\", attention_weights.sum(dim=-1))\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "icOWE4Nd9EHS",
    "outputId": "e5a82ea4-a02c-492b-c188-abf7d7518a67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute context vectors\n",
    "context_vectors = attention_weights @ inputs\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHO01f949axm"
   },
   "source": [
    "## Implement Self-Attention with Trainable Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXBMeitE-fzu"
   },
   "source": [
    "## Compute KVQ vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bRd0eUAT9TNs"
   },
   "outputs": [],
   "source": [
    "# define some variables\n",
    "x_2 = inputs[1] # 2nd input token is the query\n",
    "input_embedding_dim = inputs.shape[1] # dim=3\n",
    "output_embedding_dim = 5 # output dim=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPAW_klv-8Fk"
   },
   "source": [
    "Let's create KVQ matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "eGxQrpiX-6IR"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(211)\n",
    "W_query = torch.nn.Parameter(torch.rand(input_embedding_dim, output_embedding_dim),\n",
    "                             requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(input_embedding_dim, output_embedding_dim),\n",
    "                           requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(input_embedding_dim, output_embedding_dim),\n",
    "                             requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oN1pUVyB_cmv",
    "outputId": "291940c7-10a1-40db-aad6-e67b3a9d4444"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_2: tensor([0.3530, 0.8074, 0.7720, 0.9246, 0.7118])\n",
      "key_2: tensor([0.6885, 0.4568, 1.4283, 1.4383, 1.2989])\n",
      "value_2: tensor([1.2123, 0.6055, 1.0735, 1.3861, 1.5435])\n"
     ]
    }
   ],
   "source": [
    "# project x_2 into KVQ spaces\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(\"query_2:\", query_2)\n",
    "print(\"key_2:\", key_2)\n",
    "print(\"value_2:\", value_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvmuD6rbAGm3"
   },
   "source": [
    "Let's compute all the projected keys and values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ugizxMq7AGF1",
    "outputId": "21645d2d-344e-494f-d2dd-62ccd4783356"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: tensor([[0.4207, 0.2279, 1.2477, 0.9477, 0.7168],\n",
      "        [0.6885, 0.4568, 1.4283, 1.4383, 1.2989],\n",
      "        [0.6925, 0.4572, 1.4219, 1.4353, 1.2789],\n",
      "        [0.3541, 0.2496, 0.7168, 0.7615, 0.7592],\n",
      "        [0.5700, 0.3358, 0.9077, 0.9768, 0.5587],\n",
      "        [0.3412, 0.2621, 0.8338, 0.8443, 0.9977]])\n",
      "values: tensor([[0.6973, 0.4836, 1.1020, 1.0985, 0.9538],\n",
      "        [1.2123, 0.6055, 1.0735, 1.3861, 1.5435],\n",
      "        [1.2069, 0.6000, 1.0771, 1.3826, 1.5277],\n",
      "        [0.6661, 0.3190, 0.4886, 0.7061, 0.8630],\n",
      "        [0.7701, 0.3332, 0.8395, 0.9301, 0.8138],\n",
      "        [0.7621, 0.3945, 0.4978, 0.7934, 1.0709]])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "\n",
    "print(\"keys:\", keys)\n",
    "print(\"values:\", values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kUsvmIFV_4Ss",
    "outputId": "8dbe9b91-a65d-40d8-a728-ae77fe7f2cc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.6821, 3.9688, 3.9486, 2.1243, 2.4738, 2.4665])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the attention scores of x_2\n",
    "attention_scores_2 = query_2 @ keys.T\n",
    "attention_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6cNV7cnIBYTi",
    "outputId": "30fc9624-4185-4003-f154-b4c36bb33bae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1401, 0.2491, 0.2468, 0.1092, 0.1276, 0.1272])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute attention weights\n",
    "keys_embedding_dim = keys.shape[-1]\n",
    "attention_weights_2 = torch.softmax(attention_scores_2 / keys_embedding_dim**0.5, dim=-1)\n",
    "attention_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X1D0yBaICQDL",
    "outputId": "507b6bcb-c6e3-40ad-f16c-ba0edc16ad80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9655, 0.4942, 0.9114, 1.1371, 1.2295])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute context vectors\n",
    "context_vector_2 = attention_weights_2 @ values\n",
    "context_vector_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QXvRnhOCwL1"
   },
   "source": [
    "## Implement a Self-Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "JRybDDwICduL"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttentionV1(nn.Module):\n",
    "  def __init__(self, input_embedding_dim, output_embedding_dim):\n",
    "    super().__init__()\n",
    "    self.W_query = nn.Parameter(torch.rand(input_embedding_dim, output_embedding_dim))\n",
    "    self.W_key = nn.Parameter(torch.rand(input_embedding_dim, output_embedding_dim))\n",
    "    self.W_value = nn.Parameter(torch.rand(input_embedding_dim, output_embedding_dim))\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    keys = inputs @ self.W_key\n",
    "    values = inputs @ self.W_value\n",
    "    queries = inputs @ self.W_query\n",
    "\n",
    "    attention_scores = queries @ keys.T\n",
    "    attention_weights = torch.softmax(\n",
    "        attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "    context_vectors = attention_weights @ values\n",
    "    return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SwsYMYeXMVRR",
    "outputId": "f606df13-363d-49a1-c1f0-43273273e17e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9414, 0.4835, 0.8960, 1.1131, 1.1991],\n",
       "        [0.9655, 0.4942, 0.9114, 1.1371, 1.2295],\n",
       "        [0.9641, 0.4936, 0.9103, 1.1357, 1.2278],\n",
       "        [0.9298, 0.4771, 0.8825, 1.0980, 1.1843],\n",
       "        [0.9173, 0.4713, 0.8726, 1.0845, 1.1688],\n",
       "        [0.9458, 0.4847, 0.8955, 1.1156, 1.2045]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(211)\n",
    "sa_v1 = SelfAttentionV1(input_embedding_dim, output_embedding_dim)\n",
    "context_vectors = sa_v1(inputs)\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-gNMn1AMuNU"
   },
   "source": [
    "## Implement a Self-Attention Class With Linear Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "-Bj72H8ZMcbg"
   },
   "outputs": [],
   "source": [
    "class SelfAttentionV2(nn.Module):\n",
    "  def __init__(self, input_embedding_dim, output_embedding_dim, qkv_bias=False):\n",
    "    super().__init__()\n",
    "    self.W_query = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
    "                             bias=qkv_bias)\n",
    "    self.W_key = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
    "                             bias=qkv_bias)\n",
    "    self.W_value = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
    "                             bias=qkv_bias)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    keys = self.W_key(inputs)\n",
    "    values = self.W_value(inputs)\n",
    "    queries = self.W_query(inputs)\n",
    "\n",
    "    attention_scores = queries @ keys.T\n",
    "    attention_weights = torch.softmax(\n",
    "        attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "    context_vectors = attention_weights @ values\n",
    "    return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bOaIC_yxNXA-",
    "outputId": "c8c68998-e6c8-4d59-a101-64ff5ebfe26d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2783,  0.4747, -0.4724, -0.0341,  0.3074],\n",
       "        [ 0.2768,  0.4746, -0.4740, -0.0356,  0.3078],\n",
       "        [ 0.2768,  0.4747, -0.4741, -0.0353,  0.3078],\n",
       "        [ 0.2798,  0.4775, -0.4724, -0.0288,  0.3086],\n",
       "        [ 0.2779,  0.4780, -0.4733, -0.0269,  0.3090],\n",
       "        [ 0.2799,  0.4764, -0.4724, -0.0316,  0.3081]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(211)\n",
    "sa_v2 = SelfAttentionV2(input_embedding_dim, output_embedding_dim)\n",
    "context_vectors = sa_v2(inputs)\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8WP_-3pObGl"
   },
   "source": [
    "## Causal Attention Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2M2C9oZwNZV_",
    "outputId": "04e6db8c-025a-4247-db82-3d03d36de768"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1403, -0.0855, -0.1058,  0.0263, -0.4402,  0.2156],\n",
       "        [-0.1708, -0.0352, -0.0575,  0.0688, -0.4432,  0.2793],\n",
       "        [-0.1734, -0.0347, -0.0566,  0.0683, -0.4349,  0.2748],\n",
       "        [-0.0777, -0.0100, -0.0220,  0.0403, -0.2321,  0.1540],\n",
       "        [-0.1706, -0.0153, -0.0234,  0.0407, -0.1629,  0.1161],\n",
       "        [-0.0599, -0.0171, -0.0340,  0.0472, -0.3295,  0.2079]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attention_scores = queries @ keys.T\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jw22moHsPRW9",
    "outputId": "809f3f45-5cef-4501-ff0f-8ab6fa6e6cd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[False,  True,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True,  True],\n",
      "        [False, False, False,  True,  True,  True],\n",
      "        [False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False,  True],\n",
      "        [False, False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "# create the lower triangle matrix of 1's\n",
    "context_length = inputs.shape[0]\n",
    "masked_matrix = torch.triu(torch.ones(context_length, context_length),\n",
    "                           diagonal=1\n",
    "                           )\n",
    "print(masked_matrix)\n",
    "print(masked_matrix.bool())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lx-j4d0gPuV9",
    "outputId": "e440411d-742c-401f-f059-3ab735d735f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1403,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.1708, -0.0352,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.1734, -0.0347, -0.0566,    -inf,    -inf,    -inf],\n",
       "        [-0.0777, -0.0100, -0.0220,  0.0403,    -inf,    -inf],\n",
       "        [-0.1706, -0.0153, -0.0234,  0.0407, -0.1629,    -inf],\n",
       "        [-0.0599, -0.0171, -0.0340,  0.0472, -0.3295,  0.2079]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform masked attention: fill upper triangle of attention_scores with -inf\n",
    "masked_attention_scores = attention_scores.masked_fill(\n",
    "    masked_matrix.bool(), -torch.inf)\n",
    "masked_attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kORX3A3kP-2p",
    "outputId": "f69a9433-2441-45fb-a9f4-de1350618f7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4848, 0.5152, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3208, 0.3413, 0.3380, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2433, 0.2508, 0.2494, 0.2565, 0.0000, 0.0000],\n",
       "        [0.1907, 0.2045, 0.2037, 0.2097, 0.1914, 0.0000],\n",
       "        [0.1641, 0.1673, 0.1660, 0.1722, 0.1455, 0.1850]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute masked attention weights\n",
    "masked_attention_weights = torch.softmax(\n",
    "    masked_attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "masked_attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QK7WgxdlS0Md"
   },
   "source": [
    "## Masked Attention Weights + Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QKINtb4JSod-",
    "outputId": "2bc5e1e5-43c1-4af0-a92c-43e8c7703080"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.9697, 1.0303, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6415, 0.6826, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.4989, 0.5130, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.4075, 0.4193, 0.0000, 0.0000],\n",
       "        [0.3282, 0.3346, 0.3320, 0.3443, 0.0000, 0.3700]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(211)\n",
    "dropout_05 = torch.nn.Dropout(p=0.5)\n",
    "dropout_05(masked_attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYklxXgVTZoC"
   },
   "source": [
    "## Implement a Causual Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jky4e_iBTFkW",
    "outputId": "11fbe50a-1a2f-42b6-e757-ad0dcdaac68f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch setup\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "vu82Uxw4T1qS"
   },
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "  def __init__(self,\n",
    "               input_embedding_dim,\n",
    "               output_embedding_dim,\n",
    "               context_length,\n",
    "               dropout,\n",
    "               qkv_bias=False):\n",
    "    super().__init__()\n",
    "    self.output_embedding_dim = output_embedding_dim\n",
    "    self.W_query = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
    "                             bias=qkv_bias)\n",
    "    self.W_key = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
    "                             bias=qkv_bias)\n",
    "    self.W_value = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
    "                             bias=qkv_bias)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.register_buffer(\n",
    "        \"mask\",\n",
    "        torch.triu(torch.ones(context_length, context_length),\n",
    "                                            diagonal=1))\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    batch, num_tokens, input_embedding_dim = inputs.shape\n",
    "    keys = self.W_key(inputs)\n",
    "    values = self.W_value(inputs)\n",
    "    queries = self.W_query(inputs)\n",
    "\n",
    "    attention_scores = queries @ keys.transpose(1, 2)\n",
    "    attention_scores.masked_fill_(\n",
    "        self.mask.bool()[:num_tokens, :num_tokens], - torch.inf)\n",
    "    masked_attention_weight = torch.softmax(\n",
    "        attention_scores / (keys.shape[-1]**0.5),\n",
    "        dim=-1)\n",
    "    masked_attention_dropout_weight = self.dropout(masked_attention_weight)\n",
    "\n",
    "    context_vector = masked_attention_dropout_weight @ values\n",
    "    return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jTjmgbV9ir6e",
    "outputId": "5984f9ba-5a4f-4f50-cd91-5e8d9b741ecd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vector shape:  torch.Size([2, 6, 5])\n"
     ]
    }
   ],
   "source": [
    "# create an instance\n",
    "torch.manual_seed(211)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(input_embedding_dim=input_embedding_dim,\n",
    "                     output_embedding_dim=output_embedding_dim,\n",
    "                     context_length=context_length,\n",
    "                     dropout=0.2)\n",
    "context_vectors = ca(batch)\n",
    "print(\"context_vector shape: \", context_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dmo8EJwejpUT"
   },
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3IQKOURkqBC"
   },
   "source": [
    "### Create a Wrapper Class to Implement Multi-Head Attention (SEQUENTIALLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "LH9s883cjY2c"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "  def __init__(self,\n",
    "               input_embedding_dim,\n",
    "               output_embedding_dim,\n",
    "               context_length,\n",
    "               dropout,\n",
    "               num_heads,\n",
    "               qkv_bias=False):\n",
    "    super().__init__()\n",
    "    self.heads = nn.ModuleList(\n",
    "        [CausalAttention(input_embedding_dim,\n",
    "                         output_embedding_dim,\n",
    "                         context_length,\n",
    "                         dropout,\n",
    "                         qkv_bias)\n",
    "            for _ in range(num_heads)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    return torch.cat(\n",
    "        [heads(inputs) for heads in self.heads], # process SEQUENTIALLY\n",
    "        dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ltmlPTY0l7s4",
    "outputId": "b1b886ee-5876-4286-9c22-d1d53fab44fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vectors shape:  torch.Size([2, 6, 10])\n",
      "context_vectors: \n",
      "\n",
      " tensor([[[ 0.7202,  0.5802, -0.3213,  0.0145,  0.3137,  0.5136,  0.5068,\n",
      "           0.6836,  0.4716, -0.4601],\n",
      "         [ 0.5586,  0.6915, -0.5847, -0.0264,  0.4228,  0.4128,  0.0706,\n",
      "           0.3255,  0.1374, -0.3347],\n",
      "         [ 0.3679,  0.4561, -0.3822, -0.0102,  0.2780,  0.5537,  0.0995,\n",
      "           0.4337,  0.1837, -0.4539],\n",
      "         [ 0.2402,  0.5018, -0.5408, -0.0536,  0.3349,  0.3344,  0.0324,\n",
      "           0.2566,  0.0996, -0.2617],\n",
      "         [ 0.3334,  0.5203, -0.5027, -0.0410,  0.3332,  0.6177,  0.2268,\n",
      "           0.5055,  0.2518, -0.5587],\n",
      "         [ 0.2826,  0.4628, -0.4534, -0.0323,  0.2980,  0.6232,  0.1607,\n",
      "           0.5190,  0.2407, -0.5171]],\n",
      "\n",
      "        [[ 0.7202,  0.5802, -0.3213,  0.0145,  0.3137,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5586,  0.6915, -0.5847, -0.0264,  0.4228,  0.6798,  0.3340,\n",
      "           0.6809,  0.3825, -0.5738],\n",
      "         [ 0.3679,  0.4561, -0.3822, -0.0102,  0.2780,  0.7343,  0.2777,\n",
      "           0.6741,  0.3495, -0.6156],\n",
      "         [ 0.3144,  0.4437, -0.4129, -0.0391,  0.2803,  0.3363,  0.1668,\n",
      "           0.3333,  0.1873, -0.2874],\n",
      "         [ 0.3532,  0.5547, -0.4901,  0.0468,  0.3447,  0.2698,  0.0229,\n",
      "           0.2087,  0.0806, -0.2080],\n",
      "         [ 0.3249,  0.5264, -0.5059, -0.0218,  0.3366,  0.6232,  0.1607,\n",
      "           0.5190,  0.2407, -0.5171]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# variable setup\n",
    "torch.manual_seed(211)\n",
    "context_length = batch.shape[1]\n",
    "input_embedding_dim = 3\n",
    "output_embedding_dim = 5\n",
    "\n",
    "\n",
    "# create an instance\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    input_embedding_dim,\n",
    "    output_embedding_dim,\n",
    "    context_length,\n",
    "    dropout=0.2,\n",
    "    num_heads=2\n",
    ")\n",
    "context_vectors = mha(batch)\n",
    "\n",
    "print(\"context_vectors shape: \", context_vectors.shape)\n",
    "print(\"context_vectors: \\n\\n\", context_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFMMvEcUnbOT"
   },
   "source": [
    "Implement Multi-Head Attention With Weight Splits (PARALLELLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "onbmcsi5mqfd"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self,\n",
    "               input_embedding_dim,\n",
    "               output_embedding_dim,\n",
    "               context_length,\n",
    "               dropout,\n",
    "               num_heads,\n",
    "               qkv_bias=False):\n",
    "    super().__init__()\n",
    "    assert (output_embedding_dim % num_heads == 0), \\\n",
    "          \"output_embedding_dim must be divisible by num_heads\"\n",
    "\n",
    "    self.output_embedding_dim = output_embedding_dim\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = output_embedding_dim // num_heads\n",
    "    self.W_query = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
    "                             bias=qkv_bias)\n",
    "    self.W_key = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
    "                             bias=qkv_bias)\n",
    "    self.W_value = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
    "                             bias=qkv_bias)\n",
    "    self.output_projection = nn.Linear(output_embedding_dim,\n",
    "                                       output_embedding_dim) # to combine head outputs\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.register_buffer(\n",
    "        \"mask\",\n",
    "        torch.triu(torch.ones(context_length, context_length),\n",
    "                                            diagonal=1))\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    batch, num_tokens, input_embedding_dim = inputs.shape\n",
    "\n",
    "    # qkv shapes : (batch, num_tokens, output_embedding_dim)\n",
    "    keys = self.W_key(inputs)\n",
    "    values = self.W_value(inputs)\n",
    "    queries = self.W_query(inputs)\n",
    "\n",
    "    # qkv shapes : (batch, num_tokens, num_heads, head_dim)\n",
    "    keys = keys.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "    values = values.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "    queries = queries.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "    # qkv shapes : (batch, num_heads, num_tokens, head_dim)\n",
    "    keys = keys.transpose(1, 2)\n",
    "    values = values.transpose(1, 2)\n",
    "    queries = queries.transpose(1, 2)\n",
    "\n",
    "    # compute attention scores for each head\n",
    "    attention_scores = queries @ keys.transpose(3, 2)\n",
    "    attention_scores.masked_fill(\n",
    "        self.mask.bool()[:num_tokens, :num_tokens], - torch.inf)\n",
    "\n",
    "    # compute attention weights + dropout\n",
    "    masked_attention_weight = torch.softmax(\n",
    "        attention_scores / (keys.shape[-1]**0.5),\n",
    "        dim=-1)\n",
    "    masked_attention_dropout_weight = self.dropout(masked_attention_weight)\n",
    "\n",
    "    # compute context vectors\n",
    "    # shape : (batch, num_tokens, num_heads, head_dim)\n",
    "    context_vector = (masked_attention_dropout_weight @ values).transpose(1, 2)\n",
    "\n",
    "    # combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "    # shape : (batch, num_tokens, output_embedding_dim)\n",
    "    context_vector = context_vector.contiguous().view(\n",
    "        batch, num_tokens, self.output_embedding_dim)\n",
    "\n",
    "    # linear projection (optional)\n",
    "    context_vector = self.output_projection(context_vector)\n",
    "\n",
    "    return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dZC0ppudsI5V",
    "outputId": "199daf7e-6122-459f-8c79-b39b87cb1ff6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "context_vectors shape:  torch.Size([2, 6, 6])\n",
      "context_vectors: \n",
      "\n",
      " tensor([[[-0.1217,  0.6112,  0.7202,  0.5802, -0.1637, -0.5053],\n",
      "         [-0.2210,  0.4563,  0.5639,  0.6879,  0.0155, -0.3606],\n",
      "         [-0.2433,  0.4075,  0.5112,  0.7247, -0.0413, -0.6645],\n",
      "         [-0.2400,  0.3400,  0.3825,  0.5433, -0.0060, -0.5926],\n",
      "         [-0.1524,  0.2967,  0.2414,  0.5206, -0.0074, -0.4772],\n",
      "         [-0.1859,  0.2038,  0.2334,  0.5033,  0.0194, -0.2693]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.2210,  0.4563,  0.5639,  0.6879, -0.0694, -0.6227],\n",
      "         [-0.1442,  0.2974,  0.5112,  0.7247,  0.0000,  0.0000],\n",
      "         [-0.1672,  0.2592,  0.1380,  0.2996, -0.0060, -0.5926],\n",
      "         [-0.1342,  0.2008,  0.2775,  0.4027, -0.0591, -0.5935],\n",
      "         [-0.0740,  0.1984,  0.3534,  0.5999, -0.0239, -0.3668]]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# variable setup\n",
    "torch.manual_seed(211)\n",
    "print(batch.shape)\n",
    "batch_size, context_length, input_embedding_dim = batch.shape\n",
    "output_embedding_dim = 2\n",
    "\n",
    "\n",
    "# create an instance\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    input_embedding_dim,\n",
    "    output_embedding_dim,\n",
    "    context_length,\n",
    "    dropout=0.2,\n",
    "    num_heads=3\n",
    ")\n",
    "context_vectors = mha(batch)\n",
    "\n",
    "print(\"context_vectors shape: \", context_vectors.shape)\n",
    "print(\"context_vectors: \\n\\n\", context_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhCoX6gi3mtO"
   },
   "source": [
    "## Some Note On `.contiguous()`\n",
    "\n",
    "> A tensor is **contiguous** if its data is stored in memory in row-major (C-style) order — meaning the next element in a row is stored next to the current one in memory. Think of it as how the tensor is laid out in memory, not how it looks visually.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yFrYxJvq3o0w",
    "outputId": "fcf65bd3-93cc-4d3f-bec7-d9ed64c3ff5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([2, 3])\n",
      "Is contiguous? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 2x3 tensor\n",
    "x = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "print(\"Original shape:\", x.shape)\n",
    "print(\"Is contiguous?\", x.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hZPCzmkk3r89",
    "outputId": "8e682bf5-59b9-4f91-ab91-dec00c999761"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "Transposed shape: torch.Size([3, 2])\n",
      "Is contiguous? False\n"
     ]
    }
   ],
   "source": [
    "# Transpose it\n",
    "x_t = x.transpose(0, 1)\n",
    "print(x_t)\n",
    "print(\"Transposed shape:\", x_t.shape)\n",
    "print(\"Is contiguous?\", x_t.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fsv5xTAH7tI6",
    "outputId": "b276f936-5d5f-4e93-b8af-8e3d68ba816e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x.stride())      # e.g., (3, 1) → row-major: step 3 for next row\n",
    "print(x_t.stride())    # e.g., (1, 3) → shows it’s not row-major anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NA0H_kPL3r4l",
    "outputId": "2f56b0b3-e538-4980-f6ef-4d2ab1fc8ddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n"
     ]
    }
   ],
   "source": [
    "# Try to view it — this will raise an error if you don't call contiguous()\n",
    "try:\n",
    "    x_t.view(-1)\n",
    "except RuntimeError as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8hvnevQb3rxA",
    "outputId": "1e38a77e-3ddd-4a60-ecca-bcb9222061a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed and reshaped: tensor([1, 4, 2, 5, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "# Fix with .contiguous()\n",
    "x_fixed = x_t.contiguous().view(-1)\n",
    "print(\"Fixed and reshaped:\", x_fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAIR1yb-2duA"
   },
   "source": [
    "## Some Note On `output_projection()`\n",
    "\n",
    "This linear layer:\n",
    "\n",
    "- Mixes information across heads\n",
    "\n",
    "- Allows the model to recombine features learned by each head\n",
    "\n",
    "- Helps fuse multiple perspectives (heads) into a single output space\n",
    "\n",
    "Without it:\n",
    "- Each head contributes independently\n",
    "\n",
    "- No opportunity to re-weight or mix features learned by other heads\n",
    "\n",
    "- You would limit the representational power\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9NZ9-dBltF4A",
    "outputId": "bc4c2858-c412-4bf6-f16f-131469f523f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook runtime: 0 min 3.18 sec\n"
     ]
    }
   ],
   "source": [
    "notebook_end_time = time.time()\n",
    "runtime_in_seconds = notebook_end_time - notebook_start_time\n",
    "\n",
    "# format as minutes and seconds\n",
    "minutes, seconds = divmod(runtime_in_seconds, 60)\n",
    "print(f\"Notebook runtime: {int(minutes)} min {seconds:.2f} sec\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "LLMs from scratch (uv)",
   "language": "python",
   "name": "llms_uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
