{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Buffers in Pytorch are useful when dealing with GPUs. Unlike parameters, buffers do not require gradient computation, but they still need to be on the correct device\n"
      ],
      "metadata": {
        "id": "s3k8ycUR45c0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup input\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "torch.manual_seed(211)\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "context_length = batch.shape[1]\n",
        "input_embedding_dim = inputs.shape[1]\n",
        "output_embedding_dim = 5\n",
        "\n",
        "print(batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URdH65c3cQO4",
        "outputId": "e41ae200-8de6-48d4-a607-15bc59c42b89"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Causal Self-Attention Class Without Buffers"
      ],
      "metadata": {
        "id": "bJy6ow3b5Z3l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YyCnDY0_V5oh"
      },
      "outputs": [],
      "source": [
        "class CausalAttentionWithoutBuffers(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_embedding_dim,\n",
        "               output_embedding_dim,\n",
        "               context_length,\n",
        "               dropout,\n",
        "               qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.output_embedding_dim = output_embedding_dim\n",
        "    self.W_query = nn.Linear(input_embedding_dim,\n",
        "                             output_embedding_dim,\n",
        "                             bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(input_embedding_dim,\n",
        "                           output_embedding_dim,\n",
        "                           bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(input_embedding_dim,\n",
        "                              output_embedding_dim,\n",
        "                              bias=qkv_bias)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.mask = torch.triu(\n",
        "        torch.ones(context_length, context_length),\n",
        "        diagonal=1)\n",
        "\n",
        "  def forward(self,inputs):\n",
        "    batch, num_tokens, input_embedding_dim = inputs.shape\n",
        "    keys = self.W_key(inputs)\n",
        "    queries = self.W_query(inputs)\n",
        "    values = self.W_value(inputs)\n",
        "\n",
        "    attention_scores = queries @ keys.transpose(1, 2)\n",
        "    attention_scores.masked_fill_(\n",
        "        self.mask.bool()[:num_tokens, :num_tokens], - torch.inf)\n",
        "    masked_attention_weight = torch.softmax(\n",
        "        attention_scores / (keys.shape[-1]**0.5),\n",
        "        dim=-1)\n",
        "    masked_attention_dropout_weight = self.dropout(masked_attention_weight)\n",
        "\n",
        "    context_vector = masked_attention_dropout_weight @ values\n",
        "    return context_vector"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create an instance\n",
        "ca_wo_buffers = CausalAttentionWithoutBuffers(\n",
        "    input_embedding_dim=input_embedding_dim,\n",
        "    output_embedding_dim=output_embedding_dim,\n",
        "    context_length=context_length,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "  context_vectors = ca_wo_buffers(batch)\n",
        "\n",
        "print(context_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGo972ZaZ8BI",
        "outputId": "e74f116e-42bc-41ad-d63e-355eaa63f14f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.6402,  0.5157, -0.2856,  0.0129,  0.2789],\n",
            "         [ 0.4965,  0.6147, -0.5197, -0.0234,  0.3758],\n",
            "         [ 0.4503,  0.6470, -0.5922, -0.0287,  0.4065],\n",
            "         [ 0.3693,  0.5715, -0.5502, -0.0445,  0.3656],\n",
            "         [ 0.3410,  0.5678, -0.5272,  0.0224,  0.3588],\n",
            "         [ 0.3110,  0.5293, -0.5249, -0.0352,  0.3423]],\n",
            "\n",
            "        [[ 0.6402,  0.5157, -0.2856,  0.0129,  0.2789],\n",
            "         [ 0.4965,  0.6147, -0.5197, -0.0234,  0.3758],\n",
            "         [ 0.4503,  0.6470, -0.5922, -0.0287,  0.4065],\n",
            "         [ 0.3693,  0.5715, -0.5502, -0.0445,  0.3656],\n",
            "         [ 0.2671,  0.4230, -0.3759,  0.0342,  0.2633],\n",
            "         [ 0.2814,  0.4623, -0.4287,  0.0150,  0.2920]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything workds fine.\n",
        "\n",
        "Now let's transfer the `CausalAttentionWithoutBuffers` module to a GPU device"
      ],
      "metadata": {
        "id": "BOB66VAqdZBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Machine has GPU:\", torch.cuda.is_available())\n",
        "\n",
        "batch = batch.to(\"cuda\")\n",
        "ca_wo_buffers.to(\"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBJN3II6c-SD",
        "outputId": "f3554382-9926-4075-cf36-a7083e76fe2b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine has GPU: True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CausalAttentionWithoutBuffers(\n",
              "  (W_query): Linear(in_features=3, out_features=5, bias=False)\n",
              "  (W_key): Linear(in_features=3, out_features=5, bias=False)\n",
              "  (W_value): Linear(in_features=3, out_features=5, bias=False)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  context_vectors = ca_wo_buffers(batch)\n",
        "\n",
        "print(context_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "mofljrVed2EQ",
        "outputId": "fc5ab2f0-8266-4195-c9a4-8d0d380bee88"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "expected self and mask to be on the same device, but got mask on cpu and self on cuda:0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-6b4f4bd6ba49>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mcontext_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mca_wo_buffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-9120da14e997>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueries\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     attention_scores.masked_fill_(\n\u001b[0m\u001b[1;32m     32\u001b[0m         self.mask.bool()[:num_tokens, :num_tokens], - torch.inf)\n\u001b[1;32m     33\u001b[0m     masked_attention_weight = torch.softmax(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected self and mask to be on the same device, but got mask on cpu and self on cuda:0"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems like we attempted a matrix multiplication between a tensor on a GPU and a tensor on a CPU. But we moved the module to the GPU!?\n",
        "\n",
        "Let's double check the device locations of some of the tensors:"
      ],
      "metadata": {
        "id": "u5sJHOKweAKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"W_query.device:\", ca_wo_buffers.W_query.weight.device)\n",
        "print(\"mask.device:\", ca_wo_buffers.mask.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1LVKk2md7Sc",
        "outputId": "b959cbf7-5a4c-45af-96d6-5896e3a32079"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_query.device: cuda:0\n",
            "mask.device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " `mask` was not moved onto the GPU. That's because it's not a PyTorch parameter like the weights\n",
        "\n",
        " Let's manually move it to the GPU via `.to(\"cuda\")`:"
      ],
      "metadata": {
        "id": "LPEjbSNreQww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ca_wo_buffers.mask = ca_wo_buffers.mask.to(\"cuda\")\n",
        "print(\"mask.device:\", ca_wo_buffers.mask.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqnFh9zUeOXt",
        "outputId": "b2a6feb0-e376-4f91-a5f4-ec6f386c0bca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mask.device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    context_vecs = ca_wo_buffers(batch)\n",
        "\n",
        "print(context_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Le935A4Gef1n",
        "outputId": "533dbf14-40ad-41d8-8181-2ffba90db5c6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.6402,  0.5157, -0.2856,  0.0129,  0.2789],\n",
            "         [ 0.3104,  0.2500, -0.1385,  0.0063,  0.1352],\n",
            "         [ 0.4503,  0.6470, -0.5922, -0.0287,  0.4065],\n",
            "         [ 0.2135,  0.4460, -0.4807, -0.0476,  0.2977],\n",
            "         [ 0.3139,  0.4931, -0.4356,  0.0416,  0.3064],\n",
            "         [ 0.3110,  0.5293, -0.5249, -0.0352,  0.3423]],\n",
            "\n",
            "        [[ 0.6402,  0.5157, -0.2856,  0.0129,  0.2789],\n",
            "         [ 0.4965,  0.6147, -0.5197, -0.0234,  0.3758],\n",
            "         [ 0.4503,  0.6470, -0.5922, -0.0287,  0.4065],\n",
            "         [ 0.2795,  0.3944, -0.3671, -0.0348,  0.2492],\n",
            "         [ 0.3410,  0.5678, -0.5272,  0.0224,  0.3588],\n",
            "         [ 0.3110,  0.5293, -0.5249, -0.0352,  0.3423]]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It worked!!!\n",
        "\n",
        "But remembering to move individual tensors to the GPU can be tedious.\n",
        "\n",
        "Let's use `register_buffer` to register the `mask` as a buffer:"
      ],
      "metadata": {
        "id": "UJP1KdSWeoyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Causal Self-Attention Class With Buffers"
      ],
      "metadata": {
        "id": "IbTioyRQe4v-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttentionWithBuffer(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_embedding_dim,\n",
        "               output_embedding_dim,\n",
        "               context_length,\n",
        "               dropout,\n",
        "               qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.output_embedding_dim = output_embedding_dim\n",
        "    self.W_query = nn.Linear(input_embedding_dim,\n",
        "                             output_embedding_dim,\n",
        "                             bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(input_embedding_dim,\n",
        "                           output_embedding_dim,\n",
        "                           bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(input_embedding_dim,\n",
        "                              output_embedding_dim,\n",
        "                              bias=qkv_bias)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    # old:\n",
        "    # self.mask = torch.triu(\n",
        "    #     torch.ones(context_length, context_length),\n",
        "    #     diagonal=1)\n",
        "\n",
        "    # new:\n",
        "    self.register_buffer(\"mask\",\n",
        "                         torch.triu(\n",
        "                             torch.ones(context_length, context_length),\n",
        "                             diagonal=1))\n",
        "\n",
        "  def forward(self,inputs):\n",
        "    batch, num_tokens, input_embedding_dim = inputs.shape\n",
        "    keys = self.W_key(inputs)\n",
        "    queries = self.W_query(inputs)\n",
        "    values = self.W_value(inputs)\n",
        "\n",
        "    attention_scores = queries @ keys.transpose(1, 2)\n",
        "    attention_scores.masked_fill_(\n",
        "        self.mask.bool()[:num_tokens, :num_tokens], - torch.inf)\n",
        "    masked_attention_weight = torch.softmax(\n",
        "        attention_scores / (keys.shape[-1]**0.5),\n",
        "        dim=-1)\n",
        "    masked_attention_dropout_weight = self.dropout(masked_attention_weight)\n",
        "\n",
        "    context_vector = masked_attention_dropout_weight @ values\n",
        "    return context_vector"
      ],
      "metadata": {
        "id": "jc9nTLsfeiXl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, conveniently, if we move the module to the GPU, the mask will be located on the GPU as well:"
      ],
      "metadata": {
        "id": "nHUBY4QMfQZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ca_with_buffer = CausalAttentionWithBuffer(input_embedding_dim,\n",
        "                                           output_embedding_dim,\n",
        "                                           context_length,\n",
        "                                           0.1)\n",
        "ca_with_buffer.to(\"cuda\")\n",
        "\n",
        "print(\"W_query.device:\", ca_with_buffer.W_query.weight.device)\n",
        "print(\"mask.device:\", ca_with_buffer.mask.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lKKaClZfP8w",
        "outputId": "a35a16f2-f861-48b3-ee16-64fabb7062b6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_query.device: cuda:0\n",
            "mask.device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    context_vecs = ca_with_buffer(batch)\n",
        "\n",
        "print(context_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkdMEaKufbFb",
        "outputId": "b23b42a7-3aea-4787-ed61-90a2921f0302"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.6924, -0.1047,  0.2637, -0.5263,  0.0409],\n",
            "         [ 0.7756, -0.0087,  0.5004, -0.2959, -0.0401],\n",
            "         [ 0.8001,  0.0166,  0.5725, -0.2217, -0.0680],\n",
            "         [ 0.7084,  0.0397,  0.5361, -0.1610, -0.0656],\n",
            "         [ 0.6688, -0.0150,  0.4947, -0.1358, -0.0907],\n",
            "         [ 0.4174,  0.0600,  0.3324, -0.0908, -0.0254]],\n",
            "\n",
            "        [[ 0.6924, -0.1047,  0.2637, -0.5263,  0.0409],\n",
            "         [ 0.7756, -0.0087,  0.5004, -0.2959, -0.0401],\n",
            "         [ 0.5785,  0.0501,  0.4881, -0.0532, -0.0810],\n",
            "         [ 0.4953,  0.0194,  0.3557, -0.1413, -0.0365],\n",
            "         [ 0.3220, -0.0451,  0.2021, -0.1039, -0.0421],\n",
            "         [ 0.6422,  0.0302,  0.5043, -0.1089, -0.0783]]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Buffers and `state_dict`\n",
        "Another advantage of PyTorch buffers, over regular tensors, is that they get included in a model's `state_dict`:"
      ],
      "metadata": {
        "id": "t8ElCwsxfooQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# without buffer\n",
        "ca_wo_buffers.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N155UzhcflI6",
        "outputId": "cd9cb116-d446-4b06-f577-99c76e35ef53"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('W_query.weight',\n",
              "              tensor([[-0.5424,  0.0719, -0.1568],\n",
              "                      [-0.5603, -0.3087, -0.5218],\n",
              "                      [-0.2370, -0.5260,  0.4572],\n",
              "                      [-0.0705, -0.0621, -0.1545],\n",
              "                      [ 0.3552, -0.3376, -0.2241]], device='cuda:0')),\n",
              "             ('W_key.weight',\n",
              "              tensor([[ 0.1450, -0.1837,  0.5103],\n",
              "                      [ 0.5660, -0.1473, -0.2244],\n",
              "                      [-0.2593, -0.1432,  0.1937],\n",
              "                      [ 0.4595, -0.4400, -0.5255],\n",
              "                      [ 0.4428, -0.0302, -0.0299]], device='cuda:0')),\n",
              "             ('W_value.weight',\n",
              "              tensor([[ 0.2712, -0.2171,  0.5530],\n",
              "                      [ 0.5426,  0.2207,  0.2221],\n",
              "                      [-0.3114, -0.5317, -0.0487],\n",
              "                      [ 0.4521, -0.2174, -0.1687],\n",
              "                      [ 0.2986,  0.2177,  0.1011]], device='cuda:0'))])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with buffer\n",
        "ca_with_buffer.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBQhUSl4f55-",
        "outputId": "19e4a841-0bae-47c8-e258-234bbfaa0252"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('mask',\n",
              "              tensor([[0., 1., 1., 1., 1., 1.],\n",
              "                      [0., 0., 1., 1., 1., 1.],\n",
              "                      [0., 0., 0., 1., 1., 1.],\n",
              "                      [0., 0., 0., 0., 1., 1.],\n",
              "                      [0., 0., 0., 0., 0., 1.],\n",
              "                      [0., 0., 0., 0., 0., 0.]], device='cuda:0')),\n",
              "             ('W_query.weight',\n",
              "              tensor([[-0.1119,  0.5082, -0.3537],\n",
              "                      [ 0.2697, -0.0218, -0.5054],\n",
              "                      [-0.2409, -0.1694, -0.1194],\n",
              "                      [ 0.1612, -0.0203, -0.1356],\n",
              "                      [-0.4135,  0.2456, -0.5693]], device='cuda:0')),\n",
              "             ('W_key.weight',\n",
              "              tensor([[-0.2781, -0.2590, -0.3284],\n",
              "                      [-0.0922, -0.3013,  0.3599],\n",
              "                      [ 0.3255,  0.2196, -0.0739],\n",
              "                      [ 0.1620,  0.5560,  0.4008],\n",
              "                      [-0.5715, -0.4526,  0.2410]], device='cuda:0')),\n",
              "             ('W_value.weight',\n",
              "              tensor([[ 0.4314,  0.2725,  0.4458],\n",
              "                      [-0.3770,  0.3037,  0.0251],\n",
              "                      [ 0.1726,  0.5733,  0.0867],\n",
              "                      [-0.0936,  0.3978, -0.5540],\n",
              "                      [-0.2064, -0.1111,  0.1598]], device='cuda:0'))])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "saving and loading the `mask` is maybe not useful, but in the case where it is modified, it will remain unchange even if we save and load the `state_dict`:"
      ],
      "metadata": {
        "id": "LGN-ZMEPgRvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ca_with_buffer.mask[ca_with_buffer.mask == 1.] = 2.\n",
        "ca_with_buffer.mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_9YtJkkgvTB",
        "outputId": "234ae273-2b9d-4e7f-a5aa-c8f54fd973b0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 2., 2., 2., 2., 2.],\n",
              "        [0., 0., 2., 2., 2., 2.],\n",
              "        [0., 0., 0., 2., 2., 2.],\n",
              "        [0., 0., 0., 0., 2., 2.],\n",
              "        [0., 0., 0., 0., 0., 2.],\n",
              "        [0., 0., 0., 0., 0., 0.]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(ca_with_buffer.state_dict(), \"model.pth\")\n",
        "\n",
        "new_ca_with_buffer = CausalAttentionWithBuffer(input_embedding_dim,\n",
        "                                               output_embedding_dim,\n",
        "                                               context_length,\n",
        "                                               0.1)\n",
        "new_ca_with_buffer.load_state_dict(torch.load(\"model.pth\"))\n",
        "\n",
        "new_ca_with_buffer.mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F8L3XSegwB5",
        "outputId": "3c0911fd-08d7-4281-e794-8bd0fad76599"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 2., 2., 2., 2., 2.],\n",
              "        [0., 0., 2., 2., 2., 2.],\n",
              "        [0., 0., 0., 2., 2., 2.],\n",
              "        [0., 0., 0., 0., 2., 2.],\n",
              "        [0., 0., 0., 0., 0., 2.],\n",
              "        [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "without buffers, this is not true:"
      ],
      "metadata": {
        "id": "SvQNS9Ahhhe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ca_wo_buffers.mask[ca_wo_buffers.mask == 1.] = 2.\n",
        "\n",
        "torch.save(ca_wo_buffers.state_dict(), \"model.pth\")\n",
        "\n",
        "new_ca_wo_buffer = CausalAttentionWithoutBuffers(input_embedding_dim,\n",
        "                                                 output_embedding_dim,\n",
        "                                                 context_length,\n",
        "                                                 0.1)\n",
        "new_ca_wo_buffer.load_state_dict(torch.load(\"model.pth\"))\n",
        "\n",
        "new_ca_wo_buffer.mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSbAKkbIhaky",
        "outputId": "8c767234-bb00-49a1-b02f-097bc52f7968"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 1., 1., 1., 1., 1.],\n",
              "        [0., 0., 1., 1., 1., 1.],\n",
              "        [0., 0., 0., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9f___RZIhyGZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}