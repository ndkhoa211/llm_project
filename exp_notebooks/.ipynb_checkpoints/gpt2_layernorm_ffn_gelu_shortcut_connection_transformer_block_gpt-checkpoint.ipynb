{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "notebook_start_time = time.time()"
      ],
      "metadata": {
        "id": "80lwPwQjVVKS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An LLM Architecture (Backbone)"
      ],
      "metadata": {
        "id": "cjrfqGg19--I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set the configuration:"
      ],
      "metadata": {
        "id": "K0wX2QPw-Kig"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jMThqIsL9CQO"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Placeholder GPT2 Model Architecture Class"
      ],
      "metadata": {
        "id": "nVMi_qlRZkUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DummyGPTModel(nn.Module):\n",
        "  def __init__(self,\n",
        "               config\n",
        "               ):\n",
        "    super().__init__()\n",
        "    self.token_emb = nn.Embedding(config[\"vocab_size\"],\n",
        "                                  config[\"emb_dim\"])\n",
        "    self.position_emb = nn.Embedding(config[\"context_length\"],\n",
        "                                     config[\"emb_dim\"])\n",
        "    self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "    # a placeholder for TransformerBlock\n",
        "    self.transformer_blocks = nn.Sequential(\n",
        "        *[DummyTransformerBlock(config) for _ in range(config[\"n_layers\"])])\n",
        "\n",
        "    # a placeholder for LayerNorm\n",
        "    self.final_norm = DummyLayerNorm(config[\"emb_dim\"])\n",
        "\n",
        "    self.out_head = nn.Linear(config[\"emb_dim\"],\n",
        "                              config[\"vocab_size\"],\n",
        "                              bias=False)\n",
        "\n",
        "  def forward(self, input_token):\n",
        "    batch_size, sequence_length = input_token.shape\n",
        "    token_embeds = self.token_emb(input_token)\n",
        "    position_embeds = self.position_emb(\n",
        "        torch.arange(sequence_length,\n",
        "                     device=input_token.device))\n",
        "    embeds = token_embeds + position_embeds\n",
        "    x = self.drop_emb(embeds)\n",
        "    x = self.transformer_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits\n",
        "\n",
        "\n",
        "# placeholder\n",
        "class DummyTransformerBlock(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x\n",
        "\n",
        "\n",
        "# placeholder\n",
        "class DummyLayerNorm(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x"
      ],
      "metadata": {
        "id": "KaxnqgJS-wT5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup example\n",
        "import tiktoken\n",
        "\n",
        "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "batch = []\n",
        "\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "\n",
        "batch.append(torch.tensor(bpe_tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(bpe_tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch)\n",
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-ezJkcmc7nB",
        "outputId": "4522fc60-03d4-47ad-f3f8-ced50dc03d69"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[6109, 3626, 6100,  345],\n",
              "        [6109, 1110, 6622,  257]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create an instance\n",
        "torch.manual_seed(211)\n",
        "model = DummyGPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "logits = model(batch)\n",
        "print(\"output shape: \", logits.shape)\n",
        "logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp5JsM-HdeTi",
        "outputId": "424af401-6b87-447e-f91a-6eb754587d72"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output shape:  torch.Size([2, 4, 50257])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.4087, -1.2679,  0.3791,  ...,  0.2964,  0.7493,  0.3239],\n",
              "         [-0.2316,  0.2405, -0.4450,  ...,  0.3991,  0.0026,  0.1075],\n",
              "         [-0.0059,  0.2291, -0.9134,  ..., -1.7940, -0.0074, -0.3314],\n",
              "         [ 1.5600, -0.6588,  0.6326,  ..., -0.5926, -0.8299,  2.0698]],\n",
              "\n",
              "        [[-0.8229, -1.3703,  0.1186,  ...,  0.2430,  0.7330,  0.2529],\n",
              "         [-0.7035, -0.2469, -0.4727,  ...,  0.0581,  0.4801,  0.4443],\n",
              "         [ 0.6955, -0.2896,  0.0219,  ..., -1.2846,  0.4151, -0.0240],\n",
              "         [ 0.5667, -0.1500, -0.0109,  ..., -1.5656, -1.3297,  2.0111]]],\n",
              "       grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement LayerNorm"
      ],
      "metadata": {
        "id": "ev8RMHLOeZgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's experiment:"
      ],
      "metadata": {
        "id": "eAddun8Gepo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example setup\n",
        "torch.manual_seed(211)\n",
        "\n",
        "# create 3 training examples with 5 dimensions each\n",
        "batch_example = torch.randn(3, 5)\n",
        "\n",
        "layer = nn.Sequential(nn.Linear(5, 7),\n",
        "                      nn.ReLU())\n",
        "\n",
        "output = layer(batch_example)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPOnv_4vd7ja",
        "outputId": "b287b578-b8b5-4431-ea99-892afcc3ca9e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0540, 0.2893, 0.0442, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.8197, 0.1849, 0.1102, 1.7938, 0.3290, 1.2042, 0.0000],\n",
              "        [0.3081, 0.1382, 0.2541, 0.0000, 0.5109, 0.0000, 0.0000]],\n",
              "       grad_fn=<ReluBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute mean and variance of each sample\n",
        "mean = output.mean(dim=-1, keepdim=True)\n",
        "var = output.var(dim=-1, keepdim=True)\n",
        "print(\"mean: \", mean)\n",
        "print(\"variance: \", var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tp4IIwPjfX6W",
        "outputId": "23af507d-4356-45d3-c4d3-654dabe402b4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean:  tensor([[0.1982],\n",
            "        [0.6345],\n",
            "        [0.1730]], grad_fn=<MeanBackward1>)\n",
            "variance:  tensor([[0.1536],\n",
            "        [0.4460],\n",
            "        [0.0383]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's normalize (aka layer normalization) the output:"
      ],
      "metadata": {
        "id": "0-rhSYgqf-jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "norm_output = (output - mean) / torch.sqrt(var)\n",
        "norm_mean = norm_output.mean(dim=-1, keepdim=True)\n",
        "norm_var = norm_output.var(dim=-1, keepdim=True)\n",
        "\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "print(\"normalized mean: \\n\", norm_mean)\n",
        "print(\"normalized variance: \\n\", norm_var)\n",
        "print(\"normalized layer output: \\n\", norm_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLKzU-Jsf1w_",
        "outputId": "2b5b033c-2f85-4afd-dd15-f271a61142be"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "normalized mean: \n",
            " tensor([[     0.0000],\n",
            "        [     0.0000],\n",
            "        [    -0.0000]], grad_fn=<MeanBackward1>)\n",
            "normalized variance: \n",
            " tensor([[1.0000],\n",
            "        [1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n",
            "normalized layer output: \n",
            " tensor([[ 2.1836,  0.2324, -0.3930, -0.5058, -0.5058, -0.5058, -0.5058],\n",
            "        [ 0.2772, -0.6733, -0.7852,  1.7360, -0.4575,  0.8530, -0.9502],\n",
            "        [ 0.6897, -0.1777,  0.4139, -0.8838,  1.7256, -0.8838, -0.8838]],\n",
            "       grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement LayerNorm class"
      ],
      "metadata": {
        "id": "zAaAVob4hk33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.epsilon = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    var = x.var(dim=-1,\n",
        "                unbiased=False, # Bessel's correction (n-1)\n",
        "                keepdim=True)\n",
        "    norm_x = (x - mean) / torch.sqrt(var + self.epsilon)\n",
        "    return self.scale * norm_x + self.shift"
      ],
      "metadata": {
        "id": "SuIWCiaHgYnF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup example\n",
        "layer_norm = LayerNorm(emb_dim=5)\n",
        "layer_norm_output = layer_norm(batch_example)\n",
        "mean = layer_norm_output.mean(dim=-1, keepdim=True)\n",
        "var = layer_norm_output.var(dim=-1, unbiased=False, keepdim=True)\n",
        "\n",
        "print(\"normalized mean: \\n\", mean)\n",
        "print(\"normalized variance: \\n\", var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5HDk-driSaW",
        "outputId": "10a2c1fb-4c56-47d6-cbe8-9ba99ad0f34c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "normalized mean: \n",
            " tensor([[    -0.0000],\n",
            "        [    -0.0000],\n",
            "        [    -0.0000]], grad_fn=<MeanBackward1>)\n",
            "normalized variance: \n",
            " tensor([[1.0000],\n",
            "        [1.0000],\n",
            "        [0.9999]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement a FFN With GELU Activations"
      ],
      "metadata": {
        "id": "yVz_hlRtjOpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GELU activation function\n",
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return 0.5 * x * ( 1 + torch.tanh(\n",
        "        torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "        (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))"
      ],
      "metadata": {
        "id": "GC3RkaKBjF-h"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot GELU\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "gelu = GELU()\n",
        "x = torch.linspace(-5, 5, 100)\n",
        "plt.plot(x, gelu(x))\n",
        "plt.title(\"GELU Activation Function\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"GELU(x)\")\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "gki8bcEDkhyA",
        "outputId": "e9fd7bbf-c850-4603-f130-0f03e02bdb73"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FFN module\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(config[\"emb_dim\"],       # 768\n",
        "                  4 * config[\"emb_dim\"]),  # 3072\n",
        "        GELU(),                            # 3072\n",
        "        nn.Linear(4 * config[\"emb_dim\"],   # 3072\n",
        "                  config[\"emb_dim\"])       # 768\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "_xn7uy_TkuEH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an instance\n",
        "ffn = FeedForward(GPT_CONFIG_124M)\n",
        "x = torch.randn(3, 5, 768)\n",
        "output = ffn(x)\n",
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEnDAqRYlp07",
        "outputId": "a1334c04-c018-4419-dd17-52768d305e62"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 5, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add Skip Connections"
      ],
      "metadata": {
        "id": "evrJcgC5m2Qm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make an example nn to show how skip connection affect learning ability of a model:"
      ],
      "metadata": {
        "id": "_HknY1CGnoKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExampleDeepNN(nn.Module):\n",
        "  def __init__(self, layer_sizes_list, use_skip_connection):\n",
        "    super().__init__()\n",
        "    self.use_skip_connection = use_skip_connection\n",
        "    self.layers = nn.ModuleList([\n",
        "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
        "    ])\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      # Compute the output of the current layer\n",
        "      layer_output = layer(x)\n",
        "      # Check if skip_connection can be applied\n",
        "      if self.use_skip_connection and x.shape == layer_output.shape:\n",
        "        x = x + layer_output\n",
        "      else:\n",
        "        x = layer_output\n",
        "    return x\n",
        "\n",
        "\n",
        "def print_gradients(model, x):\n",
        "    # forward pass\n",
        "    output = model(x)\n",
        "    target = torch.tensor([[0.]])\n",
        "\n",
        "    # calculate loss\n",
        "    loss = nn.MSELoss()\n",
        "    loss = loss(output, target)\n",
        "\n",
        "    # calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            # Print the mean absolute gradient of the weights\n",
        "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
      ],
      "metadata": {
        "id": "6V6_1LmXmaXy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example setup\n",
        "layer_sizes = [4, 4, 4, 4, 4, 1]\n",
        "sample_input = torch.tensor([[1., 0., -1., 1]])"
      ],
      "metadata": {
        "id": "_AqGy8QAo0qG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# without skip connection\n",
        "torch.manual_seed(211)\n",
        "model_wo_skip_connection = ExampleDeepNN(\n",
        "    layer_sizes, use_skip_connection=False\n",
        ")\n",
        "print_gradients(model_wo_skip_connection, sample_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2I3YVLfqqFW6",
        "outputId": "2cf7da8d-0ae0-465d-b47e-1174712446c7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.0.weight has gradient mean of 0.0001875293382909149\n",
            "layers.1.0.weight has gradient mean of 0.00015768714365549386\n",
            "layers.2.0.weight has gradient mean of 0.0004450336564332247\n",
            "layers.3.0.weight has gradient mean of 0.0012923781760036945\n",
            "layers.4.0.weight has gradient mean of 0.01807313971221447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with skip connection\n",
        "torch.manual_seed(211)\n",
        "model_w_skip_connection = ExampleDeepNN(\n",
        "    layer_sizes, use_skip_connection=True\n",
        ")\n",
        "print_gradients(model_w_skip_connection, sample_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SbMjIG0pj1M",
        "outputId": "ca50dc43-1674-4d78-9cde-d511b3158bfa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.0.weight has gradient mean of 0.6975078582763672\n",
            "layers.1.0.weight has gradient mean of 0.6017700433731079\n",
            "layers.2.0.weight has gradient mean of 0.6710034012794495\n",
            "layers.3.0.weight has gradient mean of 0.4883677363395691\n",
            "layers.4.0.weight has gradient mean of 4.474058151245117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement Transformer Block"
      ],
      "metadata": {
        "id": "lio4bauuJlLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "id": "bHUYqA9fqIQw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ch03 import MultiHeadAttention\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.attention = MultiHeadAttention(input_embedding_dim=config[\"emb_dim\"],\n",
        "                                        output_embedding_dim=config[\"emb_dim\"],\n",
        "                                        context_length=config[\"context_length\"],\n",
        "                                        dropout=config[\"drop_rate\"],\n",
        "                                        num_heads=config[\"n_heads\"],\n",
        "                                        qkv_bias=config[\"qkv_bias\"])\n",
        "    self.feed_forward = FeedForward(config)\n",
        "    self.layer_norm1 = LayerNorm(config[\"emb_dim\"])\n",
        "    self.layer_norm2 = LayerNorm(config[\"emb_dim\"])\n",
        "    self.drop_skip = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "  def forward(self, x):\n",
        "    # skip connection for attention block\n",
        "    shortcut = x\n",
        "    x = self.layer_norm1(x)\n",
        "    x = self.attention(x)  # shape: [batch_size, num_tokens, emb_size]\n",
        "    x = self.drop_skip(x)\n",
        "    x = shortcut + x # skip connection\n",
        "\n",
        "    # skip connection for feed forward block\n",
        "    shortcut = x\n",
        "    x = self.layer_norm2(x)\n",
        "    x = self.feed_forward(x)\n",
        "    x = self.drop_skip(x)\n",
        "    x = shortcut + x # skip connection\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "MsRjs-REAsmb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an instance\n",
        "torch.manual_seed(211)\n",
        "x = torch.rand(3, 5, 768) # shape: [batch_size, num_tokens, emb_size]\n",
        "transformer_block = TransformerBlock(GPT_CONFIG_124M)\n",
        "output = transformer_block(x)\n",
        "\n",
        "print(\"input shape: \", x.shape)\n",
        "print(\"output shape: \", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuhrbmnqEEJ3",
        "outputId": "ada95525-73eb-4c3a-eed0-a85c0494ff34"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input shape:  torch.Size([3, 5, 768])\n",
            "output shape:  torch.Size([3, 5, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement The GPT2 Model"
      ],
      "metadata": {
        "id": "j7AdBXBoLrL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Model(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.token_emb = nn.Embedding(config[\"vocab_size\"],\n",
        "                                  config[\"emb_dim\"])\n",
        "    self.position_emb = nn.Embedding(config[\"context_length\"],\n",
        "                                     config[\"emb_dim\"])\n",
        "    self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "    self.transformer_blocks = nn.Sequential(\n",
        "        *[TransformerBlock(config) for _ in range(config[\"n_layers\"])])\n",
        "\n",
        "    self.final_norm = LayerNorm(config[\"emb_dim\"])\n",
        "\n",
        "    self.out_head = nn.Linear(config[\"emb_dim\"],\n",
        "                              config[\"vocab_size\"],\n",
        "                              bias=False)\n",
        "\n",
        "  def forward(self, input_token):\n",
        "    batch_size, sequence_length = input_token.shape\n",
        "    token_embeds = self.token_emb(input_token)\n",
        "    position_embeds = self.position_emb(\n",
        "        torch.arange(sequence_length,\n",
        "                     device=input_token.device))\n",
        "    embeds = token_embeds + position_embeds\n",
        "    x = self.drop_emb(embeds)\n",
        "    x = self.transformer_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "iLo0pRoWG3NM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an instance\n",
        "torch.manual_seed(211)\n",
        "model = GPT2Model(GPT_CONFIG_124M)\n",
        "\n",
        "logits = model(batch)\n",
        "print(\"input batch: \\n\", batch)\n",
        "print(\"output shape: \", logits.shape)\n",
        "logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IfWhCzxMjLf",
        "outputId": "e12d2df1-f12d-454c-ed64-d117046ddfe1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input batch: \n",
            " tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n",
            "output shape:  torch.Size([2, 4, 50257])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1543,  0.2344, -0.8564,  ..., -0.2881,  0.2903,  0.1609],\n",
              "         [-0.3195,  0.1662, -1.0898,  ..., -0.3479,  0.1823, -0.1557],\n",
              "         [-0.3561, -1.0340, -1.0551,  ...,  0.0591, -0.5454, -0.2428],\n",
              "         [-0.3049, -0.2747, -0.7511,  ..., -0.3644,  0.7971,  1.3598]],\n",
              "\n",
              "        [[ 0.5644,  0.4271, -0.6928,  ...,  0.1898,  0.2794,  0.3338],\n",
              "         [ 0.1548,  1.1717, -1.0100,  ..., -0.1034,  0.4662, -0.0544],\n",
              "         [-0.1585,  0.4482, -0.4226,  ...,  0.6623,  0.0827, -0.5258],\n",
              "         [-0.0396, -0.0070, -1.1453,  ...,  0.2888,  0.4412, -0.0258]]],\n",
              "       grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore the model:"
      ],
      "metadata": {
        "id": "tSSvAdQEOCHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}: {param.shape}\")\n",
        "    #print(f\"{name}:\\n{param}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbmO5jwUNvkL",
        "outputId": "5f51042f-14b7-4605-d9fd-f2903a351dba"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token_emb.weight: torch.Size([50257, 768])\n",
            "position_emb.weight: torch.Size([1024, 768])\n",
            "transformer_blocks.0.attention.W_query.weight: torch.Size([768, 768])\n",
            "transformer_blocks.0.attention.W_key.weight: torch.Size([768, 768])\n",
            "transformer_blocks.0.attention.W_value.weight: torch.Size([768, 768])\n",
            "transformer_blocks.0.attention.output_projection.weight: torch.Size([768, 768])\n",
            "transformer_blocks.0.attention.output_projection.bias: torch.Size([768])\n",
            "transformer_blocks.0.feed_forward.layers.0.weight: torch.Size([3072, 768])\n",
            "transformer_blocks.0.feed_forward.layers.0.bias: torch.Size([3072])\n",
            "transformer_blocks.0.feed_forward.layers.2.weight: torch.Size([768, 3072])\n",
            "transformer_blocks.0.feed_forward.layers.2.bias: torch.Size([768])\n",
            "transformer_blocks.0.layer_norm1.scale: torch.Size([768])\n",
            "transformer_blocks.0.layer_norm1.shift: torch.Size([768])\n",
            "transformer_blocks.0.layer_norm2.scale: torch.Size([768])\n",
            "transformer_blocks.0.layer_norm2.shift: torch.Size([768])\n",
            "transformer_blocks.1.attention.W_query.weight: torch.Size([768, 768])\n",
            "transformer_blocks.1.attention.W_key.weight: torch.Size([768, 768])\n",
            "transformer_blocks.1.attention.W_value.weight: torch.Size([768, 768])\n",
            "transformer_blocks.1.attention.output_projection.weight: torch.Size([768, 768])\n",
            "transformer_blocks.1.attention.output_projection.bias: torch.Size([768])\n",
            "transformer_blocks.1.feed_forward.layers.0.weight: torch.Size([3072, 768])\n",
            "transformer_blocks.1.feed_forward.layers.0.bias: torch.Size([3072])\n",
            "transformer_blocks.1.feed_forward.layers.2.weight: torch.Size([768, 3072])\n",
            "transformer_blocks.1.feed_forward.layers.2.bias: torch.Size([768])\n",
            "transformer_blocks.1.layer_norm1.scale: torch.Size([768])\n",
            "transformer_blocks.1.layer_norm1.shift: torch.Size([768])\n",
            "transformer_blocks.1.layer_norm2.scale: torch.Size([768])\n",
            "transformer_blocks.1.layer_norm2.shift: torch.Size([768])\n",
            "transformer_blocks.2.attention.W_query.weight: torch.Size([768, 768])\n",
            "transformer_blocks.2.attention.W_key.weight: torch.Size([768, 768])\n",
            "transformer_blocks.2.attention.W_value.weight: torch.Size([768, 768])\n",
            "transformer_blocks.2.attention.output_projection.weight: torch.Size([768, 768])\n",
            "transformer_blocks.2.attention.output_projection.bias: torch.Size([768])\n",
            "transformer_blocks.2.feed_forward.layers.0.weight: torch.Size([3072, 768])\n",
            "transformer_blocks.2.feed_forward.layers.0.bias: torch.Size([3072])\n",
            "transformer_blocks.2.feed_forward.layers.2.weight: torch.Size([768, 3072])\n",
            "transformer_blocks.2.feed_forward.layers.2.bias: torch.Size([768])\n",
            "transformer_blocks.2.layer_norm1.scale: torch.Size([768])\n",
            "transformer_blocks.2.layer_norm1.shift: torch.Size([768])\n",
            "transformer_blocks.2.layer_norm2.scale: torch.Size([768])\n",
            "transformer_blocks.2.layer_norm2.shift: torch.Size([768])\n",
            "transformer_blocks.3.attention.W_query.weight: torch.Size([768, 768])\n",
            "transformer_blocks.3.attention.W_key.weight: torch.Size([768, 768])\n",
            "transformer_blocks.3.attention.W_value.weight: torch.Size([768, 768])\n",
            "transformer_blocks.3.attention.output_projection.weight: torch.Size([768, 768])\n",
            "transformer_blocks.3.attention.output_projection.bias: torch.Size([768])\n",
            "transformer_blocks.3.feed_forward.layers.0.weight: torch.Size([3072, 768])\n",
            "transformer_blocks.3.feed_forward.layers.0.bias: torch.Size([3072])\n",
            "transformer_blocks.3.feed_forward.layers.2.weight: torch.Size([768, 3072])\n",
            "transformer_blocks.3.feed_forward.layers.2.bias: torch.Size([768])\n",
            "transformer_blocks.3.layer_norm1.scale: torch.Size([768])\n",
            "transformer_blocks.3.layer_norm1.shift: torch.Size([768])\n",
            "transformer_blocks.3.layer_norm2.scale: torch.Size([768])\n",
            "transformer_blocks.3.layer_norm2.shift: torch.Size([768])\n",
            "transformer_blocks.4.attention.W_query.weight: torch.Size([768, 768])\n",
            "transformer_blocks.4.attention.W_key.weight: torch.Size([768, 768])\n",
            "transformer_blocks.4.attention.W_value.weight: torch.Size([768, 768])\n",
            "transformer_blocks.4.attention.output_projection.weight: torch.Size([768, 768])\n",
            "transformer_blocks.4.attention.output_projection.bias: torch.Size([768])\n",
            "transformer_blocks.4.feed_forward.layers.0.weight: torch.Size([3072, 768])\n",
            "transformer_blocks.4.feed_forward.layers.0.bias: torch.Size([3072])\n",
            "transformer_blocks.4.feed_forward.layers.2.weight: torch.Size([768, 3072])\n",
            "transformer_blocks.4.feed_forward.layers.2.bias: torch.Size([768])\n",
            "transformer_blocks.4.layer_norm1.scale: torch.Size([768])\n",
            "transformer_blocks.4.layer_norm1.shift: torch.Size([768])\n",
            "transformer_blocks.4.layer_norm2.scale: torch.Size([768])\n",
            "transformer_blocks.4.layer_norm2.shift: torch.Size([768])\n",
            "transformer_blocks.5.attention.W_query.weight: torch.Size([768, 768])\n",
            "transformer_blocks.5.attention.W_key.weight: torch.Size([768, 768])\n",
            "transformer_blocks.5.attention.W_value.weight: torch.Size([768, 768])\n",
            "transformer_blocks.5.attention.output_projection.weight: torch.Size([768, 768])\n",
            "transformer_blocks.5.attention.output_projection.bias: torch.Size([768])\n",
            "transformer_blocks.5.feed_forward.layers.0.weight: torch.Size([3072, 768])\n",
            "transformer_blocks.5.feed_forward.layers.0.bias: torch.Size([3072])\n",
            "transformer_blocks.5.feed_forward.layers.2.weight: torch.Size([768, 3072])\n",
            "transformer_blocks.5.feed_forward.layers.2.bias: torch.Size([768])\n",
            "transformer_blocks.5.layer_norm1.scale: torch.Size([768])\n",
            "transformer_blocks.5.layer_norm1.shift: torch.Size([768])\n",
            "transformer_blocks.5.layer_norm2.scale: torch.Size([768])\n",
            "transformer_blocks.5.layer_norm2.shift: torch.Size([768])\n",
            "transformer_blocks.6.attention.W_query.weight: torch.Size([768, 768])\n",
            "transformer_blocks.6.attention.W_key.weight: torch.Size([768, 768])\n",
            "transformer_blocks.6.attention.W_value.weight: torch.Size([768, 768])\n",
            "transformer_blocks.6.attention.output_projection.weight: torch.Size([768, 768])\n",
            "transformer_blocks.6.attention.output_projection.bias: torch.Size([768])\n",
            "transformer_blocks.6.feed_forward.layers.0.weight: torch.Size([3072, 768])\n",
            "transformer_blocks.6.feed_forward.layers.0.bias: torch.Size([3072])\n",
            "transformer_blocks.6.feed_forward.layers.2.weight: torch.Size([768, 3072])\n",
            "transformer_blocks.6.feed_forward.layers.2.bias: torch.Size([768])\n",
            "transformer_blocks.6.layer_norm1.scale: torch.Size([768])\n",
            "transformer_blocks.6.layer_norm1.shift: torch.Size([768])\n",
            "transformer_blocks.6.layer_norm2.scale: torch.Size([768])\n",
            "transformer_blocks.6.layer_norm2.shift: torch.Size([768])\n",
            "transformer_blocks.7.attention.W_query.weight: torch.Size([768, 768])\n",
            "transformer_blocks.7.attention.W_key.weight: torch.Size([768, 768])\n",
            "transformer_blocks.7.attention.W_value.weight: torch.Size([768, 768])\n",
            "transformer_blocks.7.attention.output_projection.weight: torch.Size([768, 768])\n",
            "transformer_blocks.7.attention.output_projection.bias: torch.Size([768])\n",
            "transformer_blocks.7.feed_forward.layers.0.weight: torch.Size([3072, 768])\n",
            "transformer_blocks.7.feed_forward.layers.0.bias: torch.Size([3072])\n",
            "transformer_blocks.7.feed_forward.layers.2.weight: torch.Size([768, 3072])\n",
            "transformer_blocks.7.feed_forward.layers.2.bias: torch.Size([768])\n",
            "transformer_blocks.7.layer_norm1.scale: torch.Size([768])\n",
            "transformer_blocks.7.layer_norm1.shift: torch.Size([768])\n",
            "transformer_blocks.7.layer_norm2.scale: torch.Size([768])\n",
            "transformer_blocks.7.layer_norm2.shift: torch.Size([768])\n",
            "transformer_blocks.8.attention.W_query.weight: torch.Size([768, 768])\n",
            "transformer_blocks.8.attention.W_key.weight: torch.Size([768, 768])\n",
            "transformer_blocks.8.attention.W_value.weight: torch.Size([768, 768])\n",
            "transformer_blocks.8.attention.output_projection.weight: torch.Size([768, 768])\n",
            "transformer_blocks.8.attention.output_projection.bias: torch.Size([768])\n",
            "transformer_blocks.8.feed_forward.layers.0.weight: torch.Size([3072, 768])\n",
            "transformer_blocks.8.feed_forward.layers.0.bias: torch.Size([3072])\n",
            "transformer_blocks.8.feed_forward.layers.2.weight: torch.Size([768, 3072])\n",
            "transformer_blocks.8.feed_forward.layers.2.bias: torch.Size([768])\n",
            "transformer_blocks.8.layer_norm1.scale: torch.Size([768])\n",
            "transformer_blocks.8.layer_norm1.shift: torch.Size([768])\n",
            "transformer_blocks.8.layer_norm2.scale: torch.Size([768])\n",
            "transformer_blocks.8.layer_norm2.shift: torch.Size([768])\n",
            "transformer_blocks.9.attention.W_query.weight: torch.Size([768, 768])\n",
            "transformer_blocks.9.attention.W_key.weight: torch.Size([768, 768])\n",
            "transformer_blocks.9.attention.W_value.weight: torch.Size([768, 768])\n",
            "transformer_blocks.9.attention.output_projection.weight: torch.Size([768, 768])\n",
            "transformer_blocks.9.attention.output_projection.bias: torch.Size([768])\n",
            "transformer_blocks.9.feed_forward.layers.0.weight: torch.Size([3072, 768])\n",
            "transformer_blocks.9.feed_forward.layers.0.bias: torch.Size([3072])\n",
            "transformer_blocks.9.feed_forward.layers.2.weight: torch.Size([768, 3072])\n",
            "transformer_blocks.9.feed_forward.layers.2.bias: torch.Size([768])\n",
            "transformer_blocks.9.layer_norm1.scale: torch.Size([768])\n",
            "transformer_blocks.9.layer_norm1.shift: torch.Size([768])\n",
            "transformer_blocks.9.layer_norm2.scale: torch.Size([768])\n",
            "transformer_blocks.9.layer_norm2.shift: torch.Size([768])\n",
            "transformer_blocks.10.attention.W_query.weight: torch.Size([768, 768])\n",
            "transformer_blocks.10.attention.W_key.weight: torch.Size([768, 768])\n",
            "transformer_blocks.10.attention.W_value.weight: torch.Size([768, 768])\n",
            "transformer_blocks.10.attention.output_projection.weight: torch.Size([768, 768])\n",
            "transformer_blocks.10.attention.output_projection.bias: torch.Size([768])\n",
            "transformer_blocks.10.feed_forward.layers.0.weight: torch.Size([3072, 768])\n",
            "transformer_blocks.10.feed_forward.layers.0.bias: torch.Size([3072])\n",
            "transformer_blocks.10.feed_forward.layers.2.weight: torch.Size([768, 3072])\n",
            "transformer_blocks.10.feed_forward.layers.2.bias: torch.Size([768])\n",
            "transformer_blocks.10.layer_norm1.scale: torch.Size([768])\n",
            "transformer_blocks.10.layer_norm1.shift: torch.Size([768])\n",
            "transformer_blocks.10.layer_norm2.scale: torch.Size([768])\n",
            "transformer_blocks.10.layer_norm2.shift: torch.Size([768])\n",
            "transformer_blocks.11.attention.W_query.weight: torch.Size([768, 768])\n",
            "transformer_blocks.11.attention.W_key.weight: torch.Size([768, 768])\n",
            "transformer_blocks.11.attention.W_value.weight: torch.Size([768, 768])\n",
            "transformer_blocks.11.attention.output_projection.weight: torch.Size([768, 768])\n",
            "transformer_blocks.11.attention.output_projection.bias: torch.Size([768])\n",
            "transformer_blocks.11.feed_forward.layers.0.weight: torch.Size([3072, 768])\n",
            "transformer_blocks.11.feed_forward.layers.0.bias: torch.Size([3072])\n",
            "transformer_blocks.11.feed_forward.layers.2.weight: torch.Size([768, 3072])\n",
            "transformer_blocks.11.feed_forward.layers.2.bias: torch.Size([768])\n",
            "transformer_blocks.11.layer_norm1.scale: torch.Size([768])\n",
            "transformer_blocks.11.layer_norm1.shift: torch.Size([768])\n",
            "transformer_blocks.11.layer_norm2.scale: torch.Size([768])\n",
            "transformer_blocks.11.layer_norm2.shift: torch.Size([768])\n",
            "final_norm.scale: torch.Size([768])\n",
            "final_norm.shift: torch.Size([768])\n",
            "out_head.weight: torch.Size([50257, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute total number of params\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters()\n",
        "                        if p.requires_grad)\n",
        "print(f\"Total number of parameters: {total_params}\")\n",
        "print(f\"Total number of trainable parameters: {trainable_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhm4u5hTNVzI",
        "outputId": "a3ac346a-3dc3-42b8-bf15-d2f2b06ef79b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 163009536\n",
            "Total number of trainable parameters: 163009536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# can retrieve weights and layer names with state_dict()\n",
        "for name, weights in model.state_dict().items():\n",
        "    print(f\"{name}  {weights.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-leGItDQNtq4",
        "outputId": "8963cbc1-0f57-49b2-8d24-953c77ddd7b8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token_emb.weight  torch.Size([50257, 768])\n",
            "position_emb.weight  torch.Size([1024, 768])\n",
            "transformer_blocks.0.attention.mask  torch.Size([1024, 1024])\n",
            "transformer_blocks.0.attention.W_query.weight  torch.Size([768, 768])\n",
            "transformer_blocks.0.attention.W_key.weight  torch.Size([768, 768])\n",
            "transformer_blocks.0.attention.W_value.weight  torch.Size([768, 768])\n",
            "transformer_blocks.0.attention.output_projection.weight  torch.Size([768, 768])\n",
            "transformer_blocks.0.attention.output_projection.bias  torch.Size([768])\n",
            "transformer_blocks.0.feed_forward.layers.0.weight  torch.Size([3072, 768])\n",
            "transformer_blocks.0.feed_forward.layers.0.bias  torch.Size([3072])\n",
            "transformer_blocks.0.feed_forward.layers.2.weight  torch.Size([768, 3072])\n",
            "transformer_blocks.0.feed_forward.layers.2.bias  torch.Size([768])\n",
            "transformer_blocks.0.layer_norm1.scale  torch.Size([768])\n",
            "transformer_blocks.0.layer_norm1.shift  torch.Size([768])\n",
            "transformer_blocks.0.layer_norm2.scale  torch.Size([768])\n",
            "transformer_blocks.0.layer_norm2.shift  torch.Size([768])\n",
            "transformer_blocks.1.attention.mask  torch.Size([1024, 1024])\n",
            "transformer_blocks.1.attention.W_query.weight  torch.Size([768, 768])\n",
            "transformer_blocks.1.attention.W_key.weight  torch.Size([768, 768])\n",
            "transformer_blocks.1.attention.W_value.weight  torch.Size([768, 768])\n",
            "transformer_blocks.1.attention.output_projection.weight  torch.Size([768, 768])\n",
            "transformer_blocks.1.attention.output_projection.bias  torch.Size([768])\n",
            "transformer_blocks.1.feed_forward.layers.0.weight  torch.Size([3072, 768])\n",
            "transformer_blocks.1.feed_forward.layers.0.bias  torch.Size([3072])\n",
            "transformer_blocks.1.feed_forward.layers.2.weight  torch.Size([768, 3072])\n",
            "transformer_blocks.1.feed_forward.layers.2.bias  torch.Size([768])\n",
            "transformer_blocks.1.layer_norm1.scale  torch.Size([768])\n",
            "transformer_blocks.1.layer_norm1.shift  torch.Size([768])\n",
            "transformer_blocks.1.layer_norm2.scale  torch.Size([768])\n",
            "transformer_blocks.1.layer_norm2.shift  torch.Size([768])\n",
            "transformer_blocks.2.attention.mask  torch.Size([1024, 1024])\n",
            "transformer_blocks.2.attention.W_query.weight  torch.Size([768, 768])\n",
            "transformer_blocks.2.attention.W_key.weight  torch.Size([768, 768])\n",
            "transformer_blocks.2.attention.W_value.weight  torch.Size([768, 768])\n",
            "transformer_blocks.2.attention.output_projection.weight  torch.Size([768, 768])\n",
            "transformer_blocks.2.attention.output_projection.bias  torch.Size([768])\n",
            "transformer_blocks.2.feed_forward.layers.0.weight  torch.Size([3072, 768])\n",
            "transformer_blocks.2.feed_forward.layers.0.bias  torch.Size([3072])\n",
            "transformer_blocks.2.feed_forward.layers.2.weight  torch.Size([768, 3072])\n",
            "transformer_blocks.2.feed_forward.layers.2.bias  torch.Size([768])\n",
            "transformer_blocks.2.layer_norm1.scale  torch.Size([768])\n",
            "transformer_blocks.2.layer_norm1.shift  torch.Size([768])\n",
            "transformer_blocks.2.layer_norm2.scale  torch.Size([768])\n",
            "transformer_blocks.2.layer_norm2.shift  torch.Size([768])\n",
            "transformer_blocks.3.attention.mask  torch.Size([1024, 1024])\n",
            "transformer_blocks.3.attention.W_query.weight  torch.Size([768, 768])\n",
            "transformer_blocks.3.attention.W_key.weight  torch.Size([768, 768])\n",
            "transformer_blocks.3.attention.W_value.weight  torch.Size([768, 768])\n",
            "transformer_blocks.3.attention.output_projection.weight  torch.Size([768, 768])\n",
            "transformer_blocks.3.attention.output_projection.bias  torch.Size([768])\n",
            "transformer_blocks.3.feed_forward.layers.0.weight  torch.Size([3072, 768])\n",
            "transformer_blocks.3.feed_forward.layers.0.bias  torch.Size([3072])\n",
            "transformer_blocks.3.feed_forward.layers.2.weight  torch.Size([768, 3072])\n",
            "transformer_blocks.3.feed_forward.layers.2.bias  torch.Size([768])\n",
            "transformer_blocks.3.layer_norm1.scale  torch.Size([768])\n",
            "transformer_blocks.3.layer_norm1.shift  torch.Size([768])\n",
            "transformer_blocks.3.layer_norm2.scale  torch.Size([768])\n",
            "transformer_blocks.3.layer_norm2.shift  torch.Size([768])\n",
            "transformer_blocks.4.attention.mask  torch.Size([1024, 1024])\n",
            "transformer_blocks.4.attention.W_query.weight  torch.Size([768, 768])\n",
            "transformer_blocks.4.attention.W_key.weight  torch.Size([768, 768])\n",
            "transformer_blocks.4.attention.W_value.weight  torch.Size([768, 768])\n",
            "transformer_blocks.4.attention.output_projection.weight  torch.Size([768, 768])\n",
            "transformer_blocks.4.attention.output_projection.bias  torch.Size([768])\n",
            "transformer_blocks.4.feed_forward.layers.0.weight  torch.Size([3072, 768])\n",
            "transformer_blocks.4.feed_forward.layers.0.bias  torch.Size([3072])\n",
            "transformer_blocks.4.feed_forward.layers.2.weight  torch.Size([768, 3072])\n",
            "transformer_blocks.4.feed_forward.layers.2.bias  torch.Size([768])\n",
            "transformer_blocks.4.layer_norm1.scale  torch.Size([768])\n",
            "transformer_blocks.4.layer_norm1.shift  torch.Size([768])\n",
            "transformer_blocks.4.layer_norm2.scale  torch.Size([768])\n",
            "transformer_blocks.4.layer_norm2.shift  torch.Size([768])\n",
            "transformer_blocks.5.attention.mask  torch.Size([1024, 1024])\n",
            "transformer_blocks.5.attention.W_query.weight  torch.Size([768, 768])\n",
            "transformer_blocks.5.attention.W_key.weight  torch.Size([768, 768])\n",
            "transformer_blocks.5.attention.W_value.weight  torch.Size([768, 768])\n",
            "transformer_blocks.5.attention.output_projection.weight  torch.Size([768, 768])\n",
            "transformer_blocks.5.attention.output_projection.bias  torch.Size([768])\n",
            "transformer_blocks.5.feed_forward.layers.0.weight  torch.Size([3072, 768])\n",
            "transformer_blocks.5.feed_forward.layers.0.bias  torch.Size([3072])\n",
            "transformer_blocks.5.feed_forward.layers.2.weight  torch.Size([768, 3072])\n",
            "transformer_blocks.5.feed_forward.layers.2.bias  torch.Size([768])\n",
            "transformer_blocks.5.layer_norm1.scale  torch.Size([768])\n",
            "transformer_blocks.5.layer_norm1.shift  torch.Size([768])\n",
            "transformer_blocks.5.layer_norm2.scale  torch.Size([768])\n",
            "transformer_blocks.5.layer_norm2.shift  torch.Size([768])\n",
            "transformer_blocks.6.attention.mask  torch.Size([1024, 1024])\n",
            "transformer_blocks.6.attention.W_query.weight  torch.Size([768, 768])\n",
            "transformer_blocks.6.attention.W_key.weight  torch.Size([768, 768])\n",
            "transformer_blocks.6.attention.W_value.weight  torch.Size([768, 768])\n",
            "transformer_blocks.6.attention.output_projection.weight  torch.Size([768, 768])\n",
            "transformer_blocks.6.attention.output_projection.bias  torch.Size([768])\n",
            "transformer_blocks.6.feed_forward.layers.0.weight  torch.Size([3072, 768])\n",
            "transformer_blocks.6.feed_forward.layers.0.bias  torch.Size([3072])\n",
            "transformer_blocks.6.feed_forward.layers.2.weight  torch.Size([768, 3072])\n",
            "transformer_blocks.6.feed_forward.layers.2.bias  torch.Size([768])\n",
            "transformer_blocks.6.layer_norm1.scale  torch.Size([768])\n",
            "transformer_blocks.6.layer_norm1.shift  torch.Size([768])\n",
            "transformer_blocks.6.layer_norm2.scale  torch.Size([768])\n",
            "transformer_blocks.6.layer_norm2.shift  torch.Size([768])\n",
            "transformer_blocks.7.attention.mask  torch.Size([1024, 1024])\n",
            "transformer_blocks.7.attention.W_query.weight  torch.Size([768, 768])\n",
            "transformer_blocks.7.attention.W_key.weight  torch.Size([768, 768])\n",
            "transformer_blocks.7.attention.W_value.weight  torch.Size([768, 768])\n",
            "transformer_blocks.7.attention.output_projection.weight  torch.Size([768, 768])\n",
            "transformer_blocks.7.attention.output_projection.bias  torch.Size([768])\n",
            "transformer_blocks.7.feed_forward.layers.0.weight  torch.Size([3072, 768])\n",
            "transformer_blocks.7.feed_forward.layers.0.bias  torch.Size([3072])\n",
            "transformer_blocks.7.feed_forward.layers.2.weight  torch.Size([768, 3072])\n",
            "transformer_blocks.7.feed_forward.layers.2.bias  torch.Size([768])\n",
            "transformer_blocks.7.layer_norm1.scale  torch.Size([768])\n",
            "transformer_blocks.7.layer_norm1.shift  torch.Size([768])\n",
            "transformer_blocks.7.layer_norm2.scale  torch.Size([768])\n",
            "transformer_blocks.7.layer_norm2.shift  torch.Size([768])\n",
            "transformer_blocks.8.attention.mask  torch.Size([1024, 1024])\n",
            "transformer_blocks.8.attention.W_query.weight  torch.Size([768, 768])\n",
            "transformer_blocks.8.attention.W_key.weight  torch.Size([768, 768])\n",
            "transformer_blocks.8.attention.W_value.weight  torch.Size([768, 768])\n",
            "transformer_blocks.8.attention.output_projection.weight  torch.Size([768, 768])\n",
            "transformer_blocks.8.attention.output_projection.bias  torch.Size([768])\n",
            "transformer_blocks.8.feed_forward.layers.0.weight  torch.Size([3072, 768])\n",
            "transformer_blocks.8.feed_forward.layers.0.bias  torch.Size([3072])\n",
            "transformer_blocks.8.feed_forward.layers.2.weight  torch.Size([768, 3072])\n",
            "transformer_blocks.8.feed_forward.layers.2.bias  torch.Size([768])\n",
            "transformer_blocks.8.layer_norm1.scale  torch.Size([768])\n",
            "transformer_blocks.8.layer_norm1.shift  torch.Size([768])\n",
            "transformer_blocks.8.layer_norm2.scale  torch.Size([768])\n",
            "transformer_blocks.8.layer_norm2.shift  torch.Size([768])\n",
            "transformer_blocks.9.attention.mask  torch.Size([1024, 1024])\n",
            "transformer_blocks.9.attention.W_query.weight  torch.Size([768, 768])\n",
            "transformer_blocks.9.attention.W_key.weight  torch.Size([768, 768])\n",
            "transformer_blocks.9.attention.W_value.weight  torch.Size([768, 768])\n",
            "transformer_blocks.9.attention.output_projection.weight  torch.Size([768, 768])\n",
            "transformer_blocks.9.attention.output_projection.bias  torch.Size([768])\n",
            "transformer_blocks.9.feed_forward.layers.0.weight  torch.Size([3072, 768])\n",
            "transformer_blocks.9.feed_forward.layers.0.bias  torch.Size([3072])\n",
            "transformer_blocks.9.feed_forward.layers.2.weight  torch.Size([768, 3072])\n",
            "transformer_blocks.9.feed_forward.layers.2.bias  torch.Size([768])\n",
            "transformer_blocks.9.layer_norm1.scale  torch.Size([768])\n",
            "transformer_blocks.9.layer_norm1.shift  torch.Size([768])\n",
            "transformer_blocks.9.layer_norm2.scale  torch.Size([768])\n",
            "transformer_blocks.9.layer_norm2.shift  torch.Size([768])\n",
            "transformer_blocks.10.attention.mask  torch.Size([1024, 1024])\n",
            "transformer_blocks.10.attention.W_query.weight  torch.Size([768, 768])\n",
            "transformer_blocks.10.attention.W_key.weight  torch.Size([768, 768])\n",
            "transformer_blocks.10.attention.W_value.weight  torch.Size([768, 768])\n",
            "transformer_blocks.10.attention.output_projection.weight  torch.Size([768, 768])\n",
            "transformer_blocks.10.attention.output_projection.bias  torch.Size([768])\n",
            "transformer_blocks.10.feed_forward.layers.0.weight  torch.Size([3072, 768])\n",
            "transformer_blocks.10.feed_forward.layers.0.bias  torch.Size([3072])\n",
            "transformer_blocks.10.feed_forward.layers.2.weight  torch.Size([768, 3072])\n",
            "transformer_blocks.10.feed_forward.layers.2.bias  torch.Size([768])\n",
            "transformer_blocks.10.layer_norm1.scale  torch.Size([768])\n",
            "transformer_blocks.10.layer_norm1.shift  torch.Size([768])\n",
            "transformer_blocks.10.layer_norm2.scale  torch.Size([768])\n",
            "transformer_blocks.10.layer_norm2.shift  torch.Size([768])\n",
            "transformer_blocks.11.attention.mask  torch.Size([1024, 1024])\n",
            "transformer_blocks.11.attention.W_query.weight  torch.Size([768, 768])\n",
            "transformer_blocks.11.attention.W_key.weight  torch.Size([768, 768])\n",
            "transformer_blocks.11.attention.W_value.weight  torch.Size([768, 768])\n",
            "transformer_blocks.11.attention.output_projection.weight  torch.Size([768, 768])\n",
            "transformer_blocks.11.attention.output_projection.bias  torch.Size([768])\n",
            "transformer_blocks.11.feed_forward.layers.0.weight  torch.Size([3072, 768])\n",
            "transformer_blocks.11.feed_forward.layers.0.bias  torch.Size([3072])\n",
            "transformer_blocks.11.feed_forward.layers.2.weight  torch.Size([768, 3072])\n",
            "transformer_blocks.11.feed_forward.layers.2.bias  torch.Size([768])\n",
            "transformer_blocks.11.layer_norm1.scale  torch.Size([768])\n",
            "transformer_blocks.11.layer_norm1.shift  torch.Size([768])\n",
            "transformer_blocks.11.layer_norm2.scale  torch.Size([768])\n",
            "transformer_blocks.11.layer_norm2.shift  torch.Size([768])\n",
            "final_norm.scale  torch.Size([768])\n",
            "final_norm.shift  torch.Size([768])\n",
            "out_head.weight  torch.Size([50257, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "later we will use `weight tying`:"
      ],
      "metadata": {
        "id": "nYmRzS5SPSwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# weight tying\n",
        "print(\"token embedding layer shape: \", model.token_emb.weight.shape)\n",
        "print(\"output embedding layer shape: \", model.out_head.weight.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIToC8RoOpiM",
        "outputId": "2bd6a7d3-ba9f-429f-dd30-36f6613a99e9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token embedding layer shape:  torch.Size([50257, 768])\n",
            "output embedding layer shape:  torch.Size([50257, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params_gpt2 = (\n",
        "    total_params - sum(p.numel()\n",
        "    for p in model.out_head.parameters())\n",
        ")\n",
        "\n",
        "print(f\"Total number of parameters in GPT-2: {total_params_gpt2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWt2rt2NPPk9",
        "outputId": "5e30fd94-6580-44ed-9ea5-4e9a1391af43"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters in GPT-2: 124412160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compute the memory requirements:"
      ],
      "metadata": {
        "id": "mAPuiic4Pxb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_size_bytes = total_params * 4\n",
        "total_size_mb = total_size_bytes / (1024 * 1024)\n",
        "print(f\"Total size of GPT-2 in MB: {total_size_mb:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHyCpqVePnHg",
        "outputId": "c0e39fe5-96a7-49ca-f904-84d3f49c4704"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size of GPT-2 in MB: 621.83 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Text"
      ],
      "metadata": {
        "id": "vGnCW2AwQWZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model,\n",
        "                         input_batch, # [batch, num_tokens]\n",
        "                         max_new_tokens, # numbers of new tokens to be predicted\n",
        "                         context_size):\n",
        "  for _ in range(max_new_tokens):\n",
        "    # crop current context if it exceeds the supported context_size\n",
        "    crop_input_batch = input_batch[:, -context_size:]\n",
        "\n",
        "    # predict next token\n",
        "    with torch.no_grad():\n",
        "      logits = model(crop_input_batch)\n",
        "\n",
        "    # consider only logits of the last token\n",
        "    logits = logits[:, -1, :] # (batch, n_tokens, vocab_size) -> (batch, vocab_size)\n",
        "    probas = torch.softmax(logits, dim=-1) # (batch, vocab_size)\n",
        "    predicted_tokens = torch.argmax(probas, dim=-1, keepdim=True) # (batch, 1)\n",
        "    # update input_batch (append predicted tokens to the sequences)\n",
        "    input_batch = torch.cat([input_batch, predicted_tokens], dim=-1) # [batch, num_tokens+1]\n",
        "\n",
        "  return input_batch"
      ],
      "metadata": {
        "id": "rMNfdh6XP9aX"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example setup\n",
        "start_context = \"Hi, my name is\"\n",
        "encoded = bpe_tokenizer.encode(start_context)\n",
        "print(\"encoded: \", encoded)\n",
        "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "print(\"encoded tensor: \\n\", encoded_tensor)\n",
        "print(\"encoded tensor shape: \", encoded_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e72rAU1ATmGE",
        "outputId": "aceed3c4-76f5-4846-eb39-a4233630a477"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded:  [17250, 11, 616, 1438, 318]\n",
            "encoded tensor: \n",
            " tensor([[17250,    11,   616,  1438,   318]])\n",
            "encoded tensor shape:  torch.Size([1, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since, we now don't have to train, let's put the model in `.eval()` mode:"
      ],
      "metadata": {
        "id": "N6UXNhDQUYJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "output = generate_text_simple(\n",
        "    model=model,\n",
        "    input_batch=encoded_tensor,\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"output length: \", output.shape[1])\n",
        "print(\"output: \\n\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMT1IrZ8T9ZO",
        "outputId": "7b1d7e27-28f7-4360-d075-a4b723a53231"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output length:  15\n",
            "output: \n",
            " tensor([[17250,    11,   616,  1438,   318,  5465, 26393,  9713, 28268, 31268,\n",
            "         30338,  7535, 37875, 35793,  2332]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_text = bpe_tokenizer.decode(output.squeeze().tolist())\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jecz4g7SU40o",
        "outputId": "c0e9f674-95be-4438-9507-c162c8e5c9f7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi, my name is hate orc studiedracuse Corinth gloomython Kard)=( Her\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_end_time = time.time()\n",
        "runtime_in_seconds = notebook_end_time - notebook_start_time\n",
        "\n",
        "# format as minutes and seconds\n",
        "minutes, seconds = divmod(runtime_in_seconds, 60)\n",
        "print(f\"Notebook runtime: {int(minutes)} min {seconds:.2f} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL2jl7CcVF1v",
        "outputId": "c2b28b08-715a-415f-f351-a82af206040d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Notebook runtime: 0 min 14.34 sec\n"
          ]
        }
      ]
    }
  ]
}