{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I_vzm9wYCIDX"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "notebook_start_time = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Setup"
      ],
      "metadata": {
        "id": "QRbL5tmQClyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "id": "oQdYnzPECi70"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.metadata import version\n",
        "import torch\n",
        "\n",
        "print(\"torch version:\", version(\"torch\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlWYM8bSCi6k",
        "outputId": "a7192bcd-dd86-4287-e88d-878bed576f5a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,   # Vocabulary size\n",
        "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
        "    \"emb_dim\": 768,        # Embedding dimension\n",
        "    \"n_heads\": 12,         # Number of attention heads\n",
        "    \"n_layers\": 12,        # Number of layers\n",
        "    \"drop_rate\": 0.1,      # Dropout rate\n",
        "    \"qkv_bias\": False      # Query-key-value bias\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "kVnktgkuCi3o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/\"\n",
        "    \"LLMs-from-scratch/main/ch05/\"\n",
        "    \"01_main-chapter-code/gpt_download.py\"\n",
        ")\n",
        "filename = url.split(\"/\")[-1]\n",
        "urllib.request.urlretrieve(url, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp5NZbvfEUy5",
        "outputId": "7f584f14-891f-436c-de70-337575834eac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gpt_download.py', <http.client.HTTPMessage at 0x7810aa179510>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_download import download_and_load_gpt2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_embedding_dim,\n",
        "                 output_embedding_dim,\n",
        "                 context_length,\n",
        "                 dropout,\n",
        "                 num_heads,\n",
        "                 qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (output_embedding_dim % num_heads == 0), \\\n",
        "            \"output_embedding_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.output_embedding_dim = output_embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = output_embedding_dim // num_heads\n",
        "        self.W_query = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
        "                                 bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
        "                               bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(input_embedding_dim, output_embedding_dim,\n",
        "                                 bias=qkv_bias)\n",
        "        self.output_projection = nn.Linear(output_embedding_dim,\n",
        "                                           output_embedding_dim)  # to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        batch, num_tokens, input_embedding_dim = inputs.shape\n",
        "\n",
        "        # qkv shapes : (batch, num_tokens, output_embedding_dim)\n",
        "        keys = self.W_key(inputs)\n",
        "        values = self.W_value(inputs)\n",
        "        queries = self.W_query(inputs)\n",
        "\n",
        "        # qkv shapes : (batch, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # qkv shapes : (batch, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "\n",
        "        # compute attention scores for each head\n",
        "        attention_scores = queries @ keys.transpose(3, 2)\n",
        "        attention_scores.masked_fill_(\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], - torch.inf)\n",
        "\n",
        "        # compute attention weights + dropout\n",
        "        masked_attention_weight = torch.softmax(\n",
        "            attention_scores / (keys.shape[-1] ** 0.5),\n",
        "            dim=-1)\n",
        "        masked_attention_dropout_weight = self.dropout(masked_attention_weight)\n",
        "\n",
        "        # compute context vectors\n",
        "        # shape : (batch, num_tokens, num_heads, head_dim)\n",
        "        context_vector = (masked_attention_dropout_weight @ values).transpose(1, 2)\n",
        "\n",
        "        # combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        # shape : (batch, num_tokens, output_embedding_dim)\n",
        "        context_vector = context_vector.contiguous().view(\n",
        "            batch, num_tokens, self.output_embedding_dim)\n",
        "\n",
        "        # linear projection (optional)\n",
        "        context_vector = self.output_projection(context_vector)\n",
        "\n",
        "        return context_vector\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.epsilon = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1,\n",
        "                    unbiased=False,  # Bessel's correction (n-1)\n",
        "                    keepdim=True)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.epsilon)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(config[\"emb_dim\"],  # 768\n",
        "                      4 * config[\"emb_dim\"]),  # 3072\n",
        "            GELU(),  # 3072\n",
        "            nn.Linear(4 * config[\"emb_dim\"],  # 3072\n",
        "                      config[\"emb_dim\"])  # 768\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(input_embedding_dim=config[\"emb_dim\"],\n",
        "                                            output_embedding_dim=config[\"emb_dim\"],\n",
        "                                            context_length=config[\"context_length\"],\n",
        "                                            dropout=config[\"drop_rate\"],\n",
        "                                            num_heads=config[\"n_heads\"],\n",
        "                                            qkv_bias=config[\"qkv_bias\"])\n",
        "        self.feed_forward = FeedForward(config)\n",
        "        self.layer_norm1 = LayerNorm(config[\"emb_dim\"])\n",
        "        self.layer_norm2 = LayerNorm(config[\"emb_dim\"])\n",
        "        self.drop_skip = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # skip connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.layer_norm1(x)\n",
        "        x = self.attention(x)  # shape: [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_skip(x)\n",
        "        x = shortcut + x  # skip connection\n",
        "\n",
        "        # skip connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.layer_norm2(x)\n",
        "        x = self.feed_forward(x)\n",
        "        x = self.drop_skip(x)\n",
        "        x = shortcut + x  # skip connection\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT2Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(config[\"vocab_size\"],\n",
        "                                      config[\"emb_dim\"])\n",
        "        self.position_emb = nn.Embedding(config[\"context_length\"],\n",
        "                                         config[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "        self.transformer_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(config) for _ in range(config[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(config[\"emb_dim\"])\n",
        "\n",
        "        self.out_head = nn.Linear(config[\"emb_dim\"],\n",
        "                                  config[\"vocab_size\"],\n",
        "                                  bias=False)\n",
        "\n",
        "    def forward(self, input_token):\n",
        "        batch_size, sequence_length = input_token.shape\n",
        "        token_embeds = self.token_emb(input_token)\n",
        "        position_embeds = self.position_emb(\n",
        "            torch.arange(sequence_length,\n",
        "                         device=input_token.device))\n",
        "        embeds = token_embeds + position_embeds\n",
        "        x = self.drop_emb(embeds)\n",
        "        x = self.transformer_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))\n",
        "\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.position_emb.weight = assign(gpt.position_emb.weight, params['wpe'])\n",
        "    gpt.token_emb.weight = assign(gpt.token_emb.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.transformer_blocks[b].attention.W_query.weight = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_query.weight, q_w.T)\n",
        "        gpt.transformer_blocks[b].attention.W_key.weight = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_key.weight, k_w.T)\n",
        "        gpt.transformer_blocks[b].attention.W_value.weight = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.transformer_blocks[b].attention.W_query.bias = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_query.bias, q_b)\n",
        "        gpt.transformer_blocks[b].attention.W_key.bias = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_key.bias, k_b)\n",
        "        gpt.transformer_blocks[b].attention.W_value.bias = assign(\n",
        "            gpt.transformer_blocks[b].attention.W_value.bias, v_b)\n",
        "\n",
        "        gpt.transformer_blocks[b].attention.output_projection.weight = assign(\n",
        "            gpt.transformer_blocks[b].attention.output_projection.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.transformer_blocks[b].attention.output_projection.bias = assign(\n",
        "            gpt.transformer_blocks[b].attention.output_projection.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.transformer_blocks[b].feed_forward.layers[0].weight = assign(\n",
        "            gpt.transformer_blocks[b].feed_forward.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.transformer_blocks[b].feed_forward.layers[0].bias = assign(\n",
        "            gpt.transformer_blocks[b].feed_forward.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.transformer_blocks[b].feed_forward.layers[2].weight = assign(\n",
        "            gpt.transformer_blocks[b].feed_forward.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.transformer_blocks[b].feed_forward.layers[2].bias = assign(\n",
        "            gpt.transformer_blocks[b].feed_forward.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.transformer_blocks[b].layer_norm1.scale = assign(\n",
        "            gpt.transformer_blocks[b].layer_norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.transformer_blocks[b].layer_norm1.shift = assign(\n",
        "            gpt.transformer_blocks[b].layer_norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.transformer_blocks[b].layer_norm2.scale = assign(\n",
        "            gpt.transformer_blocks[b].layer_norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.transformer_blocks[b].layer_norm2.shift = assign(\n",
        "            gpt.transformer_blocks[b].layer_norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
      ],
      "metadata": {
        "id": "o5AmrLyCCi03"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(211)\n",
        "model = GPT2Model(GPT_CONFIG_124M)\n",
        "model.eval();  # Disable dropout during inference"
      ],
      "metadata": {
        "id": "tnmTe3frCizz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Initialize DataLoaders"
      ],
      "metadata": {
        "id": "dg3xIyPmEs4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "class GPT2DatasetV1(Dataset):\n",
        "  def __init__(self,\n",
        "               text,\n",
        "               tokenizer,\n",
        "               context_length, # length of each input vector\n",
        "               stride # chunk the text into overlapping sequence of context_length\n",
        "               ):\n",
        "    self.input_id_vectors = []\n",
        "    self.target_id_vectors = []\n",
        "\n",
        "    # tokenize the entire text\n",
        "    token_list = tokenizer.encode(text)\n",
        "\n",
        "    # append input and target vectors\n",
        "    for i in range(0, len(token_list) - context_length, stride):\n",
        "      input_vector = token_list[i:i+context_length]\n",
        "      target_vector = token_list[i+1:i+context_length+1]\n",
        "      self.input_id_vectors.append(torch.tensor(input_vector))\n",
        "      self.target_id_vectors.append(torch.tensor(target_vector))\n",
        "\n",
        "  # get the number of input vectors\n",
        "  def __len__(self):\n",
        "    return len(self.input_id_vectors)\n",
        "\n",
        "  # return the (input vector, target vector) pair\n",
        "  def __getitem__(self, id):\n",
        "    return self.input_id_vectors[id], self.target_id_vectors[id]\n",
        "\n",
        "\n",
        "\n",
        "def create_dataloader_V1(text,\n",
        "                 batch_size=4,\n",
        "                 context_length=256,\n",
        "                 stride=128,\n",
        "                 shuffle=True, # shuffle dataset\n",
        "                 drop_last=True, # drop last batch if it not equal required size\n",
        "                 num_workers=0 # number of CPU processes for preprocessing\n",
        "                 ):\n",
        "\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "  dataset = GPT2DatasetV1(text=text,\n",
        "                          tokenizer=tokenizer,\n",
        "                          context_length=context_length,\n",
        "                          stride=stride)\n",
        "\n",
        "  dataloader = DataLoader(dataset=dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=shuffle,\n",
        "                          drop_last=drop_last,\n",
        "                          num_workers=num_workers)\n",
        "\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "wdiBWB6kCiw2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup\n",
        "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "\n",
        "file_path = \"war_and_peace.txt\" #\"the-verdict.txt\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "  text = f.read()\n",
        "\n",
        "total_characters = len(text)\n",
        "total_tokens = len(bpe_tokenizer.encode(text))\n",
        "print(f\"total characters: {total_characters}\")\n",
        "print(f\"total tokens: {total_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6UbUxaCFk3D",
        "outputId": "9548276f-d367-473d-9c4d-ddc626386ffd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total characters: 3227520\n",
            "total tokens: 853923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/validation ratio\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text))\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_V1(\n",
        "    text[:split_idx],\n",
        "    batch_size=2,\n",
        "    context_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_V1(\n",
        "    text[split_idx:],\n",
        "    batch_size=2,\n",
        "    context_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "Dsov3NVPCiuL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Learning rate warmup"
      ],
      "metadata": {
        "id": "izxGvWUQGgfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 15\n",
        "initial_lr = 0.0001\n",
        "peak_lr = 0.01"
      ],
      "metadata": {
        "id": "YAePyIrWGfMf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_steps = len(train_loader) * n_epochs\n",
        "warmup_steps = int(0.2 * total_steps) # 20% warmup\n",
        "print(warmup_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6f43M43GfKO",
        "outputId": "bb1546ea-82c2-4973-b3e7-5fa870adcc70"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr_increment = (peak_lr - initial_lr) / warmup_steps\n",
        "\n",
        "global_step = -1\n",
        "track_lrs = []\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.1)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for input_batch, target_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        global_step += 1\n",
        "\n",
        "        if global_step < warmup_steps:\n",
        "            lr = initial_lr + global_step * lr_increment\n",
        "        else:\n",
        "            lr = peak_lr\n",
        "\n",
        "        # Apply the calculated learning rate to the optimizer\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group[\"lr\"] = lr\n",
        "        track_lrs.append(optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "        # Calculate loss and update weights\n",
        "        # ..."
      ],
      "metadata": {
        "id": "4_n2XOvPGfHa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.ylabel(\"Learning rate\")\n",
        "plt.xlabel(\"Step\")\n",
        "total_training_steps = len(train_loader) * n_epochs\n",
        "plt.plot(range(total_training_steps), track_lrs)\n",
        "plt.tight_layout(); plt.savefig(\"warmup_lr.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "dClQcuSAGfFe",
        "outputId": "b21439a0-fe09-4874-80b0-3bc14b9e5499"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Cosine Decay"
      ],
      "metadata": {
        "id": "UE4P-2HPHG8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\boxed{\n",
        "\\operatorname{lr}(g)=\n",
        "\\text{min_lr}+\n",
        "\\frac{\\text{peak_lr}-\\text{min_lr}}{2}\\,\n",
        "\\Bigl(\n",
        "1+\\cos\\!\\bigl(\n",
        "\\pi \\,\\frac{g-\\text{warmup_steps}}\n",
        "{\\text{total_training_steps}-\\text{warmup_steps}}\n",
        "\\bigr)\n",
        "\\Bigr)\n",
        "}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "YoADmthaH2wP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "min_lr = 0.1 * initial_lr\n",
        "track_lrs = []\n",
        "\n",
        "lr_increment = (peak_lr - initial_lr) / warmup_steps\n",
        "global_step = -1\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for input_batch, target_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        global_step += 1\n",
        "\n",
        "        # Adjust the learning rate based on the current phase (warmup or cosine annealing)\n",
        "        if global_step < warmup_steps:\n",
        "            # Linear warmup\n",
        "            lr = initial_lr + global_step * lr_increment\n",
        "        else:\n",
        "            # Cosine annealing after warmup\n",
        "            progress = ((global_step - warmup_steps) /\n",
        "                        (total_training_steps - warmup_steps))\n",
        "            lr = min_lr + (peak_lr - min_lr) * 0.5 * (\n",
        "                1 + math.cos(math.pi * progress))\n",
        "\n",
        "        # Apply the calculated learning rate to the optimizer\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group[\"lr\"] = lr\n",
        "        track_lrs.append(optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "        # Calculate loss and update weights"
      ],
      "metadata": {
        "id": "skI9PLU5GfC3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5, 3))\n",
        "plt.ylabel(\"Learning rate\")\n",
        "plt.xlabel(\"Step\")\n",
        "plt.plot(range(total_training_steps), track_lrs)\n",
        "plt.tight_layout(); plt.savefig(\"warmup_plus_cosine_decay_lr.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "PPQ4OgN6GfAO",
        "outputId": "f6436b32-e98f-4673-a3f0-70e900e64b0f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Gradient Clipping"
      ],
      "metadata": {
        "id": "U1UHSGRiIeGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- By setting a threshold, gradients exceeding this limit are scaled down to a maximum magnitude to ensure that the updates to the model's parameters during backpropagation remain within a manageable range\n",
        "- For instance, using the `max_norm=1.0` setting in PyTorch's `clip_grad_norm_` method means that the norm of the gradients is clipped such that their maximum norm does not exceed 1.0"
      ],
      "metadata": {
        "id": "4zpdkBzkIrex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch,\n",
        "                    target_batch,\n",
        "                    model,\n",
        "                    device):\n",
        "  input_batch = input_batch.to(device)\n",
        "  target_batch = target_batch.to(device)\n",
        "\n",
        "  logits = model(input_batch)\n",
        "  loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1),\n",
        "                                           target_batch.flatten())\n",
        "  return loss"
      ],
      "metadata": {
        "id": "XAkIyUCUGe-O"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPT2Model(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "\n",
        "loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "f9p3P6eYGe7c"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_highest_gradient(model):\n",
        "    max_grad = None\n",
        "    for param in model.parameters():\n",
        "        if param.grad is not None:\n",
        "            grad_values = param.grad.data.flatten()\n",
        "            max_grad_param = grad_values.max()\n",
        "            if max_grad is None or max_grad_param > max_grad:\n",
        "                max_grad = max_grad_param\n",
        "    return max_grad\n",
        "\n",
        "print(find_highest_gradient(model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QReFYQFGe5v",
        "outputId": "a92e69be-2649-4542-a238-97c63fb0d6b3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0520, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Applying gradient clipping, we can see that the largest gradient is now substantially smaller:"
      ],
      "metadata": {
        "id": "2-VFBGudJcmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "print(find_highest_gradient(model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdFUgaQJGe25",
        "outputId": "b230d6a4-2525-4011-bac9-f09ba275ae24"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0220, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. The modified training function"
      ],
      "metadata": {
        "id": "m4cJHXMaJtbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "  encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "  # turn the list of token IDs into tensor with batch dimension\n",
        "  encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "  return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(encoded_tensor, tokenizer):\n",
        "  # turn tensor without batch dimension to list\n",
        "  token_ids = encoded_tensor.squeeze(0).tolist()\n",
        "  text = tokenizer.decode(token_ids)\n",
        "  return text"
      ],
      "metadata": {
        "id": "0GJ3tvIkK_06"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_loader(dataloader,\n",
        "                     model,\n",
        "                     device,\n",
        "                     num_batches=None):\n",
        "  total_loss = 0.\n",
        "  if len(dataloader) == 0:\n",
        "    return float(\"nan\")\n",
        "  elif num_batches is None:\n",
        "    num_batches = len(dataloader)\n",
        "  else:\n",
        "    # reduce the number of batches to match the total number of batches in the data loader\n",
        "    # if num_batches exceeds the number of batches in the data loader\n",
        "    num_batches = min(num_batches, len(dataloader))\n",
        "  for i, (input_batch, target_batch) in enumerate(dataloader):\n",
        "    if i < num_batches:\n",
        "      loss = calc_loss_batch(input_batch,\n",
        "                             target_batch,\n",
        "                             model,\n",
        "                             device)\n",
        "      total_loss += loss.item()\n",
        "    else:\n",
        "      break\n",
        "  return total_loss / num_batches"
      ],
      "metadata": {
        "id": "hFrQ5FGuKprD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model,\n",
        "                         input_batch,  # [batch, num_tokens]\n",
        "                         max_new_tokens,  # numbers of new tokens to be predicted\n",
        "                         context_size):\n",
        "    for _ in range(max_new_tokens):\n",
        "        # crop current context if it exceeds the supported context_size\n",
        "        crop_input_batch = input_batch[:, -context_size:]\n",
        "\n",
        "        # predict next token\n",
        "        with torch.no_grad():\n",
        "            logits = model(crop_input_batch)\n",
        "\n",
        "        # consider only logits of the last token\n",
        "        logits = logits[:, -1, :]  # (batch, n_tokens, vocab_size) -> (batch, vocab_size)\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "        predicted_tokens = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "        # update input_batch (append predicted tokens to the sequences)\n",
        "        input_batch = torch.cat([input_batch, predicted_tokens], dim=-1)  # [batch, num_tokens+1]\n",
        "\n",
        "    return input_batch\n",
        "\n",
        "\n",
        "def generate_text_advanced(model,\n",
        "                           input_batch,\n",
        "                           max_new_tokens,\n",
        "                           context_size,\n",
        "                           temperature=1.0,\n",
        "                           top_k=None,\n",
        "                           top_p=None,\n",
        "                           repetition_penalty=1.0,\n",
        "                           eos_id=None):\n",
        "    \"\"\"\n",
        "    Advanced text generation with multiple decoding strategies.\n",
        "\n",
        "    Args:\n",
        "        model: The language model\n",
        "        input_batch: Input token ids [batch_size, seq_len]\n",
        "        max_new_tokens: Number of tokens to generate\n",
        "        context_size: Maximum context length the model can handle\n",
        "        temperature: Sampling temperature (1.0 = neutral, <1.0 = more focused, >1.0 = more random)\n",
        "        top_k: If set, only sample from the top k most likely tokens\n",
        "        top_p: If set, sample from the smallest set of tokens whose cumulative probability exceeds p\n",
        "        repetition_penalty: Penalty for repeating tokens (1.0 = no penalty, >1.0 = penalize repetitions)\n",
        "        eos_id: Optional end of sequence token id to stop generation early\n",
        "\n",
        "    Returns:\n",
        "        Tensor of generated token ids [batch_size, seq_len + max_new_tokens]\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Crop context if needed\n",
        "        crop_input_batch = input_batch[:, -context_size:]\n",
        "\n",
        "        # Get model predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(crop_input_batch)\n",
        "\n",
        "        # Consider only the last token's logits\n",
        "        logits = logits[:, -1, :]  # [batch_size, vocab_size]\n",
        "\n",
        "        # Apply repetition penalty\n",
        "        if repetition_penalty != 1.0:\n",
        "            # Get unique tokens in the input\n",
        "            used_tokens = torch.unique(input_batch)\n",
        "            # Penalize previously used tokens\n",
        "            logits.index_fill_(dim=-1, index=used_tokens,\n",
        "                               value=logits.index_select(dim=-1, index=used_tokens) / repetition_penalty)\n",
        "\n",
        "        # Apply temperature scaling\n",
        "        if temperature != 1.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "        # Convert to probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)\n",
        "\n",
        "        # Apply top-k filtering\n",
        "        if top_k is not None:\n",
        "            top_k = min(top_k, logits.size(-1))\n",
        "            top_logits, top_indices = torch.topk(logits, top_k)\n",
        "            # Create a mask for non-top-k values\n",
        "            mask = torch.zeros_like(logits).scatter_(dim=-1, index=top_indices, value=1)\n",
        "            # Set non-top-k values to -inf before softmax\n",
        "            logits = torch.where(mask > 0, logits, torch.tensor(-float('inf')).to(logits.device))\n",
        "            # Recompute probabilities\n",
        "            probas = torch.softmax(logits, dim=-1)\n",
        "\n",
        "        # Apply nucleus (top-p) sampling\n",
        "        if top_p is not None:\n",
        "            sorted_probas, sorted_indices = torch.sort(probas, descending=True)\n",
        "            cumsum_probas = torch.cumsum(sorted_probas, dim=-1)\n",
        "            # Remove tokens after cumsum exceeds top_p\n",
        "            mask = cumsum_probas <= top_p\n",
        "            # Always keep at least one token\n",
        "            mask[..., 0] = True\n",
        "            sorted_indices = sorted_indices[mask]\n",
        "            probas = torch.zeros_like(probas).scatter_(-1, sorted_indices, sorted_probas[mask])\n",
        "            probas.div_(probas.sum(dim=-1, keepdim=True))\n",
        "\n",
        "        # Sample next token\n",
        "        predicted_tokens = torch.multinomial(probas, num_samples=1)\n",
        "\n",
        "        # Stop if EOS token is generated\n",
        "        if eos_id is not None and (predicted_tokens == eos_id).any():\n",
        "            break\n",
        "\n",
        "        # Append prediction to input\n",
        "        input_batch = torch.cat([input_batch, predicted_tokens], dim=-1)\n",
        "\n",
        "    return input_batch"
      ],
      "metadata": {
        "id": "Y4PWsIImK28F"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model,\n",
        "                    train_loader,\n",
        "                    val_loader,\n",
        "                    device,\n",
        "                    eval_iter):\n",
        "  # set model to evaluation mode\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    # calculate loss\n",
        "    train_loss = calc_loss_loader(train_loader,\n",
        "                                  model,\n",
        "                                  device,\n",
        "                                  num_batches=eval_iter)\n",
        "    val_loss = calc_loss_loader(val_loader,\n",
        "                                model,\n",
        "                                device,\n",
        "                                num_batches=eval_iter)\n",
        "\n",
        "  # set model back to training mode\n",
        "  model.train()\n",
        "  return train_loss, val_loss\n",
        "\n",
        "\n",
        "def generate_and_print_sample(model,\n",
        "                              tokenizer,\n",
        "                              device,\n",
        "                              start_context):\n",
        "  # set model to evaluation mode\n",
        "  model.eval()\n",
        "  context_size = model.position_emb.weight.shape[0]\n",
        "  encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "  with torch.no_grad():\n",
        "    token_ids = generate_text_simple(model=model,\n",
        "                                     input_batch=encoded,\n",
        "                                     max_new_tokens=50,\n",
        "                                     context_size=context_size)\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \")) # compact print format\n",
        "  # set model back to training mode\n",
        "  model.train()"
      ],
      "metadata": {
        "id": "cdMPnQi2Ge0c"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now let's add the three concepts covered above to the `train_model_simple` function to create the more sophisticated `train_model` function below:"
      ],
      "metadata": {
        "id": "X2bw7aW8J1Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ORIG_BOOK_VERSION = False\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model,\n",
        "                train_loader,\n",
        "                val_loader,\n",
        "                optimizer,\n",
        "                device,\n",
        "                n_epochs,\n",
        "                eval_freq,\n",
        "                eval_iter,\n",
        "                start_context,\n",
        "                tokenizer,\n",
        "                warmup_steps,\n",
        "                initial_lr=3e-05,\n",
        "                min_lr=1e-6):\n",
        "\n",
        "  train_losses, val_losses = [], []\n",
        "  track_tokens_seen, track_lrs = [], []\n",
        "\n",
        "  token_seen = 0\n",
        "  global_step = -1\n",
        "\n",
        "  # retrieve the maximum/peak learning rate from the optimizer\n",
        "  peak_lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "  # calculate the total number of iterations in the training process\n",
        "  total_training_steps = len(train_loader) * n_epochs\n",
        "\n",
        "  # calculate the learning rate increment during the warmup phase\n",
        "  lr_increment = (peak_lr - initial_lr) / warmup_steps\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    for input_batch, target_batch in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      global_step += 1\n",
        "\n",
        "      # adjust the learning rate based on the current phase (warmup or cosine)\n",
        "      if global_step < warmup_steps:\n",
        "        lr = initial_lr + global_step * lr_increment\n",
        "      else:\n",
        "        # cosine annealing after warmup\n",
        "        progress = ((global_step - warmup_steps) /\n",
        "                    (total_training_steps - warmup_steps))\n",
        "        lr = (min_lr +\n",
        "         (peak_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress)))\n",
        "\n",
        "      # apply the calculated learning rate to the optimizer\n",
        "      for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "      track_lrs.append(lr) # store the current learning rate\n",
        "\n",
        "      # calculate and backpropagate the loss\n",
        "      loss = calc_loss_batch(input_batch,\n",
        "                             target_batch,\n",
        "                             model,\n",
        "                             device)\n",
        "      loss.backward()\n",
        "\n",
        "      # apply gradient clipping after the warmup phase to avoid exploding gradients\n",
        "      if ORIG_BOOK_VERSION:\n",
        "        if global_step > warmup_steps:\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "      else:\n",
        "        # the book originally used global_step > warmup_steps, which led to a skipped clipping step after warmup\n",
        "        if global_step >= warmup_steps:\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "      optimizer.step()\n",
        "      token_seen += input_batch.numel()\n",
        "\n",
        "      # periodically evaluate the model on the training and validation sets\n",
        "      if global_step % eval_freq == 0:\n",
        "        train_loss, val_loss = evaluate_model(model,\n",
        "                                              train_loader,\n",
        "                                              val_loader,\n",
        "                                              device,\n",
        "                                              eval_iter)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        track_tokens_seen.append(token_seen)\n",
        "        # print the current losses\n",
        "        print(f\"Ep {epoch+1} (Iter {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, \"\n",
        "                      f\"Val loss {val_loss:.3f}\")\n",
        "\n",
        "    # generate and print a sample from the model to monitor progess\n",
        "    generate_and_print_sample(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=device,\n",
        "        start_context=start_context\n",
        "    )\n",
        "\n",
        "  return train_losses, val_losses, track_tokens_seen, track_lrs"
      ],
      "metadata": {
        "id": "kC9rn_4VGeyQ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(211)\n",
        "model = GPT2Model(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "\n",
        "peak_lr = 0.001  # this was originally set to 5e-4 in the book by mistake\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=peak_lr, weight_decay=0.1)  # the book accidentally omitted the lr assignment\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "n_epochs = 5\n",
        "train_losses, val_losses, tokens_seen, lrs = train_model(\n",
        "    model, train_loader, val_loader, optimizer, device, n_epochs=n_epochs,\n",
        "    eval_freq=5, eval_iter=1, start_context=\"In the midst of winter\",\n",
        "    tokenizer=tokenizer, warmup_steps=warmup_steps,\n",
        "    initial_lr=1e-5, min_lr=1e-5\n",
        ")\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hJ5OPkKGeq6",
        "outputId": "fa2c4b49-cadd-4509-8da7-9734a6183abc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Iter 000000): Train loss 10.911, Val loss 10.889\n",
            "Ep 1 (Iter 000005): Train loss 10.639, Val loss 10.609\n",
            "Ep 1 (Iter 000010): Train loss 10.250, Val loss 10.283\n",
            "Ep 1 (Iter 000015): Train loss 9.895, Val loss 9.938\n",
            "Ep 1 (Iter 000020): Train loss 9.671, Val loss 9.634\n",
            "Ep 1 (Iter 000025): Train loss 9.391, Val loss 9.396\n",
            "Ep 1 (Iter 000030): Train loss 9.059, Val loss 9.201\n",
            "Ep 1 (Iter 000035): Train loss 9.222, Val loss 9.039\n",
            "Ep 1 (Iter 000040): Train loss 9.014, Val loss 8.911\n",
            "Ep 1 (Iter 000045): Train loss 8.968, Val loss 8.791\n",
            "Ep 1 (Iter 000050): Train loss 8.987, Val loss 8.686\n",
            "Ep 1 (Iter 000055): Train loss 8.616, Val loss 8.588\n",
            "Ep 1 (Iter 000060): Train loss 8.499, Val loss 8.477\n",
            "Ep 1 (Iter 000065): Train loss 8.456, Val loss 8.365\n",
            "Ep 1 (Iter 000070): Train loss 8.419, Val loss 8.261\n",
            "Ep 1 (Iter 000075): Train loss 8.223, Val loss 8.160\n",
            "Ep 1 (Iter 000080): Train loss 8.287, Val loss 8.054\n",
            "Ep 1 (Iter 000085): Train loss 7.972, Val loss 7.953\n",
            "Ep 1 (Iter 000090): Train loss 7.952, Val loss 7.850\n",
            "Ep 1 (Iter 000095): Train loss 7.867, Val loss 7.771\n",
            "Ep 1 (Iter 000100): Train loss 7.615, Val loss 7.686\n",
            "Ep 1 (Iter 000105): Train loss 7.387, Val loss 7.579\n",
            "Ep 1 (Iter 000110): Train loss 7.636, Val loss 7.471\n",
            "Ep 1 (Iter 000115): Train loss 7.978, Val loss 7.380\n",
            "Ep 1 (Iter 000120): Train loss 7.227, Val loss 7.315\n",
            "Ep 1 (Iter 000125): Train loss 7.218, Val loss 7.245\n",
            "Ep 1 (Iter 000130): Train loss 7.340, Val loss 7.173\n",
            "Ep 1 (Iter 000135): Train loss 7.366, Val loss 7.108\n",
            "Ep 1 (Iter 000140): Train loss 6.752, Val loss 7.044\n",
            "Ep 1 (Iter 000145): Train loss 6.660, Val loss 6.996\n",
            "Ep 1 (Iter 000150): Train loss 7.012, Val loss 6.948\n",
            "Ep 1 (Iter 000155): Train loss 6.564, Val loss 6.898\n",
            "Ep 1 (Iter 000160): Train loss 7.052, Val loss 6.859\n",
            "Ep 1 (Iter 000165): Train loss 6.908, Val loss 6.791\n",
            "Ep 1 (Iter 000170): Train loss 6.662, Val loss 6.753\n",
            "Ep 1 (Iter 000175): Train loss 6.366, Val loss 6.715\n",
            "Ep 1 (Iter 000180): Train loss 7.076, Val loss 6.678\n",
            "Ep 1 (Iter 000185): Train loss 6.340, Val loss 6.654\n",
            "Ep 1 (Iter 000190): Train loss 6.349, Val loss 6.635\n",
            "Ep 1 (Iter 000195): Train loss 6.158, Val loss 6.639\n",
            "Ep 1 (Iter 000200): Train loss 6.039, Val loss 6.626\n",
            "Ep 1 (Iter 000205): Train loss 6.432, Val loss 6.576\n",
            "Ep 1 (Iter 000210): Train loss 6.210, Val loss 6.536\n",
            "Ep 1 (Iter 000215): Train loss 6.255, Val loss 6.520\n",
            "Ep 1 (Iter 000220): Train loss 6.368, Val loss 6.540\n",
            "Ep 1 (Iter 000225): Train loss 5.801, Val loss 6.528\n",
            "Ep 1 (Iter 000230): Train loss 6.059, Val loss 6.522\n",
            "Ep 1 (Iter 000235): Train loss 6.259, Val loss 6.508\n",
            "Ep 1 (Iter 000240): Train loss 6.573, Val loss 6.459\n",
            "Ep 1 (Iter 000245): Train loss 6.267, Val loss 6.418\n",
            "Ep 1 (Iter 000250): Train loss 5.865, Val loss 6.435\n",
            "Ep 1 (Iter 000255): Train loss 6.393, Val loss 6.441\n",
            "Ep 1 (Iter 000260): Train loss 6.259, Val loss 6.399\n",
            "Ep 1 (Iter 000265): Train loss 5.525, Val loss 6.354\n",
            "Ep 1 (Iter 000270): Train loss 6.228, Val loss 6.353\n",
            "Ep 1 (Iter 000275): Train loss 5.775, Val loss 6.318\n",
            "Ep 1 (Iter 000280): Train loss 6.400, Val loss 6.298\n",
            "Ep 1 (Iter 000285): Train loss 5.364, Val loss 6.289\n",
            "Ep 1 (Iter 000290): Train loss 5.481, Val loss 6.298\n",
            "Ep 1 (Iter 000295): Train loss 6.158, Val loss 6.272\n",
            "Ep 1 (Iter 000300): Train loss 6.071, Val loss 6.226\n",
            "Ep 1 (Iter 000305): Train loss 5.484, Val loss 6.210\n",
            "Ep 1 (Iter 000310): Train loss 6.265, Val loss 6.180\n",
            "Ep 1 (Iter 000315): Train loss 5.493, Val loss 6.171\n",
            "Ep 1 (Iter 000320): Train loss 6.152, Val loss 6.130\n",
            "Ep 1 (Iter 000325): Train loss 6.677, Val loss 6.128\n",
            "Ep 1 (Iter 000330): Train loss 5.611, Val loss 6.137\n",
            "Ep 1 (Iter 000335): Train loss 5.592, Val loss 6.110\n",
            "Ep 1 (Iter 000340): Train loss 6.156, Val loss 6.093\n",
            "Ep 1 (Iter 000345): Train loss 6.280, Val loss 6.070\n",
            "Ep 1 (Iter 000350): Train loss 5.885, Val loss 6.072\n",
            "Ep 1 (Iter 000355): Train loss 6.112, Val loss 6.080\n",
            "Ep 1 (Iter 000360): Train loss 5.426, Val loss 6.071\n",
            "Ep 1 (Iter 000365): Train loss 6.278, Val loss 6.052\n",
            "Ep 1 (Iter 000370): Train loss 6.215, Val loss 6.013\n",
            "Ep 1 (Iter 000375): Train loss 5.840, Val loss 6.077\n",
            "Ep 1 (Iter 000380): Train loss 5.598, Val loss 6.038\n",
            "Ep 1 (Iter 000385): Train loss 6.076, Val loss 6.006\n",
            "Ep 1 (Iter 000390): Train loss 6.289, Val loss 6.037\n",
            "Ep 1 (Iter 000395): Train loss 6.248, Val loss 6.004\n",
            "Ep 1 (Iter 000400): Train loss 6.393, Val loss 6.006\n",
            "Ep 1 (Iter 000405): Train loss 5.101, Val loss 5.997\n",
            "Ep 1 (Iter 000410): Train loss 5.901, Val loss 5.970\n",
            "Ep 1 (Iter 000415): Train loss 5.593, Val loss 5.931\n",
            "Ep 1 (Iter 000420): Train loss 5.284, Val loss 5.946\n",
            "Ep 1 (Iter 000425): Train loss 5.896, Val loss 5.930\n",
            "Ep 1 (Iter 000430): Train loss 6.016, Val loss 5.912\n",
            "Ep 1 (Iter 000435): Train loss 6.242, Val loss 5.926\n",
            "Ep 1 (Iter 000440): Train loss 5.674, Val loss 5.933\n",
            "Ep 1 (Iter 000445): Train loss 5.456, Val loss 5.886\n",
            "Ep 1 (Iter 000450): Train loss 5.008, Val loss 5.887\n",
            "Ep 1 (Iter 000455): Train loss 5.205, Val loss 5.895\n",
            "Ep 1 (Iter 000460): Train loss 5.990, Val loss 5.873\n",
            "Ep 1 (Iter 000465): Train loss 5.845, Val loss 5.906\n",
            "Ep 1 (Iter 000470): Train loss 5.478, Val loss 5.895\n",
            "Ep 1 (Iter 000475): Train loss 5.467, Val loss 5.923\n",
            "Ep 1 (Iter 000480): Train loss 6.126, Val loss 5.996\n",
            "Ep 1 (Iter 000485): Train loss 5.072, Val loss 5.917\n",
            "Ep 1 (Iter 000490): Train loss 5.224, Val loss 5.851\n",
            "Ep 1 (Iter 000495): Train loss 5.940, Val loss 5.838\n",
            "Ep 1 (Iter 000500): Train loss 5.613, Val loss 5.841\n",
            "Ep 1 (Iter 000505): Train loss 6.086, Val loss 5.801\n",
            "Ep 1 (Iter 000510): Train loss 5.677, Val loss 5.845\n",
            "Ep 1 (Iter 000515): Train loss 5.781, Val loss 5.843\n",
            "Ep 1 (Iter 000520): Train loss 5.612, Val loss 5.820\n",
            "Ep 1 (Iter 000525): Train loss 5.268, Val loss 5.825\n",
            "Ep 1 (Iter 000530): Train loss 5.207, Val loss 5.782\n",
            "Ep 1 (Iter 000535): Train loss 5.702, Val loss 5.812\n",
            "Ep 1 (Iter 000540): Train loss 5.640, Val loss 5.798\n",
            "Ep 1 (Iter 000545): Train loss 5.824, Val loss 5.820\n",
            "Ep 1 (Iter 000550): Train loss 5.751, Val loss 5.765\n",
            "Ep 1 (Iter 000555): Train loss 5.639, Val loss 5.738\n",
            "Ep 1 (Iter 000560): Train loss 5.692, Val loss 5.730\n",
            "Ep 1 (Iter 000565): Train loss 5.505, Val loss 5.725\n",
            "Ep 1 (Iter 000570): Train loss 5.937, Val loss 5.735\n",
            "Ep 1 (Iter 000575): Train loss 5.505, Val loss 5.754\n",
            "Ep 1 (Iter 000580): Train loss 5.641, Val loss 5.721\n",
            "Ep 1 (Iter 000585): Train loss 5.641, Val loss 5.711\n",
            "Ep 1 (Iter 000590): Train loss 6.088, Val loss 5.744\n",
            "Ep 1 (Iter 000595): Train loss 5.419, Val loss 5.717\n",
            "Ep 1 (Iter 000600): Train loss 5.627, Val loss 5.678\n",
            "Ep 1 (Iter 000605): Train loss 5.836, Val loss 5.681\n",
            "Ep 1 (Iter 000610): Train loss 5.722, Val loss 5.687\n",
            "Ep 1 (Iter 000615): Train loss 5.363, Val loss 5.699\n",
            "Ep 1 (Iter 000620): Train loss 5.353, Val loss 5.674\n",
            "Ep 1 (Iter 000625): Train loss 5.660, Val loss 5.674\n",
            "Ep 1 (Iter 000630): Train loss 6.020, Val loss 5.694\n",
            "Ep 1 (Iter 000635): Train loss 5.830, Val loss 5.683\n",
            "Ep 1 (Iter 000640): Train loss 5.423, Val loss 5.661\n",
            "Ep 1 (Iter 000645): Train loss 5.420, Val loss 5.640\n",
            "Ep 1 (Iter 000650): Train loss 5.317, Val loss 5.656\n",
            "Ep 1 (Iter 000655): Train loss 5.327, Val loss 5.644\n",
            "Ep 1 (Iter 000660): Train loss 4.610, Val loss 5.648\n",
            "Ep 1 (Iter 000665): Train loss 5.070, Val loss 5.631\n",
            "Ep 1 (Iter 000670): Train loss 5.901, Val loss 5.632\n",
            "Ep 1 (Iter 000675): Train loss 4.251, Val loss 5.634\n",
            "Ep 1 (Iter 000680): Train loss 5.227, Val loss 5.655\n",
            "Ep 1 (Iter 000685): Train loss 5.584, Val loss 5.644\n",
            "Ep 1 (Iter 000690): Train loss 5.154, Val loss 5.634\n",
            "Ep 1 (Iter 000695): Train loss 5.133, Val loss 5.645\n",
            "Ep 1 (Iter 000700): Train loss 5.073, Val loss 5.623\n",
            "Ep 1 (Iter 000705): Train loss 5.563, Val loss 5.665\n",
            "Ep 1 (Iter 000710): Train loss 5.041, Val loss 5.642\n",
            "Ep 1 (Iter 000715): Train loss 4.663, Val loss 5.635\n",
            "Ep 1 (Iter 000720): Train loss 4.849, Val loss 5.626\n",
            "Ep 1 (Iter 000725): Train loss 5.756, Val loss 5.603\n",
            "Ep 1 (Iter 000730): Train loss 5.562, Val loss 5.590\n",
            "Ep 1 (Iter 000735): Train loss 5.461, Val loss 5.600\n",
            "Ep 1 (Iter 000740): Train loss 5.089, Val loss 5.605\n",
            "Ep 1 (Iter 000745): Train loss 4.684, Val loss 5.602\n",
            "Ep 1 (Iter 000750): Train loss 6.383, Val loss 5.604\n",
            "Ep 1 (Iter 000755): Train loss 4.673, Val loss 5.627\n",
            "Ep 1 (Iter 000760): Train loss 5.329, Val loss 5.609\n",
            "Ep 1 (Iter 000765): Train loss 5.133, Val loss 5.603\n",
            "Ep 1 (Iter 000770): Train loss 5.366, Val loss 5.610\n",
            "Ep 1 (Iter 000775): Train loss 5.081, Val loss 5.672\n",
            "Ep 1 (Iter 000780): Train loss 5.764, Val loss 5.627\n",
            "Ep 1 (Iter 000785): Train loss 4.436, Val loss 5.617\n",
            "Ep 1 (Iter 000790): Train loss 5.218, Val loss 5.620\n",
            "Ep 1 (Iter 000795): Train loss 6.067, Val loss 5.601\n",
            "Ep 1 (Iter 000800): Train loss 5.090, Val loss 5.601\n",
            "Ep 1 (Iter 000805): Train loss 5.395, Val loss 5.588\n",
            "Ep 1 (Iter 000810): Train loss 5.205, Val loss 5.563\n",
            "Ep 1 (Iter 000815): Train loss 4.438, Val loss 5.553\n",
            "Ep 1 (Iter 000820): Train loss 5.661, Val loss 5.566\n",
            "Ep 1 (Iter 000825): Train loss 5.549, Val loss 5.560\n",
            "Ep 1 (Iter 000830): Train loss 5.223, Val loss 5.598\n",
            "Ep 1 (Iter 000835): Train loss 5.750, Val loss 5.576\n",
            "Ep 1 (Iter 000840): Train loss 5.217, Val loss 5.598\n",
            "Ep 1 (Iter 000845): Train loss 4.883, Val loss 5.579\n",
            "Ep 1 (Iter 000850): Train loss 5.262, Val loss 5.559\n",
            "Ep 1 (Iter 000855): Train loss 5.050, Val loss 5.529\n",
            "Ep 1 (Iter 000860): Train loss 5.303, Val loss 5.552\n",
            "Ep 1 (Iter 000865): Train loss 5.410, Val loss 5.548\n",
            "Ep 1 (Iter 000870): Train loss 5.379, Val loss 5.530\n",
            "Ep 1 (Iter 000875): Train loss 4.754, Val loss 5.523\n",
            "Ep 1 (Iter 000880): Train loss 5.007, Val loss 5.541\n",
            "Ep 1 (Iter 000885): Train loss 5.082, Val loss 5.508\n",
            "Ep 1 (Iter 000890): Train loss 5.001, Val loss 5.484\n",
            "Ep 1 (Iter 000895): Train loss 5.344, Val loss 5.465\n",
            "Ep 1 (Iter 000900): Train loss 5.452, Val loss 5.464\n",
            "Ep 1 (Iter 000905): Train loss 4.183, Val loss 5.519\n",
            "Ep 1 (Iter 000910): Train loss 4.974, Val loss 5.500\n",
            "Ep 1 (Iter 000915): Train loss 5.128, Val loss 5.471\n",
            "Ep 1 (Iter 000920): Train loss 5.604, Val loss 5.510\n",
            "Ep 1 (Iter 000925): Train loss 5.604, Val loss 5.528\n",
            "Ep 1 (Iter 000930): Train loss 5.391, Val loss 5.481\n",
            "Ep 1 (Iter 000935): Train loss 4.270, Val loss 5.505\n",
            "Ep 1 (Iter 000940): Train loss 5.114, Val loss 5.509\n",
            "Ep 1 (Iter 000945): Train loss 5.424, Val loss 5.571\n",
            "Ep 1 (Iter 000950): Train loss 5.322, Val loss 5.511\n",
            "Ep 1 (Iter 000955): Train loss 5.243, Val loss 5.715\n",
            "Ep 1 (Iter 000960): Train loss 5.640, Val loss 5.583\n",
            "Ep 1 (Iter 000965): Train loss 5.429, Val loss 5.575\n",
            "Ep 1 (Iter 000970): Train loss 5.320, Val loss 5.528\n",
            "Ep 1 (Iter 000975): Train loss 5.002, Val loss 5.570\n",
            "Ep 1 (Iter 000980): Train loss 5.548, Val loss 5.544\n",
            "Ep 1 (Iter 000985): Train loss 5.267, Val loss 5.539\n",
            "Ep 1 (Iter 000990): Train loss 4.943, Val loss 5.537\n",
            "Ep 1 (Iter 000995): Train loss 5.546, Val loss 5.522\n",
            "Ep 1 (Iter 001000): Train loss 5.170, Val loss 5.457\n",
            "Ep 1 (Iter 001005): Train loss 5.562, Val loss 5.446\n",
            "Ep 1 (Iter 001010): Train loss 4.344, Val loss 5.464\n",
            "Ep 1 (Iter 001015): Train loss 5.331, Val loss 5.434\n",
            "Ep 1 (Iter 001020): Train loss 4.802, Val loss 5.476\n",
            "Ep 1 (Iter 001025): Train loss 5.556, Val loss 5.504\n",
            "Ep 1 (Iter 001030): Train loss 5.371, Val loss 5.521\n",
            "Ep 1 (Iter 001035): Train loss 4.886, Val loss 5.472\n",
            "Ep 1 (Iter 001040): Train loss 5.180, Val loss 5.414\n",
            "Ep 1 (Iter 001045): Train loss 5.066, Val loss 5.397\n",
            "Ep 1 (Iter 001050): Train loss 5.395, Val loss 5.456\n",
            "Ep 1 (Iter 001055): Train loss 5.413, Val loss 5.466\n",
            "Ep 1 (Iter 001060): Train loss 4.688, Val loss 5.422\n",
            "Ep 1 (Iter 001065): Train loss 5.887, Val loss 5.451\n",
            "Ep 1 (Iter 001070): Train loss 5.346, Val loss 5.384\n",
            "Ep 1 (Iter 001075): Train loss 5.138, Val loss 5.397\n",
            "Ep 1 (Iter 001080): Train loss 4.706, Val loss 5.367\n",
            "Ep 1 (Iter 001085): Train loss 4.953, Val loss 5.377\n",
            "Ep 1 (Iter 001090): Train loss 5.034, Val loss 5.368\n",
            "Ep 1 (Iter 001095): Train loss 5.202, Val loss 5.393\n",
            "Ep 1 (Iter 001100): Train loss 5.619, Val loss 5.405\n",
            "Ep 1 (Iter 001105): Train loss 4.936, Val loss 5.428\n",
            "Ep 1 (Iter 001110): Train loss 4.961, Val loss 5.413\n",
            "Ep 1 (Iter 001115): Train loss 4.798, Val loss 5.474\n",
            "Ep 1 (Iter 001120): Train loss 5.048, Val loss 5.436\n",
            "Ep 1 (Iter 001125): Train loss 5.281, Val loss 5.406\n",
            "Ep 1 (Iter 001130): Train loss 5.253, Val loss 5.405\n",
            "Ep 1 (Iter 001135): Train loss 4.478, Val loss 5.446\n",
            "Ep 1 (Iter 001140): Train loss 4.799, Val loss 5.395\n",
            "Ep 1 (Iter 001145): Train loss 4.815, Val loss 5.399\n",
            "Ep 1 (Iter 001150): Train loss 4.386, Val loss 5.332\n",
            "Ep 1 (Iter 001155): Train loss 4.768, Val loss 5.380\n",
            "Ep 1 (Iter 001160): Train loss 5.014, Val loss 5.387\n",
            "Ep 1 (Iter 001165): Train loss 5.020, Val loss 5.362\n",
            "Ep 1 (Iter 001170): Train loss 5.661, Val loss 5.346\n",
            "Ep 1 (Iter 001175): Train loss 5.359, Val loss 5.354\n",
            "Ep 1 (Iter 001180): Train loss 5.203, Val loss 5.308\n",
            "Ep 1 (Iter 001185): Train loss 5.448, Val loss 5.330\n",
            "Ep 1 (Iter 001190): Train loss 4.989, Val loss 5.378\n",
            "Ep 1 (Iter 001195): Train loss 5.636, Val loss 5.371\n",
            "Ep 1 (Iter 001200): Train loss 4.760, Val loss 5.364\n",
            "Ep 1 (Iter 001205): Train loss 5.058, Val loss 5.371\n",
            "Ep 1 (Iter 001210): Train loss 4.963, Val loss 5.353\n",
            "Ep 1 (Iter 001215): Train loss 4.978, Val loss 5.383\n",
            "Ep 1 (Iter 001220): Train loss 4.767, Val loss 5.361\n",
            "Ep 1 (Iter 001225): Train loss 5.306, Val loss 5.397\n",
            "Ep 1 (Iter 001230): Train loss 5.010, Val loss 5.375\n",
            "Ep 1 (Iter 001235): Train loss 4.729, Val loss 5.356\n",
            "Ep 1 (Iter 001240): Train loss 4.652, Val loss 5.352\n",
            "Ep 1 (Iter 001245): Train loss 5.312, Val loss 5.357\n",
            "Ep 1 (Iter 001250): Train loss 5.138, Val loss 5.347\n",
            "Ep 1 (Iter 001255): Train loss 4.571, Val loss 5.323\n",
            "Ep 1 (Iter 001260): Train loss 5.226, Val loss 5.330\n",
            "Ep 1 (Iter 001265): Train loss 4.766, Val loss 5.363\n",
            "Ep 1 (Iter 001270): Train loss 5.152, Val loss 5.329\n",
            "Ep 1 (Iter 001275): Train loss 4.623, Val loss 5.329\n",
            "Ep 1 (Iter 001280): Train loss 4.708, Val loss 5.318\n",
            "Ep 1 (Iter 001285): Train loss 5.467, Val loss 5.306\n",
            "Ep 1 (Iter 001290): Train loss 5.195, Val loss 5.277\n",
            "Ep 1 (Iter 001295): Train loss 5.335, Val loss 5.348\n",
            "Ep 1 (Iter 001300): Train loss 5.435, Val loss 5.339\n",
            "Ep 1 (Iter 001305): Train loss 4.817, Val loss 5.320\n",
            "Ep 1 (Iter 001310): Train loss 5.214, Val loss 5.326\n",
            "Ep 1 (Iter 001315): Train loss 5.122, Val loss 5.373\n",
            "Ep 1 (Iter 001320): Train loss 4.052, Val loss 5.346\n",
            "Ep 1 (Iter 001325): Train loss 5.214, Val loss 5.333\n",
            "Ep 1 (Iter 001330): Train loss 4.630, Val loss 5.283\n",
            "Ep 1 (Iter 001335): Train loss 4.077, Val loss 5.320\n",
            "Ep 1 (Iter 001340): Train loss 4.937, Val loss 5.303\n",
            "Ep 1 (Iter 001345): Train loss 5.328, Val loss 5.331\n",
            "Ep 1 (Iter 001350): Train loss 5.117, Val loss 5.382\n",
            "Ep 1 (Iter 001355): Train loss 5.083, Val loss 5.360\n",
            "Ep 1 (Iter 001360): Train loss 4.962, Val loss 5.341\n",
            "Ep 1 (Iter 001365): Train loss 5.055, Val loss 5.390\n",
            "Ep 1 (Iter 001370): Train loss 4.712, Val loss 5.353\n",
            "Ep 1 (Iter 001375): Train loss 4.414, Val loss 5.357\n",
            "Ep 1 (Iter 001380): Train loss 5.032, Val loss 5.317\n",
            "Ep 1 (Iter 001385): Train loss 5.373, Val loss 5.290\n",
            "Ep 1 (Iter 001390): Train loss 4.781, Val loss 5.291\n",
            "Ep 1 (Iter 001395): Train loss 4.786, Val loss 5.294\n",
            "Ep 1 (Iter 001400): Train loss 4.611, Val loss 5.254\n",
            "Ep 1 (Iter 001405): Train loss 5.446, Val loss 5.255\n",
            "Ep 1 (Iter 001410): Train loss 5.249, Val loss 5.241\n",
            "Ep 1 (Iter 001415): Train loss 4.827, Val loss 5.247\n",
            "Ep 1 (Iter 001420): Train loss 4.804, Val loss 5.274\n",
            "Ep 1 (Iter 001425): Train loss 5.070, Val loss 5.301\n",
            "Ep 1 (Iter 001430): Train loss 4.735, Val loss 5.316\n",
            "Ep 1 (Iter 001435): Train loss 5.543, Val loss 5.306\n",
            "Ep 1 (Iter 001440): Train loss 4.659, Val loss 5.302\n",
            "Ep 1 (Iter 001445): Train loss 5.068, Val loss 5.296\n",
            "Ep 1 (Iter 001450): Train loss 4.766, Val loss 5.250\n",
            "Ep 1 (Iter 001455): Train loss 5.123, Val loss 5.275\n",
            "Ep 1 (Iter 001460): Train loss 4.910, Val loss 5.289\n",
            "Ep 1 (Iter 001465): Train loss 4.918, Val loss 5.298\n",
            "Ep 1 (Iter 001470): Train loss 5.045, Val loss 5.346\n",
            "Ep 1 (Iter 001475): Train loss 4.764, Val loss 5.343\n",
            "Ep 1 (Iter 001480): Train loss 5.138, Val loss 5.325\n",
            "Ep 1 (Iter 001485): Train loss 4.790, Val loss 5.359\n",
            "Ep 1 (Iter 001490): Train loss 4.991, Val loss 5.287\n",
            "Ep 1 (Iter 001495): Train loss 4.838, Val loss 5.347\n",
            "Ep 1 (Iter 001500): Train loss 5.047, Val loss 5.331\n",
            "Ep 1 (Iter 001505): Train loss 5.060, Val loss 5.293\n",
            "Ep 1 (Iter 001510): Train loss 4.988, Val loss 5.326\n",
            "In the midst of winter of the the count, and the the first of the the countess, and the the count was the first to the  I have been and I s s \n",
            "Ep 2 (Iter 001515): Train loss 5.473, Val loss 5.288\n",
            "Ep 2 (Iter 001520): Train loss 5.189, Val loss 5.323\n",
            "Ep 2 (Iter 001525): Train loss 4.730, Val loss 5.341\n",
            "Ep 2 (Iter 001530): Train loss 5.306, Val loss 5.279\n",
            "Ep 2 (Iter 001535): Train loss 4.662, Val loss 5.306\n",
            "Ep 2 (Iter 001540): Train loss 4.635, Val loss 5.293\n",
            "Ep 2 (Iter 001545): Train loss 4.869, Val loss 5.272\n",
            "Ep 2 (Iter 001550): Train loss 5.235, Val loss 5.337\n",
            "Ep 2 (Iter 001555): Train loss 4.043, Val loss 5.312\n",
            "Ep 2 (Iter 001560): Train loss 4.224, Val loss 5.312\n",
            "Ep 2 (Iter 001565): Train loss 5.186, Val loss 5.299\n",
            "Ep 2 (Iter 001570): Train loss 4.728, Val loss 5.208\n",
            "Ep 2 (Iter 001575): Train loss 4.503, Val loss 5.216\n",
            "Ep 2 (Iter 001580): Train loss 4.106, Val loss 5.254\n",
            "Ep 2 (Iter 001585): Train loss 4.631, Val loss 5.247\n",
            "Ep 2 (Iter 001590): Train loss 4.817, Val loss 5.301\n",
            "Ep 2 (Iter 001595): Train loss 4.436, Val loss 5.235\n",
            "Ep 2 (Iter 001600): Train loss 4.953, Val loss 5.218\n",
            "Ep 2 (Iter 001605): Train loss 4.385, Val loss 5.243\n",
            "Ep 2 (Iter 001610): Train loss 4.856, Val loss 5.285\n",
            "Ep 2 (Iter 001615): Train loss 4.682, Val loss 5.253\n",
            "Ep 2 (Iter 001620): Train loss 5.149, Val loss 5.279\n",
            "Ep 2 (Iter 001625): Train loss 4.978, Val loss 5.253\n",
            "Ep 2 (Iter 001630): Train loss 4.308, Val loss 5.271\n",
            "Ep 2 (Iter 001635): Train loss 5.228, Val loss 5.215\n",
            "Ep 2 (Iter 001640): Train loss 4.522, Val loss 5.152\n",
            "Ep 2 (Iter 001645): Train loss 4.183, Val loss 5.160\n",
            "Ep 2 (Iter 001650): Train loss 4.462, Val loss 5.162\n",
            "Ep 2 (Iter 001655): Train loss 4.867, Val loss 5.163\n",
            "Ep 2 (Iter 001660): Train loss 4.615, Val loss 5.208\n",
            "Ep 2 (Iter 001665): Train loss 5.194, Val loss 5.279\n",
            "Ep 2 (Iter 001670): Train loss 3.886, Val loss 5.264\n",
            "Ep 2 (Iter 001675): Train loss 5.115, Val loss 5.275\n",
            "Ep 2 (Iter 001680): Train loss 4.923, Val loss 5.258\n",
            "Ep 2 (Iter 001685): Train loss 4.888, Val loss 5.300\n",
            "Ep 2 (Iter 001690): Train loss 4.895, Val loss 5.345\n",
            "Ep 2 (Iter 001695): Train loss 4.463, Val loss 5.278\n",
            "Ep 2 (Iter 001700): Train loss 4.679, Val loss 5.253\n",
            "Ep 2 (Iter 001705): Train loss 4.888, Val loss 5.316\n",
            "Ep 2 (Iter 001710): Train loss 4.819, Val loss 5.265\n",
            "Ep 2 (Iter 001715): Train loss 3.930, Val loss 5.291\n",
            "Ep 2 (Iter 001720): Train loss 4.684, Val loss 5.253\n",
            "Ep 2 (Iter 001725): Train loss 4.692, Val loss 5.233\n",
            "Ep 2 (Iter 001730): Train loss 4.852, Val loss 5.236\n",
            "Ep 2 (Iter 001735): Train loss 4.442, Val loss 5.237\n",
            "Ep 2 (Iter 001740): Train loss 4.673, Val loss 5.310\n",
            "Ep 2 (Iter 001745): Train loss 4.094, Val loss 5.327\n",
            "Ep 2 (Iter 001750): Train loss 5.362, Val loss 5.303\n",
            "Ep 2 (Iter 001755): Train loss 4.986, Val loss 5.263\n",
            "Ep 2 (Iter 001760): Train loss 4.500, Val loss 5.301\n",
            "Ep 2 (Iter 001765): Train loss 4.973, Val loss 5.295\n",
            "Ep 2 (Iter 001770): Train loss 4.551, Val loss 5.299\n",
            "Ep 2 (Iter 001775): Train loss 4.329, Val loss 5.254\n",
            "Ep 2 (Iter 001780): Train loss 4.993, Val loss 5.228\n",
            "Ep 2 (Iter 001785): Train loss 4.763, Val loss 5.240\n",
            "Ep 2 (Iter 001790): Train loss 4.819, Val loss 5.236\n",
            "Ep 2 (Iter 001795): Train loss 4.810, Val loss 5.180\n",
            "Ep 2 (Iter 001800): Train loss 5.014, Val loss 5.218\n",
            "Ep 2 (Iter 001805): Train loss 5.158, Val loss 5.185\n",
            "Ep 2 (Iter 001810): Train loss 5.034, Val loss 5.189\n",
            "Ep 2 (Iter 001815): Train loss 4.935, Val loss 5.180\n",
            "Ep 2 (Iter 001820): Train loss 4.244, Val loss 5.197\n",
            "Ep 2 (Iter 001825): Train loss 4.460, Val loss 5.230\n",
            "Ep 2 (Iter 001830): Train loss 5.246, Val loss 5.267\n",
            "Ep 2 (Iter 001835): Train loss 4.866, Val loss 5.281\n",
            "Ep 2 (Iter 001840): Train loss 4.349, Val loss 5.271\n",
            "Ep 2 (Iter 001845): Train loss 4.161, Val loss 5.258\n",
            "Ep 2 (Iter 001850): Train loss 4.063, Val loss 5.183\n",
            "Ep 2 (Iter 001855): Train loss 5.363, Val loss 5.236\n",
            "Ep 2 (Iter 001860): Train loss 4.339, Val loss 5.240\n",
            "Ep 2 (Iter 001865): Train loss 4.409, Val loss 5.195\n",
            "Ep 2 (Iter 001870): Train loss 5.299, Val loss 5.178\n",
            "Ep 2 (Iter 001875): Train loss 4.309, Val loss 5.186\n",
            "Ep 2 (Iter 001880): Train loss 4.286, Val loss 5.188\n",
            "Ep 2 (Iter 001885): Train loss 5.029, Val loss 5.232\n",
            "Ep 2 (Iter 001890): Train loss 4.832, Val loss 5.242\n",
            "Ep 2 (Iter 001895): Train loss 3.710, Val loss 5.238\n",
            "Ep 2 (Iter 001900): Train loss 4.270, Val loss 5.230\n",
            "Ep 2 (Iter 001905): Train loss 4.208, Val loss 5.236\n",
            "Ep 2 (Iter 001910): Train loss 5.157, Val loss 5.166\n",
            "Ep 2 (Iter 001915): Train loss 3.964, Val loss 5.163\n",
            "Ep 2 (Iter 001920): Train loss 4.321, Val loss 5.187\n",
            "Ep 2 (Iter 001925): Train loss 4.573, Val loss 5.264\n",
            "Ep 2 (Iter 001930): Train loss 4.504, Val loss 5.227\n",
            "Ep 2 (Iter 001935): Train loss 4.341, Val loss 5.174\n",
            "Ep 2 (Iter 001940): Train loss 4.205, Val loss 5.214\n",
            "Ep 2 (Iter 001945): Train loss 4.215, Val loss 5.208\n",
            "Ep 2 (Iter 001950): Train loss 4.501, Val loss 5.169\n",
            "Ep 2 (Iter 001955): Train loss 4.546, Val loss 5.172\n",
            "Ep 2 (Iter 001960): Train loss 5.017, Val loss 5.216\n",
            "Ep 2 (Iter 001965): Train loss 5.323, Val loss 5.225\n",
            "Ep 2 (Iter 001970): Train loss 4.789, Val loss 5.309\n",
            "Ep 2 (Iter 001975): Train loss 4.484, Val loss 5.255\n",
            "Ep 2 (Iter 001980): Train loss 4.308, Val loss 5.219\n",
            "Ep 2 (Iter 001985): Train loss 4.883, Val loss 5.195\n",
            "Ep 2 (Iter 001990): Train loss 4.747, Val loss 5.155\n",
            "Ep 2 (Iter 001995): Train loss 4.709, Val loss 5.172\n",
            "Ep 2 (Iter 002000): Train loss 4.681, Val loss 5.185\n",
            "Ep 2 (Iter 002005): Train loss 4.579, Val loss 5.168\n",
            "Ep 2 (Iter 002010): Train loss 4.063, Val loss 5.162\n",
            "Ep 2 (Iter 002015): Train loss 4.839, Val loss 5.189\n",
            "Ep 2 (Iter 002020): Train loss 4.209, Val loss 5.177\n",
            "Ep 2 (Iter 002025): Train loss 5.207, Val loss 5.198\n",
            "Ep 2 (Iter 002030): Train loss 4.780, Val loss 5.188\n",
            "Ep 2 (Iter 002035): Train loss 3.927, Val loss 5.194\n",
            "Ep 2 (Iter 002040): Train loss 4.824, Val loss 5.198\n",
            "Ep 2 (Iter 002045): Train loss 4.715, Val loss 5.212\n",
            "Ep 2 (Iter 002050): Train loss 4.161, Val loss 5.266\n",
            "Ep 2 (Iter 002055): Train loss 4.770, Val loss 5.214\n",
            "Ep 2 (Iter 002060): Train loss 4.463, Val loss 5.199\n",
            "Ep 2 (Iter 002065): Train loss 4.349, Val loss 5.211\n",
            "Ep 2 (Iter 002070): Train loss 4.166, Val loss 5.210\n",
            "Ep 2 (Iter 002075): Train loss 4.368, Val loss 5.178\n",
            "Ep 2 (Iter 002080): Train loss 4.722, Val loss 5.161\n",
            "Ep 2 (Iter 002085): Train loss 4.725, Val loss 5.151\n",
            "Ep 2 (Iter 002090): Train loss 4.853, Val loss 5.186\n",
            "Ep 2 (Iter 002095): Train loss 4.575, Val loss 5.183\n",
            "Ep 2 (Iter 002100): Train loss 4.878, Val loss 5.156\n",
            "Ep 2 (Iter 002105): Train loss 4.320, Val loss 5.155\n",
            "Ep 2 (Iter 002110): Train loss 3.790, Val loss 5.165\n",
            "Ep 2 (Iter 002115): Train loss 4.024, Val loss 5.152\n",
            "Ep 2 (Iter 002120): Train loss 5.151, Val loss 5.224\n",
            "Ep 2 (Iter 002125): Train loss 5.030, Val loss 5.187\n",
            "Ep 2 (Iter 002130): Train loss 3.939, Val loss 5.177\n",
            "Ep 2 (Iter 002135): Train loss 4.431, Val loss 5.211\n",
            "Ep 2 (Iter 002140): Train loss 4.097, Val loss 5.209\n",
            "Ep 2 (Iter 002145): Train loss 4.877, Val loss 5.191\n",
            "Ep 2 (Iter 002150): Train loss 4.888, Val loss 5.165\n",
            "Ep 2 (Iter 002155): Train loss 4.768, Val loss 5.175\n",
            "Ep 2 (Iter 002160): Train loss 4.515, Val loss 5.238\n",
            "Ep 2 (Iter 002165): Train loss 4.230, Val loss 5.193\n",
            "Ep 2 (Iter 002170): Train loss 4.655, Val loss 5.132\n",
            "Ep 2 (Iter 002175): Train loss 4.402, Val loss 5.155\n",
            "Ep 2 (Iter 002180): Train loss 4.995, Val loss 5.181\n",
            "Ep 2 (Iter 002185): Train loss 4.287, Val loss 5.175\n",
            "Ep 2 (Iter 002190): Train loss 5.070, Val loss 5.169\n",
            "Ep 2 (Iter 002195): Train loss 4.256, Val loss 5.234\n",
            "Ep 2 (Iter 002200): Train loss 4.476, Val loss 5.206\n",
            "Ep 2 (Iter 002205): Train loss 4.598, Val loss 5.186\n",
            "Ep 2 (Iter 002210): Train loss 4.264, Val loss 5.175\n",
            "Ep 2 (Iter 002215): Train loss 4.230, Val loss 5.160\n",
            "Ep 2 (Iter 002220): Train loss 4.954, Val loss 5.221\n",
            "Ep 2 (Iter 002225): Train loss 4.820, Val loss 5.214\n",
            "Ep 2 (Iter 002230): Train loss 4.233, Val loss 5.176\n",
            "Ep 2 (Iter 002235): Train loss 4.104, Val loss 5.194\n",
            "Ep 2 (Iter 002240): Train loss 4.924, Val loss 5.214\n",
            "Ep 2 (Iter 002245): Train loss 3.868, Val loss 5.152\n",
            "Ep 2 (Iter 002250): Train loss 4.559, Val loss 5.160\n",
            "Ep 2 (Iter 002255): Train loss 4.253, Val loss 5.123\n",
            "Ep 2 (Iter 002260): Train loss 4.898, Val loss 5.132\n",
            "Ep 2 (Iter 002265): Train loss 4.857, Val loss 5.141\n",
            "Ep 2 (Iter 002270): Train loss 5.089, Val loss 5.185\n",
            "Ep 2 (Iter 002275): Train loss 4.194, Val loss 5.143\n",
            "Ep 2 (Iter 002280): Train loss 5.066, Val loss 5.174\n",
            "Ep 2 (Iter 002285): Train loss 4.394, Val loss 5.214\n",
            "Ep 2 (Iter 002290): Train loss 4.012, Val loss 5.214\n",
            "Ep 2 (Iter 002295): Train loss 4.049, Val loss 5.202\n",
            "Ep 2 (Iter 002300): Train loss 3.949, Val loss 5.200\n",
            "Ep 2 (Iter 002305): Train loss 5.256, Val loss 5.250\n",
            "Ep 2 (Iter 002310): Train loss 4.612, Val loss 5.223\n",
            "Ep 2 (Iter 002315): Train loss 4.280, Val loss 5.157\n",
            "Ep 2 (Iter 002320): Train loss 5.331, Val loss 5.189\n",
            "Ep 2 (Iter 002325): Train loss 4.438, Val loss 5.165\n",
            "Ep 2 (Iter 002330): Train loss 4.556, Val loss 5.109\n",
            "Ep 2 (Iter 002335): Train loss 4.444, Val loss 5.121\n",
            "Ep 2 (Iter 002340): Train loss 4.252, Val loss 5.147\n",
            "Ep 2 (Iter 002345): Train loss 4.652, Val loss 5.130\n",
            "Ep 2 (Iter 002350): Train loss 5.142, Val loss 5.104\n",
            "Ep 2 (Iter 002355): Train loss 4.305, Val loss 5.116\n",
            "Ep 2 (Iter 002360): Train loss 4.516, Val loss 5.171\n",
            "Ep 2 (Iter 002365): Train loss 4.617, Val loss 5.201\n",
            "Ep 2 (Iter 002370): Train loss 4.475, Val loss 5.206\n",
            "Ep 2 (Iter 002375): Train loss 4.392, Val loss 5.169\n",
            "Ep 2 (Iter 002380): Train loss 3.986, Val loss 5.205\n",
            "Ep 2 (Iter 002385): Train loss 4.500, Val loss 5.185\n",
            "Ep 2 (Iter 002390): Train loss 4.485, Val loss 5.196\n",
            "Ep 2 (Iter 002395): Train loss 4.882, Val loss 5.187\n",
            "Ep 2 (Iter 002400): Train loss 4.747, Val loss 5.202\n",
            "Ep 2 (Iter 002405): Train loss 4.210, Val loss 5.176\n",
            "Ep 2 (Iter 002410): Train loss 4.344, Val loss 5.116\n",
            "Ep 2 (Iter 002415): Train loss 4.034, Val loss 5.165\n",
            "Ep 2 (Iter 002420): Train loss 4.573, Val loss 5.179\n",
            "Ep 2 (Iter 002425): Train loss 4.107, Val loss 5.206\n",
            "Ep 2 (Iter 002430): Train loss 3.985, Val loss 5.257\n",
            "Ep 2 (Iter 002435): Train loss 4.320, Val loss 5.186\n",
            "Ep 2 (Iter 002440): Train loss 4.111, Val loss 5.148\n",
            "Ep 2 (Iter 002445): Train loss 4.558, Val loss 5.179\n",
            "Ep 2 (Iter 002450): Train loss 4.408, Val loss 5.169\n",
            "Ep 2 (Iter 002455): Train loss 4.533, Val loss 5.171\n",
            "Ep 2 (Iter 002460): Train loss 4.725, Val loss 5.181\n",
            "Ep 2 (Iter 002465): Train loss 4.236, Val loss 5.149\n",
            "Ep 2 (Iter 002470): Train loss 4.798, Val loss 5.107\n",
            "Ep 2 (Iter 002475): Train loss 4.475, Val loss 5.100\n",
            "Ep 2 (Iter 002480): Train loss 4.816, Val loss 5.150\n",
            "Ep 2 (Iter 002485): Train loss 4.368, Val loss 5.117\n",
            "Ep 2 (Iter 002490): Train loss 4.288, Val loss 5.144\n",
            "Ep 2 (Iter 002495): Train loss 4.366, Val loss 5.050\n",
            "Ep 2 (Iter 002500): Train loss 4.555, Val loss 5.102\n",
            "Ep 2 (Iter 002505): Train loss 4.673, Val loss 5.095\n",
            "Ep 2 (Iter 002510): Train loss 4.441, Val loss 5.114\n",
            "Ep 2 (Iter 002515): Train loss 4.991, Val loss 5.139\n",
            "Ep 2 (Iter 002520): Train loss 4.533, Val loss 5.163\n",
            "Ep 2 (Iter 002525): Train loss 5.120, Val loss 5.145\n",
            "Ep 2 (Iter 002530): Train loss 4.595, Val loss 5.202\n",
            "Ep 2 (Iter 002535): Train loss 5.080, Val loss 5.177\n",
            "Ep 2 (Iter 002540): Train loss 4.270, Val loss 5.128\n",
            "Ep 2 (Iter 002545): Train loss 4.014, Val loss 5.099\n",
            "Ep 2 (Iter 002550): Train loss 4.947, Val loss 5.121\n",
            "Ep 2 (Iter 002555): Train loss 4.843, Val loss 5.118\n",
            "Ep 2 (Iter 002560): Train loss 4.121, Val loss 5.171\n",
            "Ep 2 (Iter 002565): Train loss 4.272, Val loss 5.178\n",
            "Ep 2 (Iter 002570): Train loss 4.789, Val loss 5.237\n",
            "Ep 2 (Iter 002575): Train loss 3.840, Val loss 5.171\n",
            "Ep 2 (Iter 002580): Train loss 4.452, Val loss 5.220\n",
            "Ep 2 (Iter 002585): Train loss 4.765, Val loss 5.308\n",
            "Ep 2 (Iter 002590): Train loss 4.155, Val loss 5.203\n",
            "Ep 2 (Iter 002595): Train loss 5.033, Val loss 5.255\n",
            "Ep 2 (Iter 002600): Train loss 4.742, Val loss 5.166\n",
            "Ep 2 (Iter 002605): Train loss 4.812, Val loss 5.224\n",
            "Ep 2 (Iter 002610): Train loss 4.625, Val loss 5.208\n",
            "Ep 2 (Iter 002615): Train loss 5.094, Val loss 5.205\n",
            "Ep 2 (Iter 002620): Train loss 4.156, Val loss 5.203\n",
            "Ep 2 (Iter 002625): Train loss 4.575, Val loss 5.176\n",
            "Ep 2 (Iter 002630): Train loss 4.211, Val loss 5.164\n",
            "Ep 2 (Iter 002635): Train loss 5.020, Val loss 5.172\n",
            "Ep 2 (Iter 002640): Train loss 4.833, Val loss 5.206\n",
            "Ep 2 (Iter 002645): Train loss 4.867, Val loss 5.239\n",
            "Ep 2 (Iter 002650): Train loss 4.821, Val loss 5.203\n",
            "Ep 2 (Iter 002655): Train loss 4.847, Val loss 5.235\n",
            "Ep 2 (Iter 002660): Train loss 4.758, Val loss 5.158\n",
            "Ep 2 (Iter 002665): Train loss 3.692, Val loss 5.185\n",
            "Ep 2 (Iter 002670): Train loss 4.834, Val loss 5.205\n",
            "Ep 2 (Iter 002675): Train loss 4.060, Val loss 5.246\n",
            "Ep 2 (Iter 002680): Train loss 3.914, Val loss 5.184\n",
            "Ep 2 (Iter 002685): Train loss 4.271, Val loss 5.186\n",
            "Ep 2 (Iter 002690): Train loss 4.630, Val loss 5.220\n",
            "Ep 2 (Iter 002695): Train loss 4.424, Val loss 5.235\n",
            "Ep 2 (Iter 002700): Train loss 4.189, Val loss 5.246\n",
            "Ep 2 (Iter 002705): Train loss 4.983, Val loss 5.201\n",
            "Ep 2 (Iter 002710): Train loss 4.681, Val loss 5.202\n",
            "Ep 2 (Iter 002715): Train loss 4.684, Val loss 5.213\n",
            "Ep 2 (Iter 002720): Train loss 4.202, Val loss 5.216\n",
            "Ep 2 (Iter 002725): Train loss 4.854, Val loss 5.254\n",
            "Ep 2 (Iter 002730): Train loss 4.026, Val loss 5.190\n",
            "Ep 2 (Iter 002735): Train loss 4.437, Val loss 5.214\n",
            "Ep 2 (Iter 002740): Train loss 4.820, Val loss 5.241\n",
            "Ep 2 (Iter 002745): Train loss 4.483, Val loss 5.236\n",
            "Ep 2 (Iter 002750): Train loss 4.246, Val loss 5.244\n",
            "Ep 2 (Iter 002755): Train loss 4.473, Val loss 5.211\n",
            "Ep 2 (Iter 002760): Train loss 4.727, Val loss 5.233\n",
            "Ep 2 (Iter 002765): Train loss 4.199, Val loss 5.222\n",
            "Ep 2 (Iter 002770): Train loss 3.779, Val loss 5.253\n",
            "Ep 2 (Iter 002775): Train loss 4.810, Val loss 5.183\n",
            "Ep 2 (Iter 002780): Train loss 4.893, Val loss 5.184\n",
            "Ep 2 (Iter 002785): Train loss 4.326, Val loss 5.207\n",
            "Ep 2 (Iter 002790): Train loss 4.280, Val loss 5.229\n",
            "Ep 2 (Iter 002795): Train loss 5.064, Val loss 5.148\n",
            "Ep 2 (Iter 002800): Train loss 4.198, Val loss 5.175\n",
            "Ep 2 (Iter 002805): Train loss 4.017, Val loss 5.163\n",
            "Ep 2 (Iter 002810): Train loss 4.071, Val loss 5.203\n",
            "Ep 2 (Iter 002815): Train loss 4.368, Val loss 5.189\n",
            "Ep 2 (Iter 002820): Train loss 4.669, Val loss 5.211\n",
            "Ep 2 (Iter 002825): Train loss 3.798, Val loss 5.131\n",
            "Ep 2 (Iter 002830): Train loss 4.946, Val loss 5.114\n",
            "Ep 2 (Iter 002835): Train loss 4.393, Val loss 5.106\n",
            "Ep 2 (Iter 002840): Train loss 4.829, Val loss 5.138\n",
            "Ep 2 (Iter 002845): Train loss 5.157, Val loss 5.164\n",
            "Ep 2 (Iter 002850): Train loss 5.393, Val loss 5.251\n",
            "Ep 2 (Iter 002855): Train loss 4.269, Val loss 5.164\n",
            "Ep 2 (Iter 002860): Train loss 4.524, Val loss 5.124\n",
            "Ep 2 (Iter 002865): Train loss 4.915, Val loss 5.162\n",
            "Ep 2 (Iter 002870): Train loss 4.904, Val loss 5.177\n",
            "Ep 2 (Iter 002875): Train loss 4.288, Val loss 5.180\n",
            "Ep 2 (Iter 002880): Train loss 4.392, Val loss 5.175\n",
            "Ep 2 (Iter 002885): Train loss 4.700, Val loss 5.210\n",
            "Ep 2 (Iter 002890): Train loss 4.008, Val loss 5.165\n",
            "Ep 2 (Iter 002895): Train loss 4.935, Val loss 5.150\n",
            "Ep 2 (Iter 002900): Train loss 4.736, Val loss 5.169\n",
            "Ep 2 (Iter 002905): Train loss 4.495, Val loss 5.132\n",
            "Ep 2 (Iter 002910): Train loss 4.272, Val loss 5.130\n",
            "Ep 2 (Iter 002915): Train loss 4.125, Val loss 5.178\n",
            "Ep 2 (Iter 002920): Train loss 4.582, Val loss 5.160\n",
            "Ep 2 (Iter 002925): Train loss 5.023, Val loss 5.154\n",
            "Ep 2 (Iter 002930): Train loss 4.839, Val loss 5.170\n",
            "Ep 2 (Iter 002935): Train loss 4.300, Val loss 5.126\n",
            "Ep 2 (Iter 002940): Train loss 4.746, Val loss 5.111\n",
            "Ep 2 (Iter 002945): Train loss 4.898, Val loss 5.135\n",
            "Ep 2 (Iter 002950): Train loss 4.752, Val loss 5.144\n",
            "Ep 2 (Iter 002955): Train loss 3.448, Val loss 5.159\n",
            "Ep 2 (Iter 002960): Train loss 5.318, Val loss 5.214\n",
            "Ep 2 (Iter 002965): Train loss 4.941, Val loss 5.162\n",
            "Ep 2 (Iter 002970): Train loss 4.452, Val loss 5.179\n",
            "Ep 2 (Iter 002975): Train loss 3.977, Val loss 5.164\n",
            "Ep 2 (Iter 002980): Train loss 4.800, Val loss 5.150\n",
            "Ep 2 (Iter 002985): Train loss 4.732, Val loss 5.131\n",
            "Ep 2 (Iter 002990): Train loss 4.379, Val loss 5.178\n",
            "Ep 2 (Iter 002995): Train loss 4.180, Val loss 5.180\n",
            "Ep 2 (Iter 003000): Train loss 4.695, Val loss 5.204\n",
            "Ep 2 (Iter 003005): Train loss 5.060, Val loss 5.174\n",
            "Ep 2 (Iter 003010): Train loss 4.512, Val loss 5.178\n",
            "Ep 2 (Iter 003015): Train loss 4.928, Val loss 5.250\n",
            "Ep 2 (Iter 003020): Train loss 4.640, Val loss 5.228\n",
            "Ep 2 (Iter 003025): Train loss 4.346, Val loss 5.190\n",
            "In the midst of winter, and the French, and the  of the French, and the  of the crowd, and the French, and the  of the French, and the French the  of the  of the  \n",
            "Ep 3 (Iter 003030): Train loss 4.518, Val loss 5.253\n",
            "Ep 3 (Iter 003035): Train loss 3.627, Val loss 5.193\n",
            "Ep 3 (Iter 003040): Train loss 4.410, Val loss 5.201\n",
            "Ep 3 (Iter 003045): Train loss 4.736, Val loss 5.188\n",
            "Ep 3 (Iter 003050): Train loss 4.528, Val loss 5.252\n",
            "Ep 3 (Iter 003055): Train loss 4.330, Val loss 5.289\n",
            "Ep 3 (Iter 003060): Train loss 4.345, Val loss 5.282\n",
            "Ep 3 (Iter 003065): Train loss 4.325, Val loss 5.222\n",
            "Ep 3 (Iter 003070): Train loss 4.696, Val loss 5.253\n",
            "Ep 3 (Iter 003075): Train loss 4.484, Val loss 5.219\n",
            "Ep 3 (Iter 003080): Train loss 4.640, Val loss 5.184\n",
            "Ep 3 (Iter 003085): Train loss 5.054, Val loss 5.178\n",
            "Ep 3 (Iter 003090): Train loss 3.887, Val loss 5.193\n",
            "Ep 3 (Iter 003095): Train loss 4.804, Val loss 5.202\n",
            "Ep 3 (Iter 003100): Train loss 4.303, Val loss 5.173\n",
            "Ep 3 (Iter 003105): Train loss 4.372, Val loss 5.134\n",
            "Ep 3 (Iter 003110): Train loss 4.532, Val loss 5.165\n",
            "Ep 3 (Iter 003115): Train loss 4.410, Val loss 5.137\n",
            "Ep 3 (Iter 003120): Train loss 4.629, Val loss 5.167\n",
            "Ep 3 (Iter 003125): Train loss 4.968, Val loss 5.144\n",
            "Ep 3 (Iter 003130): Train loss 3.875, Val loss 5.145\n",
            "Ep 3 (Iter 003135): Train loss 4.456, Val loss 5.166\n",
            "Ep 3 (Iter 003140): Train loss 3.539, Val loss 5.155\n",
            "Ep 3 (Iter 003145): Train loss 5.104, Val loss 5.215\n",
            "Ep 3 (Iter 003150): Train loss 3.728, Val loss 5.190\n",
            "Ep 3 (Iter 003155): Train loss 3.904, Val loss 5.148\n",
            "Ep 3 (Iter 003160): Train loss 4.717, Val loss 5.165\n",
            "Ep 3 (Iter 003165): Train loss 4.183, Val loss 5.202\n",
            "Ep 3 (Iter 003170): Train loss 4.600, Val loss 5.186\n",
            "Ep 3 (Iter 003175): Train loss 4.507, Val loss 5.173\n",
            "Ep 3 (Iter 003180): Train loss 5.290, Val loss 5.253\n",
            "Ep 3 (Iter 003185): Train loss 4.772, Val loss 5.253\n",
            "Ep 3 (Iter 003190): Train loss 4.457, Val loss 5.138\n",
            "Ep 3 (Iter 003195): Train loss 5.247, Val loss 5.169\n",
            "Ep 3 (Iter 003200): Train loss 4.814, Val loss 5.218\n",
            "Ep 3 (Iter 003205): Train loss 4.311, Val loss 5.243\n",
            "Ep 3 (Iter 003210): Train loss 4.690, Val loss 5.267\n",
            "Ep 3 (Iter 003215): Train loss 4.544, Val loss 5.229\n",
            "Ep 3 (Iter 003220): Train loss 4.817, Val loss 5.227\n",
            "Ep 3 (Iter 003225): Train loss 4.514, Val loss 5.187\n",
            "Ep 3 (Iter 003230): Train loss 4.173, Val loss 5.154\n",
            "Ep 3 (Iter 003235): Train loss 5.194, Val loss 5.176\n",
            "Ep 3 (Iter 003240): Train loss 4.897, Val loss 5.190\n",
            "Ep 3 (Iter 003245): Train loss 4.816, Val loss 5.215\n",
            "Ep 3 (Iter 003250): Train loss 3.790, Val loss 5.193\n",
            "Ep 3 (Iter 003255): Train loss 3.912, Val loss 5.198\n",
            "Ep 3 (Iter 003260): Train loss 4.668, Val loss 5.166\n",
            "Ep 3 (Iter 003265): Train loss 4.264, Val loss 5.170\n",
            "Ep 3 (Iter 003270): Train loss 4.219, Val loss 5.208\n",
            "Ep 3 (Iter 003275): Train loss 4.735, Val loss 5.214\n",
            "Ep 3 (Iter 003280): Train loss 4.878, Val loss 5.175\n",
            "Ep 3 (Iter 003285): Train loss 4.568, Val loss 5.223\n",
            "Ep 3 (Iter 003290): Train loss 4.105, Val loss 5.241\n",
            "Ep 3 (Iter 003295): Train loss 4.329, Val loss 5.190\n",
            "Ep 3 (Iter 003300): Train loss 4.175, Val loss 5.188\n",
            "Ep 3 (Iter 003305): Train loss 4.634, Val loss 5.156\n",
            "Ep 3 (Iter 003310): Train loss 5.147, Val loss 5.194\n",
            "Ep 3 (Iter 003315): Train loss 4.260, Val loss 5.208\n",
            "Ep 3 (Iter 003320): Train loss 4.538, Val loss 5.180\n",
            "Ep 3 (Iter 003325): Train loss 3.629, Val loss 5.172\n",
            "Ep 3 (Iter 003330): Train loss 4.459, Val loss 5.195\n",
            "Ep 3 (Iter 003335): Train loss 4.801, Val loss 5.179\n",
            "Ep 3 (Iter 003340): Train loss 5.297, Val loss 5.160\n",
            "Ep 3 (Iter 003345): Train loss 4.674, Val loss 5.095\n",
            "Ep 3 (Iter 003350): Train loss 4.349, Val loss 5.103\n",
            "Ep 3 (Iter 003355): Train loss 4.493, Val loss 5.081\n",
            "Ep 3 (Iter 003360): Train loss 4.835, Val loss 5.149\n",
            "Ep 3 (Iter 003365): Train loss 3.960, Val loss 5.143\n",
            "Ep 3 (Iter 003370): Train loss 3.367, Val loss 5.153\n",
            "Ep 3 (Iter 003375): Train loss 4.411, Val loss 5.145\n",
            "Ep 3 (Iter 003380): Train loss 4.192, Val loss 5.129\n",
            "Ep 3 (Iter 003385): Train loss 4.163, Val loss 5.112\n",
            "Ep 3 (Iter 003390): Train loss 4.155, Val loss 5.070\n",
            "Ep 3 (Iter 003395): Train loss 4.211, Val loss 5.092\n",
            "Ep 3 (Iter 003400): Train loss 4.166, Val loss 5.088\n",
            "Ep 3 (Iter 003405): Train loss 4.601, Val loss 5.112\n",
            "Ep 3 (Iter 003410): Train loss 4.564, Val loss 5.156\n",
            "Ep 3 (Iter 003415): Train loss 4.275, Val loss 5.145\n",
            "Ep 3 (Iter 003420): Train loss 4.996, Val loss 5.160\n",
            "Ep 3 (Iter 003425): Train loss 4.590, Val loss 5.112\n",
            "Ep 3 (Iter 003430): Train loss 4.716, Val loss 5.164\n",
            "Ep 3 (Iter 003435): Train loss 4.485, Val loss 5.100\n",
            "Ep 3 (Iter 003440): Train loss 3.905, Val loss 5.103\n",
            "Ep 3 (Iter 003445): Train loss 4.493, Val loss 5.142\n",
            "Ep 3 (Iter 003450): Train loss 4.026, Val loss 5.169\n",
            "Ep 3 (Iter 003455): Train loss 4.138, Val loss 5.165\n",
            "Ep 3 (Iter 003460): Train loss 4.215, Val loss 5.145\n",
            "Ep 3 (Iter 003465): Train loss 4.779, Val loss 5.179\n",
            "Ep 3 (Iter 003470): Train loss 4.006, Val loss 5.200\n",
            "Ep 3 (Iter 003475): Train loss 4.829, Val loss 5.169\n",
            "Ep 3 (Iter 003480): Train loss 4.816, Val loss 5.134\n",
            "Ep 3 (Iter 003485): Train loss 4.554, Val loss 5.205\n",
            "Ep 3 (Iter 003490): Train loss 4.116, Val loss 5.151\n",
            "Ep 3 (Iter 003495): Train loss 4.633, Val loss 5.178\n",
            "Ep 3 (Iter 003500): Train loss 4.432, Val loss 5.220\n",
            "Ep 3 (Iter 003505): Train loss 4.925, Val loss 5.232\n",
            "Ep 3 (Iter 003510): Train loss 4.190, Val loss 5.183\n",
            "Ep 3 (Iter 003515): Train loss 4.166, Val loss 5.160\n",
            "Ep 3 (Iter 003520): Train loss 4.788, Val loss 5.159\n",
            "Ep 3 (Iter 003525): Train loss 4.371, Val loss 5.187\n",
            "Ep 3 (Iter 003530): Train loss 4.086, Val loss 5.219\n",
            "Ep 3 (Iter 003535): Train loss 4.606, Val loss 5.189\n",
            "Ep 3 (Iter 003540): Train loss 4.886, Val loss 5.204\n",
            "Ep 3 (Iter 003545): Train loss 4.435, Val loss 5.230\n",
            "Ep 3 (Iter 003550): Train loss 4.109, Val loss 5.218\n",
            "Ep 3 (Iter 003555): Train loss 4.070, Val loss 5.184\n",
            "Ep 3 (Iter 003560): Train loss 4.115, Val loss 5.153\n",
            "Ep 3 (Iter 003565): Train loss 4.603, Val loss 5.168\n",
            "Ep 3 (Iter 003570): Train loss 4.766, Val loss 5.177\n",
            "Ep 3 (Iter 003575): Train loss 4.724, Val loss 5.107\n",
            "Ep 3 (Iter 003580): Train loss 4.577, Val loss 5.138\n",
            "Ep 3 (Iter 003585): Train loss 4.691, Val loss 5.080\n",
            "Ep 3 (Iter 003590): Train loss 4.231, Val loss 5.091\n",
            "Ep 3 (Iter 003595): Train loss 4.225, Val loss 5.139\n",
            "Ep 3 (Iter 003600): Train loss 4.436, Val loss 5.083\n",
            "Ep 3 (Iter 003605): Train loss 3.842, Val loss 5.106\n",
            "Ep 3 (Iter 003610): Train loss 4.631, Val loss 5.090\n",
            "Ep 3 (Iter 003615): Train loss 3.648, Val loss 5.088\n",
            "Ep 3 (Iter 003620): Train loss 4.095, Val loss 5.090\n",
            "Ep 3 (Iter 003625): Train loss 4.776, Val loss 5.106\n",
            "Ep 3 (Iter 003630): Train loss 5.017, Val loss 5.104\n",
            "Ep 3 (Iter 003635): Train loss 4.746, Val loss 5.091\n",
            "Ep 3 (Iter 003640): Train loss 4.071, Val loss 5.054\n",
            "Ep 3 (Iter 003645): Train loss 4.866, Val loss 5.066\n",
            "Ep 3 (Iter 003650): Train loss 4.156, Val loss 5.096\n",
            "Ep 3 (Iter 003655): Train loss 4.613, Val loss 5.081\n",
            "Ep 3 (Iter 003660): Train loss 3.859, Val loss 5.094\n",
            "Ep 3 (Iter 003665): Train loss 4.506, Val loss 5.099\n",
            "Ep 3 (Iter 003670): Train loss 5.058, Val loss 5.114\n",
            "Ep 3 (Iter 003675): Train loss 4.327, Val loss 5.070\n",
            "Ep 3 (Iter 003680): Train loss 4.743, Val loss 5.099\n",
            "Ep 3 (Iter 003685): Train loss 4.522, Val loss 5.112\n",
            "Ep 3 (Iter 003690): Train loss 4.385, Val loss 5.103\n",
            "Ep 3 (Iter 003695): Train loss 4.800, Val loss 5.147\n",
            "Ep 3 (Iter 003700): Train loss 4.366, Val loss 5.159\n",
            "Ep 3 (Iter 003705): Train loss 5.044, Val loss 5.134\n",
            "Ep 3 (Iter 003710): Train loss 4.537, Val loss 5.133\n",
            "Ep 3 (Iter 003715): Train loss 4.317, Val loss 5.168\n",
            "Ep 3 (Iter 003720): Train loss 4.554, Val loss 5.167\n",
            "Ep 3 (Iter 003725): Train loss 4.410, Val loss 5.149\n",
            "Ep 3 (Iter 003730): Train loss 4.379, Val loss 5.094\n",
            "Ep 3 (Iter 003735): Train loss 4.162, Val loss 5.141\n",
            "Ep 3 (Iter 003740): Train loss 4.449, Val loss 5.102\n",
            "Ep 3 (Iter 003745): Train loss 4.165, Val loss 5.072\n",
            "Ep 3 (Iter 003750): Train loss 4.842, Val loss 5.129\n",
            "Ep 3 (Iter 003755): Train loss 4.746, Val loss 5.103\n",
            "Ep 3 (Iter 003760): Train loss 4.566, Val loss 5.093\n",
            "Ep 3 (Iter 003765): Train loss 5.035, Val loss 5.034\n",
            "Ep 3 (Iter 003770): Train loss 3.944, Val loss 5.045\n",
            "Ep 3 (Iter 003775): Train loss 4.798, Val loss 5.122\n",
            "Ep 3 (Iter 003780): Train loss 4.716, Val loss 5.169\n",
            "Ep 3 (Iter 003785): Train loss 4.396, Val loss 5.160\n",
            "Ep 3 (Iter 003790): Train loss 4.610, Val loss 5.148\n",
            "Ep 3 (Iter 003795): Train loss 4.439, Val loss 5.150\n",
            "Ep 3 (Iter 003800): Train loss 4.605, Val loss 5.178\n",
            "Ep 3 (Iter 003805): Train loss 4.187, Val loss 5.153\n",
            "Ep 3 (Iter 003810): Train loss 3.696, Val loss 5.127\n",
            "Ep 3 (Iter 003815): Train loss 4.880, Val loss 5.159\n",
            "Ep 3 (Iter 003820): Train loss 4.500, Val loss 5.137\n",
            "Ep 3 (Iter 003825): Train loss 4.321, Val loss 5.129\n",
            "Ep 3 (Iter 003830): Train loss 4.542, Val loss 5.141\n",
            "Ep 3 (Iter 003835): Train loss 4.402, Val loss 5.116\n",
            "Ep 3 (Iter 003840): Train loss 5.015, Val loss 5.109\n",
            "Ep 3 (Iter 003845): Train loss 4.439, Val loss 5.130\n",
            "Ep 3 (Iter 003850): Train loss 4.786, Val loss 5.165\n",
            "Ep 3 (Iter 003855): Train loss 4.260, Val loss 5.205\n",
            "Ep 3 (Iter 003860): Train loss 5.095, Val loss 5.157\n",
            "Ep 3 (Iter 003865): Train loss 3.999, Val loss 5.136\n",
            "Ep 3 (Iter 003870): Train loss 4.613, Val loss 5.126\n",
            "Ep 3 (Iter 003875): Train loss 4.360, Val loss 5.159\n",
            "Ep 3 (Iter 003880): Train loss 4.802, Val loss 5.119\n",
            "Ep 3 (Iter 003885): Train loss 4.607, Val loss 5.121\n",
            "Ep 3 (Iter 003890): Train loss 4.277, Val loss 5.136\n",
            "Ep 3 (Iter 003895): Train loss 4.482, Val loss 5.121\n",
            "Ep 3 (Iter 003900): Train loss 4.474, Val loss 5.137\n",
            "Ep 3 (Iter 003905): Train loss 4.003, Val loss 5.126\n",
            "Ep 3 (Iter 003910): Train loss 4.599, Val loss 5.131\n",
            "Ep 3 (Iter 003915): Train loss 5.065, Val loss 5.114\n",
            "Ep 3 (Iter 003920): Train loss 4.325, Val loss 5.049\n",
            "Ep 3 (Iter 003925): Train loss 4.637, Val loss 5.087\n",
            "Ep 3 (Iter 003930): Train loss 4.162, Val loss 5.114\n",
            "Ep 3 (Iter 003935): Train loss 4.135, Val loss 5.085\n",
            "Ep 3 (Iter 003940): Train loss 3.959, Val loss 5.118\n",
            "Ep 3 (Iter 003945): Train loss 3.363, Val loss 5.141\n",
            "Ep 3 (Iter 003950): Train loss 4.429, Val loss 5.139\n",
            "Ep 3 (Iter 003955): Train loss 4.676, Val loss 5.127\n",
            "Ep 3 (Iter 003960): Train loss 4.345, Val loss 5.108\n",
            "Ep 3 (Iter 003965): Train loss 4.004, Val loss 5.086\n",
            "Ep 3 (Iter 003970): Train loss 4.338, Val loss 5.114\n",
            "Ep 3 (Iter 003975): Train loss 3.975, Val loss 5.132\n",
            "Ep 3 (Iter 003980): Train loss 4.542, Val loss 5.154\n",
            "Ep 3 (Iter 003985): Train loss 3.738, Val loss 5.131\n",
            "Ep 3 (Iter 003990): Train loss 4.265, Val loss 5.163\n",
            "Ep 3 (Iter 003995): Train loss 3.861, Val loss 5.160\n",
            "Ep 3 (Iter 004000): Train loss 5.020, Val loss 5.157\n",
            "Ep 3 (Iter 004005): Train loss 3.925, Val loss 5.152\n",
            "Ep 3 (Iter 004010): Train loss 4.897, Val loss 5.156\n",
            "Ep 3 (Iter 004015): Train loss 4.728, Val loss 5.125\n",
            "Ep 3 (Iter 004020): Train loss 3.987, Val loss 5.083\n",
            "Ep 3 (Iter 004025): Train loss 3.935, Val loss 5.076\n",
            "Ep 3 (Iter 004030): Train loss 4.591, Val loss 5.105\n",
            "Ep 3 (Iter 004035): Train loss 4.580, Val loss 5.113\n",
            "Ep 3 (Iter 004040): Train loss 4.462, Val loss 5.095\n",
            "Ep 3 (Iter 004045): Train loss 4.273, Val loss 5.082\n",
            "Ep 3 (Iter 004050): Train loss 3.858, Val loss 5.105\n",
            "Ep 3 (Iter 004055): Train loss 4.168, Val loss 5.043\n",
            "Ep 3 (Iter 004060): Train loss 3.800, Val loss 5.038\n",
            "Ep 3 (Iter 004065): Train loss 4.120, Val loss 5.068\n",
            "Ep 3 (Iter 004070): Train loss 4.558, Val loss 5.090\n",
            "Ep 3 (Iter 004075): Train loss 4.393, Val loss 5.098\n",
            "Ep 3 (Iter 004080): Train loss 4.051, Val loss 5.062\n",
            "Ep 3 (Iter 004085): Train loss 3.843, Val loss 5.066\n",
            "Ep 3 (Iter 004090): Train loss 4.646, Val loss 5.062\n",
            "Ep 3 (Iter 004095): Train loss 4.167, Val loss 5.069\n",
            "Ep 3 (Iter 004100): Train loss 4.219, Val loss 5.108\n",
            "Ep 3 (Iter 004105): Train loss 4.319, Val loss 5.091\n",
            "Ep 3 (Iter 004110): Train loss 4.161, Val loss 5.094\n",
            "Ep 3 (Iter 004115): Train loss 4.479, Val loss 5.125\n",
            "Ep 3 (Iter 004120): Train loss 4.390, Val loss 5.108\n",
            "Ep 3 (Iter 004125): Train loss 3.629, Val loss 5.068\n",
            "Ep 3 (Iter 004130): Train loss 3.961, Val loss 5.065\n",
            "Ep 3 (Iter 004135): Train loss 4.369, Val loss 5.067\n",
            "Ep 3 (Iter 004140): Train loss 4.676, Val loss 5.081\n",
            "Ep 3 (Iter 004145): Train loss 5.052, Val loss 5.054\n",
            "Ep 3 (Iter 004150): Train loss 4.185, Val loss 5.087\n",
            "Ep 3 (Iter 004155): Train loss 4.548, Val loss 5.145\n",
            "Ep 3 (Iter 004160): Train loss 4.363, Val loss 5.122\n",
            "Ep 3 (Iter 004165): Train loss 4.111, Val loss 5.064\n",
            "Ep 3 (Iter 004170): Train loss 4.417, Val loss 5.148\n",
            "Ep 3 (Iter 004175): Train loss 4.443, Val loss 5.126\n",
            "Ep 3 (Iter 004180): Train loss 4.607, Val loss 5.178\n",
            "Ep 3 (Iter 004185): Train loss 4.306, Val loss 5.180\n",
            "Ep 3 (Iter 004190): Train loss 3.836, Val loss 5.161\n",
            "Ep 3 (Iter 004195): Train loss 4.390, Val loss 5.164\n",
            "Ep 3 (Iter 004200): Train loss 4.241, Val loss 5.146\n",
            "Ep 3 (Iter 004205): Train loss 4.615, Val loss 5.148\n",
            "Ep 3 (Iter 004210): Train loss 3.637, Val loss 5.110\n",
            "Ep 3 (Iter 004215): Train loss 4.741, Val loss 5.115\n",
            "Ep 3 (Iter 004220): Train loss 4.973, Val loss 5.151\n",
            "Ep 3 (Iter 004225): Train loss 4.526, Val loss 5.160\n",
            "Ep 3 (Iter 004230): Train loss 4.288, Val loss 5.112\n",
            "Ep 3 (Iter 004235): Train loss 4.301, Val loss 5.087\n",
            "Ep 3 (Iter 004240): Train loss 4.905, Val loss 5.115\n",
            "Ep 3 (Iter 004245): Train loss 4.575, Val loss 5.123\n",
            "Ep 3 (Iter 004250): Train loss 4.333, Val loss 5.150\n",
            "Ep 3 (Iter 004255): Train loss 4.688, Val loss 5.176\n",
            "Ep 3 (Iter 004260): Train loss 4.807, Val loss 5.177\n",
            "Ep 3 (Iter 004265): Train loss 4.226, Val loss 5.123\n",
            "Ep 3 (Iter 004270): Train loss 4.313, Val loss 5.116\n",
            "Ep 3 (Iter 004275): Train loss 4.150, Val loss 5.130\n",
            "Ep 3 (Iter 004280): Train loss 4.308, Val loss 5.143\n",
            "Ep 3 (Iter 004285): Train loss 4.142, Val loss 5.170\n",
            "Ep 3 (Iter 004290): Train loss 4.377, Val loss 5.164\n",
            "Ep 3 (Iter 004295): Train loss 4.402, Val loss 5.170\n",
            "Ep 3 (Iter 004300): Train loss 4.349, Val loss 5.128\n",
            "Ep 3 (Iter 004305): Train loss 4.185, Val loss 5.159\n",
            "Ep 3 (Iter 004310): Train loss 4.060, Val loss 5.186\n",
            "Ep 3 (Iter 004315): Train loss 4.755, Val loss 5.148\n",
            "Ep 3 (Iter 004320): Train loss 4.509, Val loss 5.122\n",
            "Ep 3 (Iter 004325): Train loss 4.742, Val loss 5.133\n",
            "Ep 3 (Iter 004330): Train loss 4.064, Val loss 5.180\n",
            "Ep 3 (Iter 004335): Train loss 4.389, Val loss 5.193\n",
            "Ep 3 (Iter 004340): Train loss 4.235, Val loss 5.130\n",
            "Ep 3 (Iter 004345): Train loss 4.029, Val loss 5.127\n",
            "Ep 3 (Iter 004350): Train loss 4.025, Val loss 5.108\n",
            "Ep 3 (Iter 004355): Train loss 4.543, Val loss 5.121\n",
            "Ep 3 (Iter 004360): Train loss 4.296, Val loss 5.152\n",
            "Ep 3 (Iter 004365): Train loss 4.588, Val loss 5.135\n",
            "Ep 3 (Iter 004370): Train loss 4.690, Val loss 5.146\n",
            "Ep 3 (Iter 004375): Train loss 4.230, Val loss 5.117\n",
            "Ep 3 (Iter 004380): Train loss 4.552, Val loss 5.113\n",
            "Ep 3 (Iter 004385): Train loss 4.468, Val loss 5.084\n",
            "Ep 3 (Iter 004390): Train loss 3.954, Val loss 5.083\n",
            "Ep 3 (Iter 004395): Train loss 4.378, Val loss 5.062\n",
            "Ep 3 (Iter 004400): Train loss 4.163, Val loss 5.145\n",
            "Ep 3 (Iter 004405): Train loss 4.246, Val loss 5.181\n",
            "Ep 3 (Iter 004410): Train loss 3.849, Val loss 5.204\n",
            "Ep 3 (Iter 004415): Train loss 4.007, Val loss 5.230\n",
            "Ep 3 (Iter 004420): Train loss 4.052, Val loss 5.181\n",
            "Ep 3 (Iter 004425): Train loss 4.256, Val loss 5.157\n",
            "Ep 3 (Iter 004430): Train loss 4.435, Val loss 5.134\n",
            "Ep 3 (Iter 004435): Train loss 4.788, Val loss 5.104\n",
            "Ep 3 (Iter 004440): Train loss 4.103, Val loss 5.104\n",
            "Ep 3 (Iter 004445): Train loss 5.160, Val loss 5.128\n",
            "Ep 3 (Iter 004450): Train loss 4.638, Val loss 5.084\n",
            "Ep 3 (Iter 004455): Train loss 4.051, Val loss 5.083\n",
            "Ep 3 (Iter 004460): Train loss 4.435, Val loss 5.089\n",
            "Ep 3 (Iter 004465): Train loss 4.259, Val loss 5.083\n",
            "Ep 3 (Iter 004470): Train loss 4.366, Val loss 5.050\n",
            "Ep 3 (Iter 004475): Train loss 4.787, Val loss 5.073\n",
            "Ep 3 (Iter 004480): Train loss 4.011, Val loss 5.064\n",
            "Ep 3 (Iter 004485): Train loss 4.942, Val loss 5.084\n",
            "Ep 3 (Iter 004490): Train loss 4.379, Val loss 5.045\n",
            "Ep 3 (Iter 004495): Train loss 4.049, Val loss 5.034\n",
            "Ep 3 (Iter 004500): Train loss 3.906, Val loss 5.056\n",
            "Ep 3 (Iter 004505): Train loss 4.057, Val loss 5.066\n",
            "Ep 3 (Iter 004510): Train loss 4.554, Val loss 5.005\n",
            "Ep 3 (Iter 004515): Train loss 4.417, Val loss 4.986\n",
            "Ep 3 (Iter 004520): Train loss 4.076, Val loss 5.038\n",
            "Ep 3 (Iter 004525): Train loss 4.790, Val loss 5.088\n",
            "Ep 3 (Iter 004530): Train loss 4.168, Val loss 5.077\n",
            "Ep 3 (Iter 004535): Train loss 3.872, Val loss 5.066\n",
            "In the midst of winter, and the Emperor, and the Emperoronsieuring, and the the whole army.  I have been trees, said Prince Andrew, I am glad to be the\n",
            "Ep 4 (Iter 004540): Train loss 4.195, Val loss 5.023\n",
            "Ep 4 (Iter 004545): Train loss 4.821, Val loss 5.020\n",
            "Ep 4 (Iter 004550): Train loss 4.543, Val loss 5.029\n",
            "Ep 4 (Iter 004555): Train loss 3.848, Val loss 5.041\n",
            "Ep 4 (Iter 004560): Train loss 4.168, Val loss 5.064\n",
            "Ep 4 (Iter 004565): Train loss 4.680, Val loss 5.076\n",
            "Ep 4 (Iter 004570): Train loss 4.455, Val loss 5.088\n",
            "Ep 4 (Iter 004575): Train loss 4.655, Val loss 5.079\n",
            "Ep 4 (Iter 004580): Train loss 4.109, Val loss 5.128\n",
            "Ep 4 (Iter 004585): Train loss 4.291, Val loss 5.112\n",
            "Ep 4 (Iter 004590): Train loss 4.462, Val loss 5.070\n",
            "Ep 4 (Iter 004595): Train loss 4.291, Val loss 5.064\n",
            "Ep 4 (Iter 004600): Train loss 4.115, Val loss 5.094\n",
            "Ep 4 (Iter 004605): Train loss 4.165, Val loss 5.096\n",
            "Ep 4 (Iter 004610): Train loss 3.929, Val loss 5.107\n",
            "Ep 4 (Iter 004615): Train loss 5.287, Val loss 5.106\n",
            "Ep 4 (Iter 004620): Train loss 3.819, Val loss 5.078\n",
            "Ep 4 (Iter 004625): Train loss 3.910, Val loss 5.076\n",
            "Ep 4 (Iter 004630): Train loss 4.200, Val loss 5.116\n",
            "Ep 4 (Iter 004635): Train loss 4.181, Val loss 5.113\n",
            "Ep 4 (Iter 004640): Train loss 4.526, Val loss 5.113\n",
            "Ep 4 (Iter 004645): Train loss 4.582, Val loss 5.093\n",
            "Ep 4 (Iter 004650): Train loss 4.564, Val loss 5.040\n",
            "Ep 4 (Iter 004655): Train loss 4.315, Val loss 5.006\n",
            "Ep 4 (Iter 004660): Train loss 4.377, Val loss 5.047\n",
            "Ep 4 (Iter 004665): Train loss 4.253, Val loss 5.079\n",
            "Ep 4 (Iter 004670): Train loss 3.763, Val loss 5.057\n",
            "Ep 4 (Iter 004675): Train loss 4.092, Val loss 5.064\n",
            "Ep 4 (Iter 004680): Train loss 4.057, Val loss 5.073\n",
            "Ep 4 (Iter 004685): Train loss 4.535, Val loss 5.097\n",
            "Ep 4 (Iter 004690): Train loss 4.107, Val loss 5.117\n",
            "Ep 4 (Iter 004695): Train loss 3.771, Val loss 5.127\n",
            "Ep 4 (Iter 004700): Train loss 4.093, Val loss 5.109\n",
            "Ep 4 (Iter 004705): Train loss 4.233, Val loss 5.060\n",
            "Ep 4 (Iter 004710): Train loss 3.570, Val loss 5.060\n",
            "Ep 4 (Iter 004715): Train loss 4.289, Val loss 5.072\n",
            "Ep 4 (Iter 004720): Train loss 4.673, Val loss 5.013\n",
            "Ep 4 (Iter 004725): Train loss 4.242, Val loss 4.993\n",
            "Ep 4 (Iter 004730): Train loss 4.281, Val loss 5.035\n",
            "Ep 4 (Iter 004735): Train loss 4.031, Val loss 5.078\n",
            "Ep 4 (Iter 004740): Train loss 3.769, Val loss 5.086\n",
            "Ep 4 (Iter 004745): Train loss 3.496, Val loss 5.060\n",
            "Ep 4 (Iter 004750): Train loss 4.087, Val loss 5.093\n",
            "Ep 4 (Iter 004755): Train loss 4.171, Val loss 5.044\n",
            "Ep 4 (Iter 004760): Train loss 3.869, Val loss 5.029\n",
            "Ep 4 (Iter 004765): Train loss 4.173, Val loss 5.046\n",
            "Ep 4 (Iter 004770): Train loss 3.989, Val loss 5.054\n",
            "Ep 4 (Iter 004775): Train loss 3.792, Val loss 5.058\n",
            "Ep 4 (Iter 004780): Train loss 3.915, Val loss 5.065\n",
            "Ep 4 (Iter 004785): Train loss 3.836, Val loss 5.066\n",
            "Ep 4 (Iter 004790): Train loss 4.657, Val loss 5.049\n",
            "Ep 4 (Iter 004795): Train loss 4.595, Val loss 5.046\n",
            "Ep 4 (Iter 004800): Train loss 4.465, Val loss 5.036\n",
            "Ep 4 (Iter 004805): Train loss 4.037, Val loss 4.998\n",
            "Ep 4 (Iter 004810): Train loss 3.783, Val loss 5.012\n",
            "Ep 4 (Iter 004815): Train loss 4.150, Val loss 5.030\n",
            "Ep 4 (Iter 004820): Train loss 3.698, Val loss 5.019\n",
            "Ep 4 (Iter 004825): Train loss 4.311, Val loss 5.015\n",
            "Ep 4 (Iter 004830): Train loss 4.103, Val loss 4.997\n",
            "Ep 4 (Iter 004835): Train loss 4.242, Val loss 5.003\n",
            "Ep 4 (Iter 004840): Train loss 3.923, Val loss 5.017\n",
            "Ep 4 (Iter 004845): Train loss 4.444, Val loss 5.024\n",
            "Ep 4 (Iter 004850): Train loss 4.123, Val loss 5.025\n",
            "Ep 4 (Iter 004855): Train loss 4.540, Val loss 5.015\n",
            "Ep 4 (Iter 004860): Train loss 4.006, Val loss 5.024\n",
            "Ep 4 (Iter 004865): Train loss 4.851, Val loss 4.991\n",
            "Ep 4 (Iter 004870): Train loss 4.428, Val loss 5.007\n",
            "Ep 4 (Iter 004875): Train loss 3.937, Val loss 5.008\n",
            "Ep 4 (Iter 004880): Train loss 4.427, Val loss 4.984\n",
            "Ep 4 (Iter 004885): Train loss 5.134, Val loss 5.001\n",
            "Ep 4 (Iter 004890): Train loss 3.602, Val loss 5.035\n",
            "Ep 4 (Iter 004895): Train loss 4.238, Val loss 5.072\n",
            "Ep 4 (Iter 004900): Train loss 3.974, Val loss 5.048\n",
            "Ep 4 (Iter 004905): Train loss 4.091, Val loss 5.038\n",
            "Ep 4 (Iter 004910): Train loss 4.300, Val loss 5.017\n",
            "Ep 4 (Iter 004915): Train loss 4.030, Val loss 4.990\n",
            "Ep 4 (Iter 004920): Train loss 3.371, Val loss 4.994\n",
            "Ep 4 (Iter 004925): Train loss 4.296, Val loss 4.989\n",
            "Ep 4 (Iter 004930): Train loss 4.300, Val loss 4.978\n",
            "Ep 4 (Iter 004935): Train loss 4.131, Val loss 4.982\n",
            "Ep 4 (Iter 004940): Train loss 4.580, Val loss 5.004\n",
            "Ep 4 (Iter 004945): Train loss 4.519, Val loss 4.998\n",
            "Ep 4 (Iter 004950): Train loss 4.143, Val loss 4.961\n",
            "Ep 4 (Iter 004955): Train loss 4.419, Val loss 4.972\n",
            "Ep 4 (Iter 004960): Train loss 3.980, Val loss 4.982\n",
            "Ep 4 (Iter 004965): Train loss 3.995, Val loss 4.960\n",
            "Ep 4 (Iter 004970): Train loss 2.747, Val loss 4.974\n",
            "Ep 4 (Iter 004975): Train loss 4.383, Val loss 4.964\n",
            "Ep 4 (Iter 004980): Train loss 3.958, Val loss 4.938\n",
            "Ep 4 (Iter 004985): Train loss 3.502, Val loss 4.933\n",
            "Ep 4 (Iter 004990): Train loss 4.584, Val loss 4.938\n",
            "Ep 4 (Iter 004995): Train loss 4.399, Val loss 4.967\n",
            "Ep 4 (Iter 005000): Train loss 3.816, Val loss 4.961\n",
            "Ep 4 (Iter 005005): Train loss 4.060, Val loss 4.961\n",
            "Ep 4 (Iter 005010): Train loss 4.254, Val loss 5.001\n",
            "Ep 4 (Iter 005015): Train loss 4.804, Val loss 4.975\n",
            "Ep 4 (Iter 005020): Train loss 4.188, Val loss 4.970\n",
            "Ep 4 (Iter 005025): Train loss 4.375, Val loss 5.002\n",
            "Ep 4 (Iter 005030): Train loss 4.269, Val loss 5.023\n",
            "Ep 4 (Iter 005035): Train loss 4.272, Val loss 5.021\n",
            "Ep 4 (Iter 005040): Train loss 3.967, Val loss 4.995\n",
            "Ep 4 (Iter 005045): Train loss 4.105, Val loss 4.980\n",
            "Ep 4 (Iter 005050): Train loss 4.373, Val loss 4.957\n",
            "Ep 4 (Iter 005055): Train loss 4.660, Val loss 4.963\n",
            "Ep 4 (Iter 005060): Train loss 3.738, Val loss 5.023\n",
            "Ep 4 (Iter 005065): Train loss 4.085, Val loss 4.993\n",
            "Ep 4 (Iter 005070): Train loss 4.317, Val loss 4.979\n",
            "Ep 4 (Iter 005075): Train loss 3.901, Val loss 4.970\n",
            "Ep 4 (Iter 005080): Train loss 4.109, Val loss 4.967\n",
            "Ep 4 (Iter 005085): Train loss 4.312, Val loss 4.968\n",
            "Ep 4 (Iter 005090): Train loss 4.128, Val loss 4.935\n",
            "Ep 4 (Iter 005095): Train loss 3.919, Val loss 4.931\n",
            "Ep 4 (Iter 005100): Train loss 4.425, Val loss 4.941\n",
            "Ep 4 (Iter 005105): Train loss 3.990, Val loss 4.947\n",
            "Ep 4 (Iter 005110): Train loss 4.443, Val loss 4.913\n",
            "Ep 4 (Iter 005115): Train loss 4.269, Val loss 4.879\n",
            "Ep 4 (Iter 005120): Train loss 3.479, Val loss 4.892\n",
            "Ep 4 (Iter 005125): Train loss 4.417, Val loss 4.884\n",
            "Ep 4 (Iter 005130): Train loss 4.536, Val loss 4.900\n",
            "Ep 4 (Iter 005135): Train loss 4.236, Val loss 4.918\n",
            "Ep 4 (Iter 005140): Train loss 4.740, Val loss 4.930\n",
            "Ep 4 (Iter 005145): Train loss 4.013, Val loss 4.957\n",
            "Ep 4 (Iter 005150): Train loss 3.789, Val loss 4.984\n",
            "Ep 4 (Iter 005155): Train loss 4.357, Val loss 4.970\n",
            "Ep 4 (Iter 005160): Train loss 4.630, Val loss 4.954\n",
            "Ep 4 (Iter 005165): Train loss 4.758, Val loss 4.940\n",
            "Ep 4 (Iter 005170): Train loss 4.545, Val loss 4.969\n",
            "Ep 4 (Iter 005175): Train loss 4.089, Val loss 4.952\n",
            "Ep 4 (Iter 005180): Train loss 3.403, Val loss 4.960\n",
            "Ep 4 (Iter 005185): Train loss 4.709, Val loss 4.947\n",
            "Ep 4 (Iter 005190): Train loss 3.963, Val loss 4.923\n",
            "Ep 4 (Iter 005195): Train loss 4.465, Val loss 4.964\n",
            "Ep 4 (Iter 005200): Train loss 4.640, Val loss 4.944\n",
            "Ep 4 (Iter 005205): Train loss 3.723, Val loss 4.939\n",
            "Ep 4 (Iter 005210): Train loss 4.335, Val loss 4.926\n",
            "Ep 4 (Iter 005215): Train loss 4.250, Val loss 4.921\n",
            "Ep 4 (Iter 005220): Train loss 4.044, Val loss 4.936\n",
            "Ep 4 (Iter 005225): Train loss 3.974, Val loss 4.939\n",
            "Ep 4 (Iter 005230): Train loss 4.035, Val loss 4.944\n",
            "Ep 4 (Iter 005235): Train loss 4.000, Val loss 4.920\n",
            "Ep 4 (Iter 005240): Train loss 4.273, Val loss 4.906\n",
            "Ep 4 (Iter 005245): Train loss 4.621, Val loss 4.928\n",
            "Ep 4 (Iter 005250): Train loss 4.577, Val loss 4.959\n",
            "Ep 4 (Iter 005255): Train loss 3.728, Val loss 4.962\n",
            "Ep 4 (Iter 005260): Train loss 3.861, Val loss 4.964\n",
            "Ep 4 (Iter 005265): Train loss 4.307, Val loss 4.951\n",
            "Ep 4 (Iter 005270): Train loss 4.203, Val loss 4.950\n",
            "Ep 4 (Iter 005275): Train loss 4.520, Val loss 4.954\n",
            "Ep 4 (Iter 005280): Train loss 4.707, Val loss 4.965\n",
            "Ep 4 (Iter 005285): Train loss 3.674, Val loss 4.975\n",
            "Ep 4 (Iter 005290): Train loss 3.914, Val loss 4.994\n",
            "Ep 4 (Iter 005295): Train loss 4.368, Val loss 4.974\n",
            "Ep 4 (Iter 005300): Train loss 4.196, Val loss 4.937\n",
            "Ep 4 (Iter 005305): Train loss 3.608, Val loss 4.940\n",
            "Ep 4 (Iter 005310): Train loss 3.694, Val loss 4.948\n",
            "Ep 4 (Iter 005315): Train loss 3.795, Val loss 4.968\n",
            "Ep 4 (Iter 005320): Train loss 4.037, Val loss 5.006\n",
            "Ep 4 (Iter 005325): Train loss 3.820, Val loss 5.013\n",
            "Ep 4 (Iter 005330): Train loss 4.011, Val loss 5.000\n",
            "Ep 4 (Iter 005335): Train loss 4.438, Val loss 4.975\n",
            "Ep 4 (Iter 005340): Train loss 4.363, Val loss 4.943\n",
            "Ep 4 (Iter 005345): Train loss 3.921, Val loss 4.912\n",
            "Ep 4 (Iter 005350): Train loss 3.779, Val loss 4.896\n",
            "Ep 4 (Iter 005355): Train loss 3.756, Val loss 4.899\n",
            "Ep 4 (Iter 005360): Train loss 3.993, Val loss 4.896\n",
            "Ep 4 (Iter 005365): Train loss 3.573, Val loss 4.937\n",
            "Ep 4 (Iter 005370): Train loss 4.650, Val loss 4.949\n",
            "Ep 4 (Iter 005375): Train loss 3.978, Val loss 4.936\n",
            "Ep 4 (Iter 005380): Train loss 3.805, Val loss 4.943\n",
            "Ep 4 (Iter 005385): Train loss 4.253, Val loss 4.967\n",
            "Ep 4 (Iter 005390): Train loss 4.182, Val loss 4.948\n",
            "Ep 4 (Iter 005395): Train loss 3.669, Val loss 4.961\n",
            "Ep 4 (Iter 005400): Train loss 4.562, Val loss 4.970\n",
            "Ep 4 (Iter 005405): Train loss 4.152, Val loss 4.976\n",
            "Ep 4 (Iter 005410): Train loss 4.704, Val loss 4.977\n",
            "Ep 4 (Iter 005415): Train loss 4.079, Val loss 4.982\n",
            "Ep 4 (Iter 005420): Train loss 4.511, Val loss 4.965\n",
            "Ep 4 (Iter 005425): Train loss 3.837, Val loss 4.968\n",
            "Ep 4 (Iter 005430): Train loss 4.015, Val loss 4.959\n",
            "Ep 4 (Iter 005435): Train loss 4.032, Val loss 4.949\n",
            "Ep 4 (Iter 005440): Train loss 3.901, Val loss 4.966\n",
            "Ep 4 (Iter 005445): Train loss 3.942, Val loss 4.960\n",
            "Ep 4 (Iter 005450): Train loss 4.363, Val loss 4.924\n",
            "Ep 4 (Iter 005455): Train loss 3.812, Val loss 4.930\n",
            "Ep 4 (Iter 005460): Train loss 4.216, Val loss 4.929\n",
            "Ep 4 (Iter 005465): Train loss 4.593, Val loss 4.939\n",
            "Ep 4 (Iter 005470): Train loss 3.965, Val loss 4.943\n",
            "Ep 4 (Iter 005475): Train loss 3.169, Val loss 4.966\n",
            "Ep 4 (Iter 005480): Train loss 4.255, Val loss 4.955\n",
            "Ep 4 (Iter 005485): Train loss 3.417, Val loss 4.958\n",
            "Ep 4 (Iter 005490): Train loss 4.446, Val loss 4.964\n",
            "Ep 4 (Iter 005495): Train loss 4.200, Val loss 4.974\n",
            "Ep 4 (Iter 005500): Train loss 3.546, Val loss 4.954\n",
            "Ep 4 (Iter 005505): Train loss 4.123, Val loss 4.938\n",
            "Ep 4 (Iter 005510): Train loss 4.255, Val loss 4.952\n",
            "Ep 4 (Iter 005515): Train loss 3.653, Val loss 4.960\n",
            "Ep 4 (Iter 005520): Train loss 4.448, Val loss 4.934\n",
            "Ep 4 (Iter 005525): Train loss 4.167, Val loss 4.915\n",
            "Ep 4 (Iter 005530): Train loss 3.621, Val loss 4.922\n",
            "Ep 4 (Iter 005535): Train loss 4.004, Val loss 4.929\n",
            "Ep 4 (Iter 005540): Train loss 4.291, Val loss 4.924\n",
            "Ep 4 (Iter 005545): Train loss 4.380, Val loss 4.909\n",
            "Ep 4 (Iter 005550): Train loss 3.957, Val loss 4.923\n",
            "Ep 4 (Iter 005555): Train loss 4.543, Val loss 4.924\n",
            "Ep 4 (Iter 005560): Train loss 3.297, Val loss 4.930\n",
            "Ep 4 (Iter 005565): Train loss 3.860, Val loss 4.947\n",
            "Ep 4 (Iter 005570): Train loss 3.660, Val loss 4.967\n",
            "Ep 4 (Iter 005575): Train loss 3.812, Val loss 4.941\n",
            "Ep 4 (Iter 005580): Train loss 3.722, Val loss 4.941\n",
            "Ep 4 (Iter 005585): Train loss 4.140, Val loss 4.975\n",
            "Ep 4 (Iter 005590): Train loss 4.207, Val loss 5.007\n",
            "Ep 4 (Iter 005595): Train loss 4.414, Val loss 4.988\n",
            "Ep 4 (Iter 005600): Train loss 3.671, Val loss 4.984\n",
            "Ep 4 (Iter 005605): Train loss 3.266, Val loss 4.956\n",
            "Ep 4 (Iter 005610): Train loss 3.545, Val loss 4.958\n",
            "Ep 4 (Iter 005615): Train loss 4.046, Val loss 4.956\n",
            "Ep 4 (Iter 005620): Train loss 3.340, Val loss 4.964\n",
            "Ep 4 (Iter 005625): Train loss 4.071, Val loss 4.956\n",
            "Ep 4 (Iter 005630): Train loss 3.879, Val loss 4.954\n",
            "Ep 4 (Iter 005635): Train loss 4.589, Val loss 4.930\n",
            "Ep 4 (Iter 005640): Train loss 3.816, Val loss 4.963\n",
            "Ep 4 (Iter 005645): Train loss 4.179, Val loss 4.994\n",
            "Ep 4 (Iter 005650): Train loss 3.961, Val loss 4.964\n",
            "Ep 4 (Iter 005655): Train loss 4.348, Val loss 4.959\n",
            "Ep 4 (Iter 005660): Train loss 4.074, Val loss 4.998\n",
            "Ep 4 (Iter 005665): Train loss 3.500, Val loss 4.983\n",
            "Ep 4 (Iter 005670): Train loss 3.408, Val loss 4.971\n",
            "Ep 4 (Iter 005675): Train loss 3.906, Val loss 4.954\n",
            "Ep 4 (Iter 005680): Train loss 3.683, Val loss 4.948\n",
            "Ep 4 (Iter 005685): Train loss 4.379, Val loss 4.980\n",
            "Ep 4 (Iter 005690): Train loss 3.520, Val loss 4.978\n",
            "Ep 4 (Iter 005695): Train loss 3.608, Val loss 4.964\n",
            "Ep 4 (Iter 005700): Train loss 3.989, Val loss 4.956\n",
            "Ep 4 (Iter 005705): Train loss 4.079, Val loss 4.951\n",
            "Ep 4 (Iter 005710): Train loss 3.720, Val loss 4.926\n",
            "Ep 4 (Iter 005715): Train loss 3.924, Val loss 4.913\n",
            "Ep 4 (Iter 005720): Train loss 4.147, Val loss 4.911\n",
            "Ep 4 (Iter 005725): Train loss 4.502, Val loss 4.897\n",
            "Ep 4 (Iter 005730): Train loss 3.332, Val loss 4.894\n",
            "Ep 4 (Iter 005735): Train loss 3.585, Val loss 4.893\n",
            "Ep 4 (Iter 005740): Train loss 4.015, Val loss 4.883\n",
            "Ep 4 (Iter 005745): Train loss 4.414, Val loss 4.890\n",
            "Ep 4 (Iter 005750): Train loss 3.513, Val loss 4.907\n",
            "Ep 4 (Iter 005755): Train loss 3.655, Val loss 4.909\n",
            "Ep 4 (Iter 005760): Train loss 3.614, Val loss 4.915\n",
            "Ep 4 (Iter 005765): Train loss 4.251, Val loss 4.927\n",
            "Ep 4 (Iter 005770): Train loss 3.294, Val loss 4.944\n",
            "Ep 4 (Iter 005775): Train loss 4.061, Val loss 4.943\n",
            "Ep 4 (Iter 005780): Train loss 3.621, Val loss 4.905\n",
            "Ep 4 (Iter 005785): Train loss 3.716, Val loss 4.906\n",
            "Ep 4 (Iter 005790): Train loss 4.269, Val loss 4.911\n",
            "Ep 4 (Iter 005795): Train loss 4.066, Val loss 4.911\n",
            "Ep 4 (Iter 005800): Train loss 3.962, Val loss 4.903\n",
            "Ep 4 (Iter 005805): Train loss 4.295, Val loss 4.895\n",
            "Ep 4 (Iter 005810): Train loss 3.996, Val loss 4.928\n",
            "Ep 4 (Iter 005815): Train loss 4.201, Val loss 4.920\n",
            "Ep 4 (Iter 005820): Train loss 3.595, Val loss 4.922\n",
            "Ep 4 (Iter 005825): Train loss 4.135, Val loss 4.917\n",
            "Ep 4 (Iter 005830): Train loss 3.855, Val loss 4.883\n",
            "Ep 4 (Iter 005835): Train loss 4.082, Val loss 4.847\n",
            "Ep 4 (Iter 005840): Train loss 3.986, Val loss 4.848\n",
            "Ep 4 (Iter 005845): Train loss 4.037, Val loss 4.868\n",
            "Ep 4 (Iter 005850): Train loss 2.987, Val loss 4.883\n",
            "Ep 4 (Iter 005855): Train loss 3.874, Val loss 4.876\n",
            "Ep 4 (Iter 005860): Train loss 3.859, Val loss 4.874\n",
            "Ep 4 (Iter 005865): Train loss 3.984, Val loss 4.878\n",
            "Ep 4 (Iter 005870): Train loss 3.888, Val loss 4.906\n",
            "Ep 4 (Iter 005875): Train loss 3.792, Val loss 4.898\n",
            "Ep 4 (Iter 005880): Train loss 3.366, Val loss 4.869\n",
            "Ep 4 (Iter 005885): Train loss 3.360, Val loss 4.859\n",
            "Ep 4 (Iter 005890): Train loss 3.538, Val loss 4.868\n",
            "Ep 4 (Iter 005895): Train loss 4.271, Val loss 4.870\n",
            "Ep 4 (Iter 005900): Train loss 4.152, Val loss 4.863\n",
            "Ep 4 (Iter 005905): Train loss 3.887, Val loss 4.858\n",
            "Ep 4 (Iter 005910): Train loss 3.837, Val loss 4.858\n",
            "Ep 4 (Iter 005915): Train loss 3.904, Val loss 4.867\n",
            "Ep 4 (Iter 005920): Train loss 3.951, Val loss 4.873\n",
            "Ep 4 (Iter 005925): Train loss 4.056, Val loss 4.885\n",
            "Ep 4 (Iter 005930): Train loss 3.552, Val loss 4.889\n",
            "Ep 4 (Iter 005935): Train loss 4.029, Val loss 4.884\n",
            "Ep 4 (Iter 005940): Train loss 3.518, Val loss 4.883\n",
            "Ep 4 (Iter 005945): Train loss 3.747, Val loss 4.887\n",
            "Ep 4 (Iter 005950): Train loss 3.730, Val loss 4.861\n",
            "Ep 4 (Iter 005955): Train loss 3.045, Val loss 4.870\n",
            "Ep 4 (Iter 005960): Train loss 4.140, Val loss 4.884\n",
            "Ep 4 (Iter 005965): Train loss 4.027, Val loss 4.888\n",
            "Ep 4 (Iter 005970): Train loss 3.521, Val loss 4.857\n",
            "Ep 4 (Iter 005975): Train loss 4.305, Val loss 4.853\n",
            "Ep 4 (Iter 005980): Train loss 4.470, Val loss 4.866\n",
            "Ep 4 (Iter 005985): Train loss 3.833, Val loss 4.855\n",
            "Ep 4 (Iter 005990): Train loss 3.529, Val loss 4.836\n",
            "Ep 4 (Iter 005995): Train loss 3.375, Val loss 4.818\n",
            "Ep 4 (Iter 006000): Train loss 4.181, Val loss 4.796\n",
            "Ep 4 (Iter 006005): Train loss 3.638, Val loss 4.791\n",
            "Ep 4 (Iter 006010): Train loss 3.955, Val loss 4.795\n",
            "Ep 4 (Iter 006015): Train loss 3.934, Val loss 4.811\n",
            "Ep 4 (Iter 006020): Train loss 3.556, Val loss 4.835\n",
            "Ep 4 (Iter 006025): Train loss 3.719, Val loss 4.827\n",
            "Ep 4 (Iter 006030): Train loss 3.379, Val loss 4.825\n",
            "Ep 4 (Iter 006035): Train loss 2.059, Val loss 4.835\n",
            "Ep 4 (Iter 006040): Train loss 3.654, Val loss 4.842\n",
            "Ep 4 (Iter 006045): Train loss 3.669, Val loss 4.846\n",
            "Ep 4 (Iter 006050): Train loss 4.134, Val loss 4.862\n",
            "In the midst of winter, and the French army, and the Russian army, and the French army, and the French army, and the French army, and the Russian army, and the French army, and the French army, and the French army, and\n",
            "Ep 5 (Iter 006055): Train loss 3.903, Val loss 4.850\n",
            "Ep 5 (Iter 006060): Train loss 3.701, Val loss 4.818\n",
            "Ep 5 (Iter 006065): Train loss 3.565, Val loss 4.823\n",
            "Ep 5 (Iter 006070): Train loss 3.908, Val loss 4.840\n",
            "Ep 5 (Iter 006075): Train loss 3.857, Val loss 4.864\n",
            "Ep 5 (Iter 006080): Train loss 3.721, Val loss 4.877\n",
            "Ep 5 (Iter 006085): Train loss 3.743, Val loss 4.893\n",
            "Ep 5 (Iter 006090): Train loss 4.091, Val loss 4.891\n",
            "Ep 5 (Iter 006095): Train loss 3.720, Val loss 4.885\n",
            "Ep 5 (Iter 006100): Train loss 3.762, Val loss 4.871\n",
            "Ep 5 (Iter 006105): Train loss 3.290, Val loss 4.886\n",
            "Ep 5 (Iter 006110): Train loss 3.924, Val loss 4.916\n",
            "Ep 5 (Iter 006115): Train loss 3.816, Val loss 4.915\n",
            "Ep 5 (Iter 006120): Train loss 3.855, Val loss 4.887\n",
            "Ep 5 (Iter 006125): Train loss 3.489, Val loss 4.892\n",
            "Ep 5 (Iter 006130): Train loss 3.457, Val loss 4.903\n",
            "Ep 5 (Iter 006135): Train loss 3.998, Val loss 4.909\n",
            "Ep 5 (Iter 006140): Train loss 3.597, Val loss 4.910\n",
            "Ep 5 (Iter 006145): Train loss 3.514, Val loss 4.914\n",
            "Ep 5 (Iter 006150): Train loss 4.087, Val loss 4.931\n",
            "Ep 5 (Iter 006155): Train loss 3.406, Val loss 4.919\n",
            "Ep 5 (Iter 006160): Train loss 3.647, Val loss 4.913\n",
            "Ep 5 (Iter 006165): Train loss 4.219, Val loss 4.905\n",
            "Ep 5 (Iter 006170): Train loss 4.023, Val loss 4.914\n",
            "Ep 5 (Iter 006175): Train loss 4.011, Val loss 4.935\n",
            "Ep 5 (Iter 006180): Train loss 3.395, Val loss 4.939\n",
            "Ep 5 (Iter 006185): Train loss 3.781, Val loss 4.936\n",
            "Ep 5 (Iter 006190): Train loss 3.581, Val loss 4.929\n",
            "Ep 5 (Iter 006195): Train loss 4.045, Val loss 4.943\n",
            "Ep 5 (Iter 006200): Train loss 3.268, Val loss 4.944\n",
            "Ep 5 (Iter 006205): Train loss 3.521, Val loss 4.946\n",
            "Ep 5 (Iter 006210): Train loss 3.889, Val loss 4.943\n",
            "Ep 5 (Iter 006215): Train loss 3.551, Val loss 4.925\n",
            "Ep 5 (Iter 006220): Train loss 3.702, Val loss 4.919\n",
            "Ep 5 (Iter 006225): Train loss 3.810, Val loss 4.922\n",
            "Ep 5 (Iter 006230): Train loss 3.604, Val loss 4.913\n",
            "Ep 5 (Iter 006235): Train loss 3.249, Val loss 4.897\n",
            "Ep 5 (Iter 006240): Train loss 3.490, Val loss 4.911\n",
            "Ep 5 (Iter 006245): Train loss 3.365, Val loss 4.907\n",
            "Ep 5 (Iter 006250): Train loss 3.724, Val loss 4.896\n",
            "Ep 5 (Iter 006255): Train loss 2.194, Val loss 4.882\n",
            "Ep 5 (Iter 006260): Train loss 3.511, Val loss 4.870\n",
            "Ep 5 (Iter 006265): Train loss 3.973, Val loss 4.862\n",
            "Ep 5 (Iter 006270): Train loss 3.439, Val loss 4.871\n",
            "Ep 5 (Iter 006275): Train loss 3.570, Val loss 4.870\n",
            "Ep 5 (Iter 006280): Train loss 3.412, Val loss 4.856\n",
            "Ep 5 (Iter 006285): Train loss 3.544, Val loss 4.857\n",
            "Ep 5 (Iter 006290): Train loss 3.734, Val loss 4.890\n",
            "Ep 5 (Iter 006295): Train loss 3.472, Val loss 4.923\n",
            "Ep 5 (Iter 006300): Train loss 2.586, Val loss 4.898\n",
            "Ep 5 (Iter 006305): Train loss 3.805, Val loss 4.877\n",
            "Ep 5 (Iter 006310): Train loss 2.959, Val loss 4.886\n",
            "Ep 5 (Iter 006315): Train loss 3.004, Val loss 4.891\n",
            "Ep 5 (Iter 006320): Train loss 3.870, Val loss 4.906\n",
            "Ep 5 (Iter 006325): Train loss 4.283, Val loss 4.903\n",
            "Ep 5 (Iter 006330): Train loss 3.824, Val loss 4.898\n",
            "Ep 5 (Iter 006335): Train loss 3.537, Val loss 4.896\n",
            "Ep 5 (Iter 006340): Train loss 3.810, Val loss 4.880\n",
            "Ep 5 (Iter 006345): Train loss 3.526, Val loss 4.864\n",
            "Ep 5 (Iter 006350): Train loss 3.330, Val loss 4.847\n",
            "Ep 5 (Iter 006355): Train loss 3.728, Val loss 4.845\n",
            "Ep 5 (Iter 006360): Train loss 3.849, Val loss 4.861\n",
            "Ep 5 (Iter 006365): Train loss 3.461, Val loss 4.877\n",
            "Ep 5 (Iter 006370): Train loss 4.112, Val loss 4.873\n",
            "Ep 5 (Iter 006375): Train loss 3.729, Val loss 4.864\n",
            "Ep 5 (Iter 006380): Train loss 3.338, Val loss 4.865\n",
            "Ep 5 (Iter 006385): Train loss 3.738, Val loss 4.872\n",
            "Ep 5 (Iter 006390): Train loss 4.339, Val loss 4.888\n",
            "Ep 5 (Iter 006395): Train loss 3.357, Val loss 4.884\n",
            "Ep 5 (Iter 006400): Train loss 3.449, Val loss 4.882\n",
            "Ep 5 (Iter 006405): Train loss 3.994, Val loss 4.893\n",
            "Ep 5 (Iter 006410): Train loss 3.842, Val loss 4.905\n",
            "Ep 5 (Iter 006415): Train loss 4.341, Val loss 4.905\n",
            "Ep 5 (Iter 006420): Train loss 3.583, Val loss 4.902\n",
            "Ep 5 (Iter 006425): Train loss 4.003, Val loss 4.892\n",
            "Ep 5 (Iter 006430): Train loss 3.915, Val loss 4.882\n",
            "Ep 5 (Iter 006435): Train loss 3.167, Val loss 4.879\n",
            "Ep 5 (Iter 006440): Train loss 3.516, Val loss 4.866\n",
            "Ep 5 (Iter 006445): Train loss 3.163, Val loss 4.879\n",
            "Ep 5 (Iter 006450): Train loss 3.297, Val loss 4.875\n",
            "Ep 5 (Iter 006455): Train loss 3.034, Val loss 4.878\n",
            "Ep 5 (Iter 006460): Train loss 3.868, Val loss 4.885\n",
            "Ep 5 (Iter 006465): Train loss 3.239, Val loss 4.879\n",
            "Ep 5 (Iter 006470): Train loss 3.569, Val loss 4.868\n",
            "Ep 5 (Iter 006475): Train loss 3.885, Val loss 4.856\n",
            "Ep 5 (Iter 006480): Train loss 3.767, Val loss 4.859\n",
            "Ep 5 (Iter 006485): Train loss 3.707, Val loss 4.858\n",
            "Ep 5 (Iter 006490): Train loss 3.435, Val loss 4.869\n",
            "Ep 5 (Iter 006495): Train loss 4.100, Val loss 4.880\n",
            "Ep 5 (Iter 006500): Train loss 3.131, Val loss 4.878\n",
            "Ep 5 (Iter 006505): Train loss 3.850, Val loss 4.870\n",
            "Ep 5 (Iter 006510): Train loss 3.620, Val loss 4.858\n",
            "Ep 5 (Iter 006515): Train loss 3.425, Val loss 4.846\n",
            "Ep 5 (Iter 006520): Train loss 3.864, Val loss 4.843\n",
            "Ep 5 (Iter 006525): Train loss 3.257, Val loss 4.842\n",
            "Ep 5 (Iter 006530): Train loss 3.469, Val loss 4.853\n",
            "Ep 5 (Iter 006535): Train loss 3.715, Val loss 4.874\n",
            "Ep 5 (Iter 006540): Train loss 3.837, Val loss 4.853\n",
            "Ep 5 (Iter 006545): Train loss 3.918, Val loss 4.849\n",
            "Ep 5 (Iter 006550): Train loss 4.237, Val loss 4.852\n",
            "Ep 5 (Iter 006555): Train loss 2.893, Val loss 4.845\n",
            "Ep 5 (Iter 006560): Train loss 3.456, Val loss 4.844\n",
            "Ep 5 (Iter 006565): Train loss 3.529, Val loss 4.860\n",
            "Ep 5 (Iter 006570): Train loss 3.551, Val loss 4.857\n",
            "Ep 5 (Iter 006575): Train loss 4.125, Val loss 4.841\n",
            "Ep 5 (Iter 006580): Train loss 3.767, Val loss 4.838\n",
            "Ep 5 (Iter 006585): Train loss 3.553, Val loss 4.853\n",
            "Ep 5 (Iter 006590): Train loss 3.362, Val loss 4.875\n",
            "Ep 5 (Iter 006595): Train loss 3.754, Val loss 4.880\n",
            "Ep 5 (Iter 006600): Train loss 3.694, Val loss 4.872\n",
            "Ep 5 (Iter 006605): Train loss 3.737, Val loss 4.854\n",
            "Ep 5 (Iter 006610): Train loss 3.419, Val loss 4.852\n",
            "Ep 5 (Iter 006615): Train loss 3.550, Val loss 4.861\n",
            "Ep 5 (Iter 006620): Train loss 3.642, Val loss 4.861\n",
            "Ep 5 (Iter 006625): Train loss 3.341, Val loss 4.846\n",
            "Ep 5 (Iter 006630): Train loss 2.963, Val loss 4.836\n",
            "Ep 5 (Iter 006635): Train loss 4.081, Val loss 4.835\n",
            "Ep 5 (Iter 006640): Train loss 3.016, Val loss 4.835\n",
            "Ep 5 (Iter 006645): Train loss 3.401, Val loss 4.839\n",
            "Ep 5 (Iter 006650): Train loss 3.427, Val loss 4.842\n",
            "Ep 5 (Iter 006655): Train loss 3.809, Val loss 4.845\n",
            "Ep 5 (Iter 006660): Train loss 3.896, Val loss 4.848\n",
            "Ep 5 (Iter 006665): Train loss 3.371, Val loss 4.847\n",
            "Ep 5 (Iter 006670): Train loss 3.583, Val loss 4.841\n",
            "Ep 5 (Iter 006675): Train loss 3.613, Val loss 4.829\n",
            "Ep 5 (Iter 006680): Train loss 3.129, Val loss 4.824\n",
            "Ep 5 (Iter 006685): Train loss 4.420, Val loss 4.820\n",
            "Ep 5 (Iter 006690): Train loss 3.437, Val loss 4.826\n",
            "Ep 5 (Iter 006695): Train loss 4.159, Val loss 4.833\n",
            "Ep 5 (Iter 006700): Train loss 3.447, Val loss 4.833\n",
            "Ep 5 (Iter 006705): Train loss 2.975, Val loss 4.829\n",
            "Ep 5 (Iter 006710): Train loss 3.552, Val loss 4.829\n",
            "Ep 5 (Iter 006715): Train loss 4.227, Val loss 4.834\n",
            "Ep 5 (Iter 006720): Train loss 3.810, Val loss 4.837\n",
            "Ep 5 (Iter 006725): Train loss 3.253, Val loss 4.836\n",
            "Ep 5 (Iter 006730): Train loss 3.410, Val loss 4.835\n",
            "Ep 5 (Iter 006735): Train loss 3.680, Val loss 4.838\n",
            "Ep 5 (Iter 006740): Train loss 3.265, Val loss 4.852\n",
            "Ep 5 (Iter 006745): Train loss 3.226, Val loss 4.861\n",
            "Ep 5 (Iter 006750): Train loss 3.462, Val loss 4.856\n",
            "Ep 5 (Iter 006755): Train loss 3.744, Val loss 4.845\n",
            "Ep 5 (Iter 006760): Train loss 3.233, Val loss 4.838\n",
            "Ep 5 (Iter 006765): Train loss 3.693, Val loss 4.836\n",
            "Ep 5 (Iter 006770): Train loss 3.642, Val loss 4.832\n",
            "Ep 5 (Iter 006775): Train loss 3.064, Val loss 4.832\n",
            "Ep 5 (Iter 006780): Train loss 1.997, Val loss 4.831\n",
            "Ep 5 (Iter 006785): Train loss 3.295, Val loss 4.827\n",
            "Ep 5 (Iter 006790): Train loss 3.602, Val loss 4.811\n",
            "Ep 5 (Iter 006795): Train loss 3.853, Val loss 4.801\n",
            "Ep 5 (Iter 006800): Train loss 3.355, Val loss 4.798\n",
            "Ep 5 (Iter 006805): Train loss 2.292, Val loss 4.798\n",
            "Ep 5 (Iter 006810): Train loss 3.220, Val loss 4.801\n",
            "Ep 5 (Iter 006815): Train loss 3.527, Val loss 4.809\n",
            "Ep 5 (Iter 006820): Train loss 3.345, Val loss 4.813\n",
            "Ep 5 (Iter 006825): Train loss 3.459, Val loss 4.814\n",
            "Ep 5 (Iter 006830): Train loss 2.742, Val loss 4.822\n",
            "Ep 5 (Iter 006835): Train loss 3.489, Val loss 4.824\n",
            "Ep 5 (Iter 006840): Train loss 3.184, Val loss 4.820\n",
            "Ep 5 (Iter 006845): Train loss 3.375, Val loss 4.812\n",
            "Ep 5 (Iter 006850): Train loss 3.202, Val loss 4.806\n",
            "Ep 5 (Iter 006855): Train loss 4.041, Val loss 4.811\n",
            "Ep 5 (Iter 006860): Train loss 3.653, Val loss 4.818\n",
            "Ep 5 (Iter 006865): Train loss 3.508, Val loss 4.822\n",
            "Ep 5 (Iter 006870): Train loss 3.421, Val loss 4.822\n",
            "Ep 5 (Iter 006875): Train loss 3.093, Val loss 4.827\n",
            "Ep 5 (Iter 006880): Train loss 3.998, Val loss 4.827\n",
            "Ep 5 (Iter 006885): Train loss 3.464, Val loss 4.826\n",
            "Ep 5 (Iter 006890): Train loss 3.593, Val loss 4.828\n",
            "Ep 5 (Iter 006895): Train loss 3.268, Val loss 4.833\n",
            "Ep 5 (Iter 006900): Train loss 3.941, Val loss 4.833\n",
            "Ep 5 (Iter 006905): Train loss 3.261, Val loss 4.824\n",
            "Ep 5 (Iter 006910): Train loss 3.071, Val loss 4.820\n",
            "Ep 5 (Iter 006915): Train loss 3.599, Val loss 4.820\n",
            "Ep 5 (Iter 006920): Train loss 2.924, Val loss 4.817\n",
            "Ep 5 (Iter 006925): Train loss 3.550, Val loss 4.815\n",
            "Ep 5 (Iter 006930): Train loss 3.529, Val loss 4.814\n",
            "Ep 5 (Iter 006935): Train loss 3.624, Val loss 4.814\n",
            "Ep 5 (Iter 006940): Train loss 3.360, Val loss 4.815\n",
            "Ep 5 (Iter 006945): Train loss 3.119, Val loss 4.816\n",
            "Ep 5 (Iter 006950): Train loss 3.956, Val loss 4.816\n",
            "Ep 5 (Iter 006955): Train loss 3.168, Val loss 4.812\n",
            "Ep 5 (Iter 006960): Train loss 3.270, Val loss 4.808\n",
            "Ep 5 (Iter 006965): Train loss 3.213, Val loss 4.805\n",
            "Ep 5 (Iter 006970): Train loss 3.521, Val loss 4.803\n",
            "Ep 5 (Iter 006975): Train loss 3.950, Val loss 4.801\n",
            "Ep 5 (Iter 006980): Train loss 3.137, Val loss 4.804\n",
            "Ep 5 (Iter 006985): Train loss 3.277, Val loss 4.803\n",
            "Ep 5 (Iter 006990): Train loss 3.739, Val loss 4.803\n",
            "Ep 5 (Iter 006995): Train loss 3.643, Val loss 4.796\n",
            "Ep 5 (Iter 007000): Train loss 3.240, Val loss 4.790\n",
            "Ep 5 (Iter 007005): Train loss 3.309, Val loss 4.785\n",
            "Ep 5 (Iter 007010): Train loss 3.051, Val loss 4.784\n",
            "Ep 5 (Iter 007015): Train loss 3.189, Val loss 4.784\n",
            "Ep 5 (Iter 007020): Train loss 3.238, Val loss 4.787\n",
            "Ep 5 (Iter 007025): Train loss 3.194, Val loss 4.787\n",
            "Ep 5 (Iter 007030): Train loss 3.891, Val loss 4.790\n",
            "Ep 5 (Iter 007035): Train loss 3.673, Val loss 4.791\n",
            "Ep 5 (Iter 007040): Train loss 3.854, Val loss 4.793\n",
            "Ep 5 (Iter 007045): Train loss 3.062, Val loss 4.792\n",
            "Ep 5 (Iter 007050): Train loss 3.439, Val loss 4.789\n",
            "Ep 5 (Iter 007055): Train loss 1.775, Val loss 4.790\n",
            "Ep 5 (Iter 007060): Train loss 2.965, Val loss 4.791\n",
            "Ep 5 (Iter 007065): Train loss 3.358, Val loss 4.790\n",
            "Ep 5 (Iter 007070): Train loss 3.656, Val loss 4.787\n",
            "Ep 5 (Iter 007075): Train loss 4.188, Val loss 4.783\n",
            "Ep 5 (Iter 007080): Train loss 3.795, Val loss 4.777\n",
            "Ep 5 (Iter 007085): Train loss 3.632, Val loss 4.776\n",
            "Ep 5 (Iter 007090): Train loss 3.551, Val loss 4.778\n",
            "Ep 5 (Iter 007095): Train loss 3.456, Val loss 4.783\n",
            "Ep 5 (Iter 007100): Train loss 4.021, Val loss 4.785\n",
            "Ep 5 (Iter 007105): Train loss 3.086, Val loss 4.788\n",
            "Ep 5 (Iter 007110): Train loss 3.260, Val loss 4.789\n",
            "Ep 5 (Iter 007115): Train loss 3.634, Val loss 4.791\n",
            "Ep 5 (Iter 007120): Train loss 3.088, Val loss 4.787\n",
            "Ep 5 (Iter 007125): Train loss 3.846, Val loss 4.783\n",
            "Ep 5 (Iter 007130): Train loss 3.747, Val loss 4.782\n",
            "Ep 5 (Iter 007135): Train loss 3.709, Val loss 4.780\n",
            "Ep 5 (Iter 007140): Train loss 3.616, Val loss 4.778\n",
            "Ep 5 (Iter 007145): Train loss 3.750, Val loss 4.778\n",
            "Ep 5 (Iter 007150): Train loss 3.567, Val loss 4.776\n",
            "Ep 5 (Iter 007155): Train loss 3.621, Val loss 4.774\n",
            "Ep 5 (Iter 007160): Train loss 3.701, Val loss 4.774\n",
            "Ep 5 (Iter 007165): Train loss 3.198, Val loss 4.775\n",
            "Ep 5 (Iter 007170): Train loss 2.943, Val loss 4.776\n",
            "Ep 5 (Iter 007175): Train loss 3.358, Val loss 4.778\n",
            "Ep 5 (Iter 007180): Train loss 3.131, Val loss 4.780\n",
            "Ep 5 (Iter 007185): Train loss 3.935, Val loss 4.782\n",
            "Ep 5 (Iter 007190): Train loss 3.644, Val loss 4.784\n",
            "Ep 5 (Iter 007195): Train loss 3.850, Val loss 4.784\n",
            "Ep 5 (Iter 007200): Train loss 3.319, Val loss 4.786\n",
            "Ep 5 (Iter 007205): Train loss 3.233, Val loss 4.784\n",
            "Ep 5 (Iter 007210): Train loss 3.454, Val loss 4.781\n",
            "Ep 5 (Iter 007215): Train loss 3.215, Val loss 4.779\n",
            "Ep 5 (Iter 007220): Train loss 3.173, Val loss 4.776\n",
            "Ep 5 (Iter 007225): Train loss 3.270, Val loss 4.774\n",
            "Ep 5 (Iter 007230): Train loss 3.615, Val loss 4.775\n",
            "Ep 5 (Iter 007235): Train loss 3.523, Val loss 4.774\n",
            "Ep 5 (Iter 007240): Train loss 3.109, Val loss 4.775\n",
            "Ep 5 (Iter 007245): Train loss 2.912, Val loss 4.774\n",
            "Ep 5 (Iter 007250): Train loss 2.944, Val loss 4.773\n",
            "Ep 5 (Iter 007255): Train loss 3.630, Val loss 4.774\n",
            "Ep 5 (Iter 007260): Train loss 2.005, Val loss 4.776\n",
            "Ep 5 (Iter 007265): Train loss 3.297, Val loss 4.778\n",
            "Ep 5 (Iter 007270): Train loss 3.659, Val loss 4.779\n",
            "Ep 5 (Iter 007275): Train loss 3.644, Val loss 4.780\n",
            "Ep 5 (Iter 007280): Train loss 3.714, Val loss 4.779\n",
            "Ep 5 (Iter 007285): Train loss 3.408, Val loss 4.779\n",
            "Ep 5 (Iter 007290): Train loss 3.669, Val loss 4.778\n",
            "Ep 5 (Iter 007295): Train loss 3.628, Val loss 4.776\n",
            "Ep 5 (Iter 007300): Train loss 3.694, Val loss 4.775\n",
            "Ep 5 (Iter 007305): Train loss 3.228, Val loss 4.775\n",
            "Ep 5 (Iter 007310): Train loss 3.455, Val loss 4.775\n",
            "Ep 5 (Iter 007315): Train loss 3.768, Val loss 4.774\n",
            "Ep 5 (Iter 007320): Train loss 2.566, Val loss 4.774\n",
            "Ep 5 (Iter 007325): Train loss 3.251, Val loss 4.774\n",
            "Ep 5 (Iter 007330): Train loss 3.189, Val loss 4.775\n",
            "Ep 5 (Iter 007335): Train loss 3.365, Val loss 4.775\n",
            "Ep 5 (Iter 007340): Train loss 3.453, Val loss 4.775\n",
            "Ep 5 (Iter 007345): Train loss 3.559, Val loss 4.775\n",
            "Ep 5 (Iter 007350): Train loss 3.291, Val loss 4.774\n",
            "Ep 5 (Iter 007355): Train loss 3.216, Val loss 4.773\n",
            "Ep 5 (Iter 007360): Train loss 3.629, Val loss 4.772\n",
            "Ep 5 (Iter 007365): Train loss 3.131, Val loss 4.771\n",
            "Ep 5 (Iter 007370): Train loss 3.529, Val loss 4.771\n",
            "Ep 5 (Iter 007375): Train loss 3.067, Val loss 4.770\n",
            "Ep 5 (Iter 007380): Train loss 2.761, Val loss 4.770\n",
            "Ep 5 (Iter 007385): Train loss 3.269, Val loss 4.771\n",
            "Ep 5 (Iter 007390): Train loss 3.367, Val loss 4.772\n",
            "Ep 5 (Iter 007395): Train loss 3.353, Val loss 4.773\n",
            "Ep 5 (Iter 007400): Train loss 3.426, Val loss 4.773\n",
            "Ep 5 (Iter 007405): Train loss 3.421, Val loss 4.774\n",
            "Ep 5 (Iter 007410): Train loss 3.005, Val loss 4.773\n",
            "Ep 5 (Iter 007415): Train loss 3.321, Val loss 4.773\n",
            "Ep 5 (Iter 007420): Train loss 3.391, Val loss 4.772\n",
            "Ep 5 (Iter 007425): Train loss 3.258, Val loss 4.771\n",
            "Ep 5 (Iter 007430): Train loss 3.251, Val loss 4.770\n",
            "Ep 5 (Iter 007435): Train loss 2.645, Val loss 4.770\n",
            "Ep 5 (Iter 007440): Train loss 3.177, Val loss 4.770\n",
            "Ep 5 (Iter 007445): Train loss 3.534, Val loss 4.770\n",
            "Ep 5 (Iter 007450): Train loss 3.124, Val loss 4.771\n",
            "Ep 5 (Iter 007455): Train loss 3.150, Val loss 4.771\n",
            "Ep 5 (Iter 007460): Train loss 3.723, Val loss 4.771\n",
            "Ep 5 (Iter 007465): Train loss 3.490, Val loss 4.772\n",
            "Ep 5 (Iter 007470): Train loss 3.230, Val loss 4.772\n",
            "Ep 5 (Iter 007475): Train loss 3.125, Val loss 4.773\n",
            "Ep 5 (Iter 007480): Train loss 2.496, Val loss 4.773\n",
            "Ep 5 (Iter 007485): Train loss 3.033, Val loss 4.773\n",
            "Ep 5 (Iter 007490): Train loss 3.492, Val loss 4.773\n",
            "Ep 5 (Iter 007495): Train loss 2.052, Val loss 4.772\n",
            "Ep 5 (Iter 007500): Train loss 3.693, Val loss 4.772\n",
            "Ep 5 (Iter 007505): Train loss 3.301, Val loss 4.772\n",
            "Ep 5 (Iter 007510): Train loss 3.290, Val loss 4.772\n",
            "Ep 5 (Iter 007515): Train loss 3.515, Val loss 4.772\n",
            "Ep 5 (Iter 007520): Train loss 3.314, Val loss 4.773\n",
            "Ep 5 (Iter 007525): Train loss 3.305, Val loss 4.773\n",
            "Ep 5 (Iter 007530): Train loss 3.066, Val loss 4.772\n",
            "Ep 5 (Iter 007535): Train loss 3.207, Val loss 4.772\n",
            "Ep 5 (Iter 007540): Train loss 3.188, Val loss 4.771\n",
            "Ep 5 (Iter 007545): Train loss 3.224, Val loss 4.770\n",
            "Ep 5 (Iter 007550): Train loss 4.051, Val loss 4.770\n",
            "Ep 5 (Iter 007555): Train loss 3.716, Val loss 4.769\n",
            "Ep 5 (Iter 007560): Train loss 4.003, Val loss 4.769\n",
            "In the midst of winter rye, and the French, and the French, and the French, and the French, and the French, and the French, and the French, and the French, and the French, and the French, and the French, and the\n",
            "Training completed in 8.40 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let's see whether the learning rate behaves as intended:"
      ],
      "metadata": {
        "id": "JdrD68r5lanF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(range(len(lrs)), lrs)\n",
        "plt.ylabel(\"Learning rate\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "QVJI2ukZCir9",
        "outputId": "99e2b376-7012-491f-e4e7-6dc3d2f71cbe"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let's plot the losses:"
      ],
      "metadata": {
        "id": "98leYn3dlk0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "def plot_losses(epoch_seen,\n",
        "                tokens_seen,\n",
        "                train_losses,\n",
        "                val_losses,\n",
        "                fig_path=\"loss_plot.pdf\"):\n",
        "  \"\"\"Plot training and validation loss.\"\"\"\n",
        "\n",
        "  fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "  # plot training and validation loss against epochs\n",
        "  ax1.plot(epoch_seen, train_losses, label=\"Training Loss\")\n",
        "  ax1.plot(epoch_seen, val_losses, linestyle=\"-.\", label=\"Validation Loss\")\n",
        "  ax1.set_xlabel(\"Epoch\")\n",
        "  ax1.set_ylabel(\"Loss\")\n",
        "  ax1.legend(loc=\"upper right\")\n",
        "  ax1.xaxis.set_major_locator(MaxNLocator(integer=True)) # only show integer labels on x-axis\n",
        "\n",
        "  # create a second x-axis for token seen\n",
        "  ax2 = ax1.twiny() # create a second x-axis that shares the same y-axis\n",
        "  ax2.plot(tokens_seen, train_losses, alpha=0) # invisible plot for aligning ticks\n",
        "  ax2.set_xlabel(\"Tokens Seen\")\n",
        "\n",
        "  fig.tight_layout() # adjust layout to make room\n",
        "  plt.savefig(fig_path)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "epochs_tensor = torch.linspace(1, n_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n",
        "plt.tight_layout(); plt.savefig(\"loss_curves.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "yKflvWBLCipP",
        "outputId": "ad648d3c-db5c-46b8-f08c-4bc4249b4d9c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K9eE9rgKlvbL"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wC9JTbUVCinp"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_end_time = time.time()\n",
        "runtime_in_seconds = notebook_end_time - notebook_start_time\n",
        "\n",
        "# format as minutes and seconds\n",
        "minutes, seconds = divmod(runtime_in_seconds, 60)\n",
        "print(f\"Notebook runtime: {int(minutes)} min {seconds:.2f} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHTtZ532CikY",
        "outputId": "1280da5d-0026-4d51-a1db-6d5e0b24bb47"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Notebook runtime: 8 min 41.55 sec\n"
          ]
        }
      ]
    }
  ]
}